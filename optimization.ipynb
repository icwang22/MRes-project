{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d111546",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T01:18:23.194286184Z",
     "start_time": "2023-06-28T01:18:22.316066685Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import numba as nb\n",
    "from pC2DMS import *\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eb9a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = datetime.now()\n",
    "logging.basicConfig(filename='report_run_time.log', filemode='w', format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "\n",
    "# file_name  peptide_sequence\n",
    "# ME14_3+:   VTIMPK(Ac)DIQLAR\n",
    "# PH4_3+     EQFDDY(p)GHMRF(NH2) \n",
    "# ME9_3+:    GWGR(Me2)EENLFSWK\n",
    "# ME16_3+:   VTIMPKDIQLAR\n",
    "# 7255:      ALDLLDRNYLQSLPSK\n",
    "# 7732:      KFIFRTAGTAGR\n",
    "# 6727:      DQARVAPSSSDPKSKFF\n",
    "# 7302:      HGMTVVIRKKF\n",
    "# 3510:      GSHQISLDNPDYQQDFFPK\n",
    "loss = 0\n",
    "for dirname in os.listdir('./peptide output/'):\n",
    "    for filename in os.listdir('./peptide output/' + str(dirname)):\n",
    "        if filename == '.ipynb_checkpoints':\n",
    "            continue\n",
    "        if filename == '20150626_1623_PH15_2+_CID_NCE_10_CVscan_scanTime60mins':\n",
    "            continue\n",
    "        sub_start = datetime.now()\n",
    "        numscan_list = []\n",
    "        for dir_name in os.listdir('./peptide output/' + str(dirname) + '/' + str(filename) + '/'):\n",
    "            if len(dir_name.split(' ')) == 2:\n",
    "                numscan_list.append(int(dir_name.split(' ')[0]))\n",
    "        numscan_list.sort()\n",
    "        if os.path.exists('./peptide output/' + str(dirname) + '/' + str(filename) + '/' + str(numscan_list[-1])+' scans/'+ filename +'_histogram.jpg'):\n",
    "            continue\n",
    "        sys.argv = [sys.argv[0], '--parpath', './peptide output/' + str(dirname) + '/' + str(filename) + '/', '--name', str(filename), '--numscan_list', numscan_list]\n",
    "        %run report.ipynb \n",
    "        print('\\n')\n",
    "        sub_stop = datetime.now()\n",
    "        logging.info(f\"The running time of {filename}: {sub_stop - sub_start}\")\n",
    "        # scan_number = numscan_list[-1]\n",
    "        # loss += 1 - np.load('./peptide output/' + str(dirname) + '/' + str(filename) + '/' + str(scan_number)+' scans/'+ name +'.npy', allow_pickle=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba507ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d63fa4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T01:18:23.194982364Z",
     "start_time": "2023-06-28T01:18:23.190840305Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculate_speed_of_convergence(x: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Computes the speed of convergence\n",
    "\n",
    "    Args:\n",
    "        x: The array of values to compute the speed of convergence for (must be at least 2 elements long)\n",
    "\n",
    "    Returns:\n",
    "        The speed of convergence\n",
    "    \"\"\"\n",
    "    rate_of_change = np.diff(x)\n",
    "    weights = np.ones(len(rate_of_change)) # np.arange(len(rate_of_change), 0, -1) / len(rate_of_change)\n",
    "    weights_sum = np.sum(weights)\n",
    "    if weights_sum == 0:\n",
    "        # Handle the case when weights sum to zero\n",
    "        return# n 1.0\n",
    "\n",
    "    speed_of_convergence = np.average(rate_of_change, weights=weights).astype(np.float32)\n",
    "    return speed_of_convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2314250",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T01:18:23.205250078Z",
     "start_time": "2023-06-28T01:18:23.195158769Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def objective_function(df, k):\n",
    "    true_labels = df['interpretation 1'].ne('')\n",
    "    predictions = df['normalised score'].astype('float')\n",
    "\n",
    "    # Calculate cost-sensitive conditional risk\n",
    "    false_positive_cost = 10.0  \n",
    "    false_negative_cost = 0.01\n",
    "\n",
    "    true_positive = np.logical_and(true_labels, predictions >= 0)\n",
    "    false_positive = np.logical_and(~true_labels, predictions >= 0)\n",
    "    true_negative = np.logical_and(~true_labels, predictions < 0)\n",
    "    false_negative = np.logical_and(true_labels, predictions < 0)\n",
    "\n",
    "    cost_sensitive_conditional_risk = (np.sum(false_positive) * false_positive_cost +\n",
    "                                       np.sum(false_negative) * false_negative_cost) / len(df)\n",
    "    \n",
    "    predictions = (predictions - np.nanmin(predictions)) / (np.nanmax(predictions) - np.nanmin(predictions))\n",
    "\n",
    "    # Calculate average precision\n",
    "    average_precision = average_precision_score(true_labels, predictions)\n",
    "\n",
    "    # Calculate top-k mean average precision\n",
    "    top_k_predictions = np.argsort(predictions)[-k:]\n",
    "    top_k_true_labels = true_labels[top_k_predictions]\n",
    "    top_k_average_precision = average_precision_score(top_k_true_labels, top_k_predictions)\n",
    "    \n",
    "    print('top_k_average_precision: ', top_k_average_precision)\n",
    "    print('cost_sensitive_conditional_risk: ', cost_sensitive_conditional_risk)\n",
    "    objective_value = (0.99 * top_k_average_precision) - (0.01 * cost_sensitive_conditional_risk)\n",
    "\n",
    "    return objective_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce980856",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T01:18:25.187719153Z",
     "start_time": "2023-06-28T01:18:23.214683868Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 10:17:46,881 - numexpr.utils - INFO - Note: detected 256 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "2023-08-16 10:17:46,883 - numexpr.utils - INFO - Note: NumExpr detected 256 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "import functools\n",
    "import pC2DMS\n",
    "import pC2DMSUtils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numba as nb\n",
    "import torch\n",
    "import itertools\n",
    "import gc\n",
    "from scipy import integrate\n",
    "\n",
    "logging.basicConfig(filename='optimize_weights.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def calculate_whole_score_list(numscan_list, top_score, new_top_feat_lows, tolerance):\n",
    "    whole_score_list = []\n",
    "    for j in top_score:\n",
    "        score_list = []\n",
    "        for i in numscan_list:\n",
    "            matches = ((abs(torch.tensor(float(j[2]) - new_top_feat_lows[i // 1000 - 1][:, 0].astype(float))) <= tolerance) &\n",
    "                       (abs(torch.tensor(float(j[3]) - new_top_feat_lows[i // 1000 - 1][:, 1].astype(float))) <= tolerance))\n",
    "            score_list.extend([[float(j[0]), int(i), 0]] + [\n",
    "                [float(j[0]), int(i), float(k[3])]\n",
    "                for k in new_top_feat_lows[i // 1000 - 1][matches]\n",
    "            ])\n",
    "        score_list = np.array(score_list)\n",
    "        whole_score_list.append(score_list)\n",
    "    return whole_score_list\n",
    "\n",
    "def loss_func1(scan_dir, weights, indexList, numScans='all'):\n",
    "    print(scan_dir)\n",
    "    scan1 = pC2DMS.Scan(scan_dir)\n",
    "    cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc(weights), numScans=numScans)\n",
    "    topfeat = cmap.sampleFeats(indexList)\n",
    "    tolerance = 0.8\n",
    "    tolerance_float = torch.tensor(tolerance, dtype=torch.float)\n",
    "    # Cut off diagonal with 5.5 Da\n",
    "    new_top_feat = []\n",
    "    for i in topfeat:\n",
    "        if abs(torch.tensor(i[0] - i[1])) >= 5.5:\n",
    "            new_top_feat.append(i)\n",
    "    new_top_feat = np.array(new_top_feat, dtype=object)\n",
    "    max_number = 1844.9996199999998\n",
    "    sequence = np.load(os.path.join(scan_dir, 'sequence.npy'), allow_pickle=True)\n",
    "    correlation = []\n",
    "    # compare the measured fragments pairs to theoretical values list\n",
    "    # if the difference is smaller than or equal to the tolerance, \n",
    "    # the array created will record this pair\n",
    "    for i in new_top_feat:\n",
    "        for j in sequence:\n",
    "            if abs(torch.tensor(i[0]-j[2])) <= tolerance and abs(torch.tensor(i[1]-j[1])) <= tolerance:\n",
    "                mass_deviation = np.sqrt((i[0]-j[2])**2+(i[1]-j[1])**2)\n",
    "                correlation.append([str(j[3]), i[0], i[1], str(j[0]), i[2], i[3], j[4], mass_deviation.round(3)])\n",
    "            elif abs(torch.tensor(i[0]-j[1])) <= tolerance and abs(torch.tensor(i[1]-j[2])) <= tolerance:\n",
    "                mass_deviation = np.sqrt((i[0]-j[1])**2+(i[1]-j[2])**2)\n",
    "                correlation.append([str(j[0]), i[0], i[1], str(j[3]), i[2], i[3], j[4], mass_deviation.round(3)])\n",
    "    correlation = np.array(correlation)\n",
    "    correlation[:,1:3]\n",
    "    correlation_list = correlation[:,1:3].astype(float).tolist()\n",
    "    # if there is no match with the theoretical values, will record the pair as unidentified fragments\n",
    "    unidentify = []\n",
    "    for item in new_top_feat.tolist():\n",
    "        if item[:2] not in correlation_list:\n",
    "            unidentify.append(['',item[0],item[1],'',item[2],item[3],'',''])\n",
    "    correlation_new = correlation.tolist()+unidentify\n",
    "    correlation_new = np.array(correlation_new)\n",
    "    # sort the fragments according to their normalised correlation scores\n",
    "    mapp = np.array([float(x) for x in correlation_new[:, 5]])\n",
    "    correlation_new = correlation_new[np.flip(mapp.argsort())]\n",
    "\n",
    "    df = pd.DataFrame(correlation_new, columns=['Interpretation A','m/z A', 'm/z B', 'Interpretation B',\n",
    "                     'CorrelationScore', 'NormalisedScore', 'Plausibility','MassDeviation'])\n",
    "    df.NormalisedScore = df.NormalisedScore.astype(float)\n",
    "    # sort the fragments according to normalisation scores and then to their interpretation plausibility and then to\n",
    "    # their mass deviation from theoretical values\n",
    "    df2 = df.sort_values(by=['NormalisedScore','Plausibility','MassDeviation'], ascending = [False, True, True])\n",
    "    correlation_new=df2.to_numpy()\n",
    "    # add index number to fragments\n",
    "    # different index numbers refer to different pairs\n",
    "    plus_index =[]\n",
    "    number=1\n",
    "    for row in nb.prange(len(correlation_new)):\n",
    "        if row < 1:\n",
    "            plus_index.append(np.insert(correlation_new[row],0,number))\n",
    "        elif row >= 1:\n",
    "            if correlation_new[row][1]==correlation_new[row-1][1] and correlation_new[row][2]==correlation_new[row-1][2]:\n",
    "                plus_index.append(np.insert(correlation_new[row],0,''))\n",
    "\n",
    "            else:\n",
    "                number = number+1\n",
    "                plus_index.append(np.insert(correlation_new[row],0,number))\n",
    "    df=pd.DataFrame(plus_index, columns=['Index', 'interpretation 1','m/z 1','m/z 2','interpretation 2','score',\n",
    "                                         'normalised score','Plausibility','MassDeviation'])\n",
    "    # drop repeated rows accoring to column m/z 1 & m/z (delete those ones with more than 1 interpretation)\n",
    "    df=df.drop_duplicates(subset=['m/z 1', 'm/z 2'], keep=\"first\").reset_index(drop=True)\n",
    "    df['m/z 1']=df['m/z 1'].astype(float)\n",
    "    df['m/z 2']=df['m/z 2'].astype(float)\n",
    "    df['normalised score']=df['normalised score'].astype(float)\n",
    "    count = [] \n",
    "    ## if there is no interpretations, array appends '1',\n",
    "    ## if there are interpretations, array appends '0'\n",
    "    for i in nb.prange(30):\n",
    "        if df.iloc[i]['interpretation 1']  == '':\n",
    "            count.append(1)\n",
    "        else:\n",
    "            count.append(0)\n",
    "    newlist=np.array(count)\n",
    "    y_list = []\n",
    "    x_list = []\n",
    "    x_value = 0\n",
    "    while len(newlist) >= 10:\n",
    "        # count the sum of array for the first 10 peaks\n",
    "        # the sum is equivalent to the number of unidentified peaks\n",
    "        first_ten = newlist[:10]\n",
    "        y_list.append(np.sum(first_ten))\n",
    "         # count the sum of array for next 10 peaks\n",
    "        newlist = newlist[10:]\n",
    "        # create an array for x-axis array\n",
    "        x_value = x_value+10\n",
    "        x_list.append(x_value)\n",
    "    \n",
    "    print('Number of unidentified peaks: ', y_list)\n",
    "    print('Weights: ', weights)\n",
    "\n",
    "    '''# Save weights\n",
    "    if os.path.exists(os.path.join(scan_dir, 'weights.npy')):\n",
    "        os.remove(os.path.join(scan_dir, 'weights.npy'))\n",
    "    np.save(os.path.join(scan_dir, 'weights.npy'), weights)'''\n",
    "    \n",
    "    return np.sum(y_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "079378e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_func2(scan_dir, weights, indexList, numScans='all'):\n",
    "    print(scan_dir)\n",
    "    scan1 = pC2DMS.Scan(scan_dir)\n",
    "    cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc(weights), numScans=numScans)\n",
    "    topfeat = cmap.sampleFeats(indexList)\n",
    "    tolerance = 0.8\n",
    "    tolerance_float = torch.tensor(tolerance, dtype=torch.float)\n",
    "    # Cut off diagonal with 5.5 Da\n",
    "    new_top_feat = []\n",
    "    for i in topfeat:\n",
    "        if abs(torch.tensor(i[0] - i[1])) >= 5.5:\n",
    "            new_top_feat.append(i)\n",
    "    new_top_feat = np.array(new_top_feat, dtype=object)\n",
    "    max_number = 1844.9996199999998\n",
    "    sequence = np.load(os.path.join(scan_dir, 'sequence.npy'), allow_pickle=True)\n",
    "    correlation = []\n",
    "    # compare the measured fragments pairs to theoretical values list\n",
    "    # if the difference is smaller than or equal to the tolerance, \n",
    "    # the array created will record this pair\n",
    "    for i in new_top_feat:\n",
    "        for j in sequence:\n",
    "            if abs(torch.tensor(i[0]-j[2])) <= tolerance and abs(torch.tensor(i[1]-j[1])) <= tolerance:\n",
    "                mass_deviation = np.sqrt((i[0]-j[2])**2+(i[1]-j[1])**2)\n",
    "                correlation.append([str(j[3]), i[0], i[1], str(j[0]), i[2], i[3], j[4], mass_deviation.round(3)])\n",
    "            elif abs(torch.tensor(i[0]-j[1])) <= tolerance and abs(torch.tensor(i[1]-j[2])) <= tolerance:\n",
    "                mass_deviation = np.sqrt((i[0]-j[1])**2+(i[1]-j[2])**2)\n",
    "                correlation.append([str(j[0]), i[0], i[1], str(j[3]), i[2], i[3], j[4], mass_deviation.round(3)])\n",
    "    correlation = np.array(correlation)\n",
    "    correlation[:,1:3]\n",
    "    correlation_list = correlation[:,1:3].astype(float).tolist()\n",
    "    # if there is no match with the theoretical values, will record the pair as unidentified fragments\n",
    "    unidentify = []\n",
    "    for item in new_top_feat.tolist():\n",
    "        if item[:2] not in correlation_list:\n",
    "            unidentify.append(['',item[0],item[1],'',item[2],item[3],'',''])\n",
    "    correlation_new = correlation.tolist()+unidentify\n",
    "    correlation_new = np.array(correlation_new)\n",
    "    # sort the fragments according to their normalised correlation scores\n",
    "    mapp = np.array([float(x) for x in correlation_new[:, 5]])\n",
    "    correlation_new = correlation_new[np.flip(mapp.argsort())]\n",
    "\n",
    "    df = pd.DataFrame(correlation_new, columns=['Interpretation A','m/z A', 'm/z B', 'Interpretation B',\n",
    "                     'CorrelationScore', 'NormalisedScore', 'Plausibility','MassDeviation'])\n",
    "    df.NormalisedScore = df.NormalisedScore.astype(float)\n",
    "    # sort the fragments according to normalisation scores and then to their interpretation plausibility and then to\n",
    "    # their mass deviation from theoretical values\n",
    "    df2 = df.sort_values(by=['NormalisedScore','Plausibility','MassDeviation'], ascending = [False, True, True])\n",
    "    correlation_new=df2.to_numpy()\n",
    "    # add index number to fragments\n",
    "    # different index numbers refer to different pairs\n",
    "    plus_index =[]\n",
    "    number=1\n",
    "    for row in nb.prange(len(correlation_new)):\n",
    "        if row < 1:\n",
    "            plus_index.append(np.insert(correlation_new[row],0,number))\n",
    "        elif row >= 1:\n",
    "            if correlation_new[row][1]==correlation_new[row-1][1] and correlation_new[row][2]==correlation_new[row-1][2]:\n",
    "                plus_index.append(np.insert(correlation_new[row],0,''))\n",
    "\n",
    "            else:\n",
    "                number = number+1\n",
    "                plus_index.append(np.insert(correlation_new[row],0,number))\n",
    "    df=pd.DataFrame(plus_index, columns=['Index', 'interpretation 1','m/z 1','m/z 2','interpretation 2','score',\n",
    "                                         'normalised score','Plausibility','MassDeviation'])\n",
    "    # drop repeated rows accoring to column m/z 1 & m/z (delete those ones with more than 1 interpretation)\n",
    "    df=df.drop_duplicates(subset=['m/z 1', 'm/z 2'], keep=\"first\").reset_index(drop=True)\n",
    "    df['m/z 1']=df['m/z 1'].astype(float)\n",
    "    df['m/z 2']=df['m/z 2'].astype(float)\n",
    "    unidentified_index = next((j+1 for j, row in df.iterrows() if row['interpretation 1'] == ''), None)\n",
    "    print('unidentified_index: ', unidentified_index)\n",
    "    print('weights: ', weights)\n",
    "    return -unidentified_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1283d4e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T01:18:25.197029120Z",
     "start_time": "2023-06-28T01:18:25.191606524Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def loss_func3(scan_dir, weights, numScans='all', top_n=600):\n",
    "    scan1 = pC2DMS.Scan(scan_dir)\n",
    "    cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc(weights), numScans=numScans)\n",
    "    topfeat = cmap.analyse(top_n)\n",
    "    # Cut off diagonal with 5.5 Da\n",
    "    new_top_feat=[]\n",
    "    for i in topfeat:\n",
    "        if abs(torch.tensor(i[0]-i[1])) >= 5.5:\n",
    "            new_top_feat.append(i)\n",
    "    new_top_feat = np.array(new_top_feat)\n",
    "    df = pd.DataFrame(new_top_feat, columns=['m/z A', 'm/z B', 'CorrelationScore', 'NormalisedScore'])\n",
    "    df.NormalisedScore = df.NormalisedScore.astype(float)\n",
    "    df2 = df.sort_values(by=['NormalisedScore'], ascending=False)\n",
    "    new_top_feat = df2.to_numpy()\n",
    "    max_number = 1844.9996199999998\n",
    "    # Find the position where the fragments mass exceeds that of peptide\n",
    "    for j in np.arange(len(new_top_feat)):\n",
    "        if df.iloc[j]['m/z A'] + df.iloc[j]['m/z B'] > max_number:\n",
    "            outlier_index = j + 1\n",
    "            break\n",
    "    for j in np.arange(len(new_top_feat)):\n",
    "        if df.iloc[j]['m/z A'] + df.iloc[j]['m/z B'] > max_number:\n",
    "            outlier_index = j + 1\n",
    "            break\n",
    "    print('outlier_index', outlier_index)\n",
    "    print('weights', weights)\n",
    "\n",
    "    # Save weights\n",
    "    if os.path.exists(os.path.join(scan_dir, 'weights.npy')):\n",
    "        os.remove(os.path.join(scan_dir, 'weights.npy'))\n",
    "    np.save(os.path.join(scan_dir, 'weights.npy'), weights)\n",
    "\n",
    "    return -outlier_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5dbb81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_func4(scan_dir, weights, indexList, numScans='all'):\n",
    "    print(scan_dir)\n",
    "    scan1 = pC2DMS.Scan(scan_dir)\n",
    "    cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc(weights), numScans=numScans)\n",
    "    topfeat = cmap.sampleFeats(indexList)\n",
    "    tolerance = 0.8\n",
    "    tolerance_float = torch.tensor(tolerance, dtype=torch.float)\n",
    "    # Cut off diagonal with 5.5 Da\n",
    "    new_top_feat = []\n",
    "    for i in topfeat:\n",
    "        if abs(torch.tensor(i[0] - i[1])) >= 5.5:\n",
    "            new_top_feat.append(i)\n",
    "    new_top_feat = np.array(new_top_feat, dtype=object)\n",
    "    max_number = 1844.9996199999998\n",
    "    sequence = np.load(os.path.join(scan_dir, 'sequence.npy'), allow_pickle=True)\n",
    "    correlation = []\n",
    "    # compare the measured fragments pairs to theoretical values list\n",
    "    # if the difference is smaller than or equal to the tolerance, \n",
    "    # the array created will record this pair\n",
    "    for i in new_top_feat:\n",
    "        for j in sequence:\n",
    "            if abs(torch.tensor(i[0]-j[2])) <= tolerance and abs(torch.tensor(i[1]-j[1])) <= tolerance:\n",
    "                mass_deviation = np.sqrt((i[0]-j[2])**2+(i[1]-j[1])**2)\n",
    "                correlation.append([str(j[3]), i[0], i[1], str(j[0]), i[2], i[3], j[4], mass_deviation.round(3)])\n",
    "            elif abs(torch.tensor(i[0]-j[1])) <= tolerance and abs(torch.tensor(i[1]-j[2])) <= tolerance:\n",
    "                mass_deviation = np.sqrt((i[0]-j[1])**2+(i[1]-j[2])**2)\n",
    "                correlation.append([str(j[0]), i[0], i[1], str(j[3]), i[2], i[3], j[4], mass_deviation.round(3)])\n",
    "    correlation = np.array(correlation)\n",
    "    correlation[:,1:3]\n",
    "    correlation_list = correlation[:,1:3].astype(float).tolist()\n",
    "    # if there is no match with the theoretical values, will record the pair as unidentified fragments\n",
    "    unidentify = []\n",
    "    for item in new_top_feat.tolist():\n",
    "        if item[:2] not in correlation_list:\n",
    "            unidentify.append(['',item[0],item[1],'',item[2],item[3],'',''])\n",
    "    correlation_new = correlation.tolist()+unidentify\n",
    "    correlation_new = np.array(correlation_new)\n",
    "    # sort the fragments according to their normalised correlation scores\n",
    "    mapp = np.array([float(x) for x in correlation_new[:, 5]])\n",
    "    correlation_new = correlation_new[np.flip(mapp.argsort())]\n",
    "\n",
    "    df = pd.DataFrame(correlation_new, columns=['Interpretation A','m/z A', 'm/z B', 'Interpretation B',\n",
    "                     'CorrelationScore', 'NormalisedScore', 'Plausibility','MassDeviation'])\n",
    "    df.NormalisedScore = df.NormalisedScore.astype(float)\n",
    "    # sort the fragments according to normalisation scores and then to their interpretation plausibility and then to\n",
    "    # their mass deviation from theoretical values\n",
    "    df2 = df.sort_values(by=['NormalisedScore','Plausibility','MassDeviation'], ascending = [False, True, True])\n",
    "    correlation_new=df2.to_numpy()\n",
    "    # add index number to fragments\n",
    "    # different index numbers refer to different pairs\n",
    "    plus_index =[]\n",
    "    number=1\n",
    "    for row in nb.prange(len(correlation_new)):\n",
    "        if row < 1:\n",
    "            plus_index.append(np.insert(correlation_new[row],0,number))\n",
    "        elif row >= 1:\n",
    "            if correlation_new[row][1]==correlation_new[row-1][1] and correlation_new[row][2]==correlation_new[row-1][2]:\n",
    "                plus_index.append(np.insert(correlation_new[row],0,''))\n",
    "\n",
    "            else:\n",
    "                number = number+1\n",
    "                plus_index.append(np.insert(correlation_new[row],0,number))\n",
    "    df=pd.DataFrame(plus_index, columns=['Index', 'interpretation 1','m/z 1','m/z 2','interpretation 2','score',\n",
    "                                         'normalised score','Plausibility','MassDeviation'])\n",
    "    # drop repeated rows accoring to column m/z 1 & m/z (delete those ones with more than 1 interpretation)\n",
    "    df=df.drop_duplicates(subset=['m/z 1', 'm/z 2'], keep=\"first\").reset_index(drop=True)\n",
    "    df['m/z 1']=df['m/z 1'].astype(float)\n",
    "    df['m/z 2']=df['m/z 2'].astype(float)\n",
    "    score = objective_function(df, 30)\n",
    "    \n",
    "    # Save weights\n",
    "    '''if os.path.exists(os.path.join(scan_dir, 'weights_de.npy')):\n",
    "        os.remove(os.path.join(scan_dir, 'weights_de.npy'))\n",
    "    np.save(os.path.join(scan_dir, 'weights_de.npy'), weights)'''\n",
    "    \n",
    "    print(\"weights: \", weights)\n",
    "    print(\"Score: \", score)\n",
    "    print(df[:50])\n",
    "    \n",
    "    return -score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecfe4a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_func5(scan_dir, weights, indexList, numScans='all'):\n",
    "    print(scan_dir)\n",
    "    scan1 = pC2DMS.Scan(scan_dir)\n",
    "    cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc(weights), numScans=numScans)\n",
    "    topfeat = cmap.sampleFeats(indexList)\n",
    "    tolerance = 0.8\n",
    "    tolerance_float = torch.tensor(tolerance, dtype=torch.float)\n",
    "    # Cut off diagonal with 5.5 Da\n",
    "    new_top_feat = []\n",
    "    for i in topfeat:\n",
    "        if abs(torch.tensor(i[0] - i[1])) >= 5.5:\n",
    "            new_top_feat.append(i)\n",
    "    new_top_feat = np.array(new_top_feat, dtype=object)\n",
    "    max_number = 1844.9996199999998\n",
    "    sequence = np.load(os.path.join(scan_dir, 'sequence.npy'), allow_pickle=True)\n",
    "    correlation = []\n",
    "    # compare the measured fragments pairs to theoretical values list\n",
    "    # if the difference is smaller than or equal to the tolerance, \n",
    "    # the array created will record this pair\n",
    "    for i in new_top_feat:\n",
    "        for j in sequence:\n",
    "            if abs(torch.tensor(i[0]-j[2])) <= tolerance and abs(torch.tensor(i[1]-j[1])) <= tolerance:\n",
    "                mass_deviation = np.sqrt((i[0]-j[2])**2+(i[1]-j[1])**2)\n",
    "                correlation.append([str(j[3]), i[0], i[1], str(j[0]), i[2], i[3], j[4], mass_deviation.round(3)])\n",
    "            elif abs(torch.tensor(i[0]-j[1])) <= tolerance and abs(torch.tensor(i[1]-j[2])) <= tolerance:\n",
    "                mass_deviation = np.sqrt((i[0]-j[1])**2+(i[1]-j[2])**2)\n",
    "                correlation.append([str(j[0]), i[0], i[1], str(j[3]), i[2], i[3], j[4], mass_deviation.round(3)])\n",
    "    correlation = np.array(correlation)\n",
    "    correlation[:,1:3]\n",
    "    correlation_list = correlation[:,1:3].astype(float).tolist()\n",
    "    # if there is no match with the theoretical values, will record the pair as unidentified fragments\n",
    "    unidentify = []\n",
    "    for item in new_top_feat.tolist():\n",
    "        if item[:2] not in correlation_list:\n",
    "            unidentify.append(['',item[0],item[1],'',item[2],item[3],'',''])\n",
    "    correlation_new = correlation.tolist()+unidentify\n",
    "    correlation_new = np.array(correlation_new)\n",
    "    # sort the fragments according to their normalised correlation scores\n",
    "    mapp = np.array([float(x) for x in correlation_new[:, 5]])\n",
    "    correlation_new = correlation_new[np.flip(mapp.argsort())]\n",
    "\n",
    "    df = pd.DataFrame(correlation_new, columns=['Interpretation A','m/z A', 'm/z B', 'Interpretation B',\n",
    "                     'CorrelationScore', 'NormalisedScore', 'Plausibility','MassDeviation'])\n",
    "    df.NormalisedScore = df.NormalisedScore.astype(float)\n",
    "    # sort the fragments according to normalisation scores and then to their interpretation plausibility and then to\n",
    "    # their mass deviation from theoretical values\n",
    "    df2 = df.sort_values(by=['NormalisedScore','Plausibility','MassDeviation'], ascending = [False, True, True])\n",
    "    correlation_new=df2.to_numpy()\n",
    "    # add index number to fragments\n",
    "    # different index numbers refer to different pairs\n",
    "    plus_index =[]\n",
    "    number=1\n",
    "    for row in nb.prange(len(correlation_new)):\n",
    "        if row < 1:\n",
    "            plus_index.append(np.insert(correlation_new[row],0,number))\n",
    "        elif row >= 1:\n",
    "            if correlation_new[row][1]==correlation_new[row-1][1] and correlation_new[row][2]==correlation_new[row-1][2]:\n",
    "                plus_index.append(np.insert(correlation_new[row],0,''))\n",
    "\n",
    "            else:\n",
    "                number = number+1\n",
    "                plus_index.append(np.insert(correlation_new[row],0,number))\n",
    "    df=pd.DataFrame(plus_index, columns=['Index', 'interpretation 1','m/z 1','m/z 2','interpretation 2','score',\n",
    "                                         'normalised score','Plausibility','MassDeviation'])\n",
    "    # drop repeated rows accoring to column m/z 1 & m/z (delete those ones with more than 1 interpretation)\n",
    "    df=df.drop_duplicates(subset=['m/z 1', 'm/z 2'], keep=\"first\").reset_index(drop=True)\n",
    "    df['m/z 1']=df['m/z 1'].astype(float)\n",
    "    df['m/z 2']=df['m/z 2'].astype(float)\n",
    "    unidentified_index = next((j+1 for j, row in df.iterrows() if row['interpretation 1'] == ''), None)\n",
    "    \n",
    "    top_number = (unidentified_index // 10) * 10\n",
    "\n",
    "    scan_low = pC2DMS.Scan(os.path.abspath(os.path.join(scan_dir, os.pardir)) + '/' + str(1000) + ' scans/')\n",
    "    top_feat_low = pC2DMS.PCovMap(scan_low, scan_low.weightFunc(weights)).sampleFeats(indexList)\n",
    "    new_top_feat_low = top_feat_low[np.abs(top_feat_low[:, 0] - top_feat_low[:, 1]) >= 5.5]\n",
    "    new_top_feat_low = new_top_feat_low[np.flip(new_top_feat_low[:, 3].argsort())]\n",
    "    count = []\n",
    "    for z, j in itertools.product(topfeat[:top_number+1], new_top_feat_low[:top_number+1]):\n",
    "        if torch.allclose(torch.tensor([z[0] - j[0], z[1] - j[1]], dtype=torch.float),\n",
    "                          torch.tensor([0.0, 0.0], dtype=torch.float), atol=tolerance_float):\n",
    "            count.append(1)\n",
    "        if torch.allclose(torch.tensor([z[0] - j[1], z[1] - j[0]], dtype=torch.float),\n",
    "                          torch.tensor([0.0, 0.0], dtype=torch.float), atol=tolerance_float):\n",
    "            count.append(1)\n",
    "            \n",
    "    print('number of correlations included in ' + str(10000) + ' scans within top '+ \n",
    "           str(top_number)+ 'peaks: ' , len(count))\n",
    "    print('weights', weights)\n",
    "    \n",
    "    '''# Save weights\n",
    "    if os.path.exists(os.path.join(scan_dir, 'weights.npy')):\n",
    "        os.remove(os.path.join(scan_dir, 'weights.npy'))\n",
    "    np.save(os.path.join(scan_dir, 'weights.npy'), weights)'''\n",
    "    \n",
    "    return -len(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38afc65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_func6(scan_dir, weights, indexList, numScans='all'):\n",
    "    print(scan_dir)\n",
    "    scan1 = pC2DMS.Scan(scan_dir)\n",
    "    cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc(weights), numScans=numScans)\n",
    "    topfeat = cmap.sampleFeats(indexList)\n",
    "    tolerance = 0.8\n",
    "    tolerance_float = torch.tensor(tolerance, dtype=torch.float)\n",
    "    # Cut off diagonal with 5.5 Da\n",
    "    new_top_feat = []\n",
    "    for i in topfeat:\n",
    "        if abs(torch.tensor(i[0] - i[1])) >= 5.5:\n",
    "            new_top_feat.append(i)\n",
    "    new_top_feat = np.array(new_top_feat, dtype=object)\n",
    "    max_number = 1844.9996199999998\n",
    "    sequence = np.load(os.path.join(scan_dir, 'sequence.npy'), allow_pickle=True)\n",
    "    correlation = []\n",
    "    # compare the measured fragments pairs to theoretical values list\n",
    "    # if the difference is smaller than or equal to the tolerance, \n",
    "    # the array created will record this pair\n",
    "    for i in new_top_feat:\n",
    "        for j in sequence:\n",
    "            if abs(torch.tensor(i[0]-j[2])) <= tolerance and abs(torch.tensor(i[1]-j[1])) <= tolerance:\n",
    "                mass_deviation = np.sqrt((i[0]-j[2])**2+(i[1]-j[1])**2)\n",
    "                correlation.append([str(j[3]), i[0], i[1], str(j[0]), i[2], i[3], j[4], mass_deviation.round(3)])\n",
    "            elif abs(torch.tensor(i[0]-j[1])) <= tolerance and abs(torch.tensor(i[1]-j[2])) <= tolerance:\n",
    "                mass_deviation = np.sqrt((i[0]-j[1])**2+(i[1]-j[2])**2)\n",
    "                correlation.append([str(j[0]), i[0], i[1], str(j[3]), i[2], i[3], j[4], mass_deviation.round(3)])\n",
    "    correlation = np.array(correlation)\n",
    "    correlation[:,1:3]\n",
    "    correlation_list = correlation[:,1:3].astype(float).tolist()\n",
    "    # if there is no match with the theoretical values, will record the pair as unidentified fragments\n",
    "    unidentify = []\n",
    "    for item in new_top_feat.tolist():\n",
    "        if item[:2] not in correlation_list:\n",
    "            unidentify.append(['',item[0],item[1],'',item[2],item[3],'',''])\n",
    "    correlation_new = correlation.tolist()+unidentify\n",
    "    correlation_new = np.array(correlation_new)\n",
    "    # sort the fragments according to their normalised correlation scores\n",
    "    mapp = np.array([float(x) for x in correlation_new[:, 5]])\n",
    "    correlation_new = correlation_new[np.flip(mapp.argsort())]\n",
    "\n",
    "    df = pd.DataFrame(correlation_new, columns=['Interpretation A','m/z A', 'm/z B', 'Interpretation B',\n",
    "                     'CorrelationScore', 'NormalisedScore', 'Plausibility','MassDeviation'])\n",
    "    df.NormalisedScore = df.NormalisedScore.astype(float)\n",
    "    # sort the fragments according to normalisation scores and then to their interpretation plausibility and then to\n",
    "    # their mass deviation from theoretical values\n",
    "    df2 = df.sort_values(by=['NormalisedScore','Plausibility','MassDeviation'], ascending = [False, True, True])\n",
    "    correlation_new=df2.to_numpy()\n",
    "    # add index number to fragments\n",
    "    # different index numbers refer to different pairs\n",
    "    plus_index =[]\n",
    "    number=1\n",
    "    for row in nb.prange(len(correlation_new)):\n",
    "        if row < 1:\n",
    "            plus_index.append(np.insert(correlation_new[row],0,number))\n",
    "        elif row >= 1:\n",
    "            if correlation_new[row][1]==correlation_new[row-1][1] and correlation_new[row][2]==correlation_new[row-1][2]:\n",
    "                plus_index.append(np.insert(correlation_new[row],0,''))\n",
    "\n",
    "            else:\n",
    "                number = number+1\n",
    "                plus_index.append(np.insert(correlation_new[row],0,number))\n",
    "    df=pd.DataFrame(plus_index, columns=['Index', 'interpretation 1','m/z 1','m/z 2','interpretation 2','score',\n",
    "                                         'normalised score','Plausibility','MassDeviation'])\n",
    "    # drop repeated rows accoring to column m/z 1 & m/z (delete those ones with more than 1 interpretation)\n",
    "    df=df.drop_duplicates(subset=['m/z 1', 'm/z 2'], keep=\"first\").reset_index(drop=True)\n",
    "    df['m/z 1']=df['m/z 1'].astype(float)\n",
    "    df['m/z 2']=df['m/z 2'].astype(float)\n",
    "    unidentified_index = next((j+1 for j, row in df.iterrows() if row['interpretation 1'] == ''), None)\n",
    "    print('unidentified_index: ', unidentified_index)\n",
    "    \n",
    "    top_number = (unidentified_index // 10) * 10\n",
    "\n",
    "    numscan_list = [1000, 9000, 10000]\n",
    "    num_array = []\n",
    "    new_top_feat_lows = []\n",
    "\n",
    "    for i in numscan_list[:-1]:\n",
    "        scan_low = pC2DMS.Scan(os.path.abspath(os.path.join(scan_dir, os.pardir)) + '/' + str(i) + ' scans/')\n",
    "        top_feat_low = pC2DMS.PCovMap(scan_low, scan_low.weightFunc(weights)).sampleFeats(indexList)\n",
    "        new_top_feat_low = top_feat_low[np.abs(top_feat_low[:, 0] - top_feat_low[:, 1]) >= 5.5]\n",
    "        new_top_feat_low = new_top_feat_low[np.flip(new_top_feat_low[:, 3].argsort())]\n",
    "        count = []\n",
    "        for z, j in itertools.product(new_top_feat[:top_number], new_top_feat_low[:top_number]):\n",
    "            if torch.allclose(torch.tensor([z[0] - j[0], z[1] - j[1]], dtype=torch.float),\n",
    "                              torch.tensor([0.0, 0.0], dtype=torch.float), atol=tolerance_float):\n",
    "                count.append(1)\n",
    "            if torch.allclose(torch.tensor([z[0] - j[1], z[1] - j[0]], dtype=torch.float),\n",
    "                              torch.tensor([0.0, 0.0], dtype=torch.float), atol=tolerance_float):\n",
    "                count.append(1)\n",
    "        num_array.append(len(count))\n",
    "        new_top_feat_lows.append(new_top_feat_low)\n",
    "        del top_feat_low, new_top_feat_low, count\n",
    "        gc.collect()\n",
    "    num_array.append(top_number)\n",
    "    new_top_feat_lows.append(new_top_feat)\n",
    "\n",
    "    print('num_array', num_array)\n",
    "    print('weights', weights)\n",
    "    \n",
    "    '''# Save weights\n",
    "    if os.path.exists(os.path.join(scan_dir, 'weights.npy')):\n",
    "        os.remove(os.path.join(scan_dir, 'weights.npy'))\n",
    "    np.save(os.path.join(scan_dir, 'weights.npy'), weights)'''\n",
    "    \n",
    "    num_array = np.array(num_array)\n",
    "    speed_of_convergence = calculate_speed_of_convergence(np.array(num_array)) / top_number\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    return - np.log(unidentified_index) - np.log(speed_of_convergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ce938b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_func7(scan_dir, weights, indexList, numScans='all'):\n",
    "    print(scan_dir)\n",
    "    scan1 = pC2DMS.Scan(scan_dir)\n",
    "    cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc(weights), numScans=numScans)\n",
    "    topfeat = cmap.sampleFeats(indexList)\n",
    "    tolerance = 0.8\n",
    "    tolerance_float = torch.tensor(tolerance, dtype=torch.float)\n",
    "    # Cut off diagonal with 5.5 Da\n",
    "    new_top_feat = []\n",
    "    for i in topfeat:\n",
    "        if abs(torch.tensor(i[0] - i[1])) >= 5.5:\n",
    "            new_top_feat.append(i)\n",
    "    new_top_feat = np.array(new_top_feat, dtype=object)\n",
    "    max_number = 1844.9996199999998\n",
    "    sequence = np.load(os.path.join(scan_dir, 'sequence.npy'), allow_pickle=True)\n",
    "    correlation = []\n",
    "    # compare the measured fragments pairs to theoretical values list\n",
    "    # if the difference is smaller than or equal to the tolerance, \n",
    "    # the array created will record this pair\n",
    "    for i in new_top_feat:\n",
    "        for j in sequence:\n",
    "            if abs(torch.tensor(i[0]-j[2])) <= tolerance and abs(torch.tensor(i[1]-j[1])) <= tolerance:\n",
    "                mass_deviation = np.sqrt((i[0]-j[2])**2+(i[1]-j[1])**2)\n",
    "                correlation.append([str(j[3]), i[0], i[1], str(j[0]), i[2], i[3], j[4], mass_deviation.round(3)])\n",
    "            elif abs(torch.tensor(i[0]-j[1])) <= tolerance and abs(torch.tensor(i[1]-j[2])) <= tolerance:\n",
    "                mass_deviation = np.sqrt((i[0]-j[1])**2+(i[1]-j[2])**2)\n",
    "                correlation.append([str(j[0]), i[0], i[1], str(j[3]), i[2], i[3], j[4], mass_deviation.round(3)])\n",
    "    correlation = np.array(correlation)\n",
    "    correlation[:,1:3]\n",
    "    correlation_list = correlation[:,1:3].astype(float).tolist()\n",
    "    # if there is no match with the theoretical values, will record the pair as unidentified fragments\n",
    "    unidentify = []\n",
    "    for item in new_top_feat.tolist():\n",
    "        if item[:2] not in correlation_list:\n",
    "            unidentify.append(['',item[0],item[1],'',item[2],item[3],'',''])\n",
    "    correlation_new = correlation.tolist()+unidentify\n",
    "    correlation_new = np.array(correlation_new)\n",
    "    # sort the fragments according to their normalised correlation scores\n",
    "    mapp = np.array([float(x) for x in correlation_new[:, 5]])\n",
    "    correlation_new = correlation_new[np.flip(mapp.argsort())]\n",
    "\n",
    "    df = pd.DataFrame(correlation_new, columns=['Interpretation A','m/z A', 'm/z B', 'Interpretation B',\n",
    "                     'CorrelationScore', 'NormalisedScore', 'Plausibility','MassDeviation'])\n",
    "    df.NormalisedScore = df.NormalisedScore.astype(float)\n",
    "    # sort the fragments according to normalisation scores and then to their interpretation plausibility and then to\n",
    "    # their mass deviation from theoretical values\n",
    "    df2 = df.sort_values(by=['NormalisedScore','Plausibility','MassDeviation'], ascending = [False, True, True])\n",
    "    correlation_new=df2.to_numpy()\n",
    "    # add index number to fragments\n",
    "    # different index numbers refer to different pairs\n",
    "    plus_index =[]\n",
    "    number=1\n",
    "    for row in nb.prange(len(correlation_new)):\n",
    "        if row < 1:\n",
    "            plus_index.append(np.insert(correlation_new[row],0,number))\n",
    "        elif row >= 1:\n",
    "            if correlation_new[row][1]==correlation_new[row-1][1] and correlation_new[row][2]==correlation_new[row-1][2]:\n",
    "                plus_index.append(np.insert(correlation_new[row],0,''))\n",
    "\n",
    "            else:\n",
    "                number = number+1\n",
    "                plus_index.append(np.insert(correlation_new[row],0,number))\n",
    "    df=pd.DataFrame(plus_index, columns=['Index', 'interpretation 1','m/z 1','m/z 2','interpretation 2','score',\n",
    "                                         'normalised score','Plausibility','MassDeviation'])\n",
    "    # drop repeated rows accoring to column m/z 1 & m/z (delete those ones with more than 1 interpretation)\n",
    "    df=df.drop_duplicates(subset=['m/z 1', 'm/z 2'], keep=\"first\").reset_index(drop=True)\n",
    "    df['m/z 1']=df['m/z 1'].astype(float)\n",
    "    df['m/z 2']=df['m/z 2'].astype(float)\n",
    "    score = objective_function(df, 100)\n",
    "    outlier_index = next((j+1 for j, row in df.iterrows() if row['m/z 1'] + row['m/z 2'] > max_number), None)\n",
    "    unidentified_index = next((j+1 for j, row in df.iterrows() if row['interpretation 1'] == ''), None)\n",
    "    print('outlier_index: ', outlier_index)\n",
    "    print('unidentified_index: ', unidentified_index)\n",
    "    \n",
    "    top_number = (unidentified_index // 10) * 10\n",
    "\n",
    "    numscan_list = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "    num_array = []\n",
    "    new_top_feat_lows = []\n",
    "\n",
    "    for i in numscan_list[:-1]:\n",
    "        scan_low = pC2DMS.Scan(os.path.abspath(os.path.join(scan_dir, os.pardir)) + '/' + str(i) + ' scans/')\n",
    "        top_feat_low = pC2DMS.PCovMap(scan_low, scan_low.weightFunc(weights)).sampleFeats(indexList)\n",
    "        new_top_feat_low = top_feat_low[np.abs(top_feat_low[:, 0] - top_feat_low[:, 1]) >= 5.5]\n",
    "        new_top_feat_low = new_top_feat_low[np.flip(new_top_feat_low[:, 3].argsort())]\n",
    "        new_top_feat_lows.append(new_top_feat_low)\n",
    "        count = []\n",
    "        for z, j in itertools.product(new_top_feat[:top_number], new_top_feat_low[:top_number]):\n",
    "            if torch.allclose(torch.tensor([z[0] - j[0], z[1] - j[1]], dtype=torch.float),\n",
    "                              torch.tensor([0.0, 0.0], dtype=torch.float), atol=tolerance_float):\n",
    "                count.append(1)\n",
    "            if torch.allclose(torch.tensor([z[0] - j[1], z[1] - j[0]], dtype=torch.float),\n",
    "                              torch.tensor([0.0, 0.0], dtype=torch.float), atol=tolerance_float):\n",
    "                count.append(1)\n",
    "        num_array.append(len(count))\n",
    "        del top_feat_low, new_top_feat_low, count\n",
    "        gc.collect()\n",
    "    num_array.append(top_number)\n",
    "    new_top_feat_lows.append(new_top_feat)\n",
    "\n",
    "    print('num_array', num_array)\n",
    "    print('weights', weights)\n",
    "    \n",
    "    top_number = 16\n",
    "    top_score = np.array([\n",
    "        [j[0], j[1], j[2], float(j[3]), j[4], j[6]]\n",
    "        for j in plus_index\n",
    "        for i in nb.prange(top_number)\n",
    "        if j[0] == i + 1 and j[1] != ''\n",
    "    ])\n",
    "\n",
    "    new_top_feat_lows = np.array(new_top_feat_lows, dtype=object)\n",
    "\n",
    "    whole_score_list = calculate_whole_score_list(numscan_list, top_score, new_top_feat_lows, tolerance)\n",
    "    \n",
    "    speed_of_convergence_list = []\n",
    "    for a in whole_score_list:\n",
    "        if len(a) > 0:\n",
    "            denominator = a[-1, 2] - a[0, 2]\n",
    "            if denominator == 0:\n",
    "                speed_of_convergence_list.append(calculate_speed_of_convergence(a[:, 2]) / 1000)\n",
    "            else:\n",
    "                speed_of_convergence_list.append(calculate_speed_of_convergence(a[:, 2]) / denominator)\n",
    "    speed_of_convergence_list = np.array(speed_of_convergence_list)\n",
    "\n",
    "    num_array = np.array(num_array)\n",
    "    speed_of_convergence = calculate_speed_of_convergence(np.array(num_array)) / ((unidentified_index // 10) * 10)\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    if len(speed_of_convergence_list) == 0:\n",
    "        print(\"Loss: \", - np.log(outlier_index) - np.log(unidentified_index) - 0.1 * np.log(score) - np.log(speed_of_convergence))\n",
    "        return - np.log(outlier_index) - np.log(unidentified_index) - 0.1 * np.log(score) - np.log(speed_of_convergence)\n",
    "    else:\n",
    "        mean_speed_of_convergence = np.mean(speed_of_convergence_list)\n",
    "        print(\"Loss: \", - np.log(outlier_index) - np.log(unidentified_index) - 0.1 * np.log(score) - np.log(speed_of_convergence) - np.log(mean_speed_of_convergence))\n",
    "        return - np.log(outlier_index) - np.log(unidentified_index) - 0.1 * np.log(score) - np.log(speed_of_convergence) - np.log(mean_speed_of_convergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56ccdff9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T01:18:25.228497432Z",
     "start_time": "2023-06-28T01:18:25.201028874Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def loss_func8(scan_dir, weights, indexList, numScans='all'):\n",
    "    print(scan_dir)\n",
    "    scan1 = pC2DMS.Scan(scan_dir)\n",
    "    cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc(weights), numScans=numScans)\n",
    "    topfeat = cmap.sampleFeats(indexList)\n",
    "    tolerance = 0.8\n",
    "    tolerance_float = torch.tensor(tolerance, dtype=torch.float)\n",
    "    # Cut off diagonal with 5.5 Da\n",
    "    new_top_feat = []\n",
    "    for i in topfeat:\n",
    "        if abs(torch.tensor(i[0] - i[1])) >= 5.5:\n",
    "            new_top_feat.append(i)\n",
    "    new_top_feat = np.array(new_top_feat, dtype=object)\n",
    "    max_number = 1844.9996199999998\n",
    "    sequence = np.load(os.path.join(scan_dir, 'sequence.npy'), allow_pickle=True)\n",
    "    correlation = []\n",
    "    # compare the measured fragments pairs to theoretical values list\n",
    "    # if the difference is smaller than or equal to the tolerance, \n",
    "    # the array created will record this pair\n",
    "    for i in new_top_feat:\n",
    "        for j in sequence:\n",
    "            if abs(torch.tensor(i[0]-j[2])) <= tolerance and abs(torch.tensor(i[1]-j[1])) <= tolerance:\n",
    "                mass_deviation = np.sqrt((i[0]-j[2])**2+(i[1]-j[1])**2)\n",
    "                correlation.append([str(j[3]), i[0], i[1], str(j[0]), i[2], i[3], j[4], mass_deviation.round(3)])\n",
    "            elif abs(torch.tensor(i[0]-j[1])) <= tolerance and abs(torch.tensor(i[1]-j[2])) <= tolerance:\n",
    "                mass_deviation = np.sqrt((i[0]-j[1])**2+(i[1]-j[2])**2)\n",
    "                correlation.append([str(j[0]), i[0], i[1], str(j[3]), i[2], i[3], j[4], mass_deviation.round(3)])\n",
    "    correlation = np.array(correlation)\n",
    "    correlation[:,1:3]\n",
    "    correlation_list = correlation[:,1:3].astype(float).tolist()\n",
    "    # if there is no match with the theoretical values, will record the pair as unidentified fragments\n",
    "    unidentify = []\n",
    "    for item in new_top_feat.tolist():\n",
    "        if item[:2] not in correlation_list:\n",
    "            unidentify.append(['',item[0],item[1],'',item[2],item[3],'',''])\n",
    "    correlation_new = correlation.tolist()+unidentify\n",
    "    correlation_new = np.array(correlation_new)\n",
    "    # sort the fragments according to their normalised correlation scores\n",
    "    mapp = np.array([float(x) for x in correlation_new[:, 5]])\n",
    "    correlation_new = correlation_new[np.flip(mapp.argsort())]\n",
    "\n",
    "    df = pd.DataFrame(correlation_new, columns=['Interpretation A','m/z A', 'm/z B', 'Interpretation B',\n",
    "                     'CorrelationScore', 'NormalisedScore', 'Plausibility','MassDeviation'])\n",
    "    df.NormalisedScore = df.NormalisedScore.astype(float)\n",
    "    # sort the fragments according to normalisation scores and then to their interpretation plausibility and then to\n",
    "    # their mass deviation from theoretical values\n",
    "    df2 = df.sort_values(by=['NormalisedScore','Plausibility','MassDeviation'], ascending = [False, True, True])\n",
    "    correlation_new=df2.to_numpy()\n",
    "    # add index number to fragments\n",
    "    # different index numbers refer to different pairs\n",
    "    plus_index =[]\n",
    "    number=1\n",
    "    for row in nb.prange(len(correlation_new)):\n",
    "        if row < 1:\n",
    "            plus_index.append(np.insert(correlation_new[row],0,number))\n",
    "        elif row >= 1:\n",
    "            if correlation_new[row][1]==correlation_new[row-1][1] and correlation_new[row][2]==correlation_new[row-1][2]:\n",
    "                plus_index.append(np.insert(correlation_new[row],0,''))\n",
    "\n",
    "            else:\n",
    "                number = number+1\n",
    "                plus_index.append(np.insert(correlation_new[row],0,number))\n",
    "    df=pd.DataFrame(plus_index, columns=['Index', 'interpretation 1','m/z 1','m/z 2','interpretation 2','score',\n",
    "                                         'normalised score','Plausibility','MassDeviation'])\n",
    "    # drop repeated rows accoring to column m/z 1 & m/z (delete those ones with more than 1 interpretation)\n",
    "    df=df.drop_duplicates(subset=['m/z 1', 'm/z 2'], keep=\"first\").reset_index(drop=True)\n",
    "    df['m/z 1']=df['m/z 1'].astype(float)\n",
    "    df['m/z 2']=df['m/z 2'].astype(float)\n",
    "    unidentified_index = next((j+1 for j, row in df.iterrows() if row['interpretation 1'] == ''), None)\n",
    "    \n",
    "    top_number = (unidentified_index // 10) * 10\n",
    "\n",
    "    numscan_list = [1000, 9000, 10000]\n",
    "    num_array = []\n",
    "    new_top_feat_lows = []\n",
    "\n",
    "    for i in numscan_list[:-1]:\n",
    "        scan_low = pC2DMS.Scan(os.path.abspath(os.path.join(scan_dir, os.pardir)) + '/' + str(i) + ' scans/')\n",
    "        top_feat_low = pC2DMS.PCovMap(scan_low, scan_low.weightFunc(weights)).sampleFeats(indexList)\n",
    "        new_top_feat_low = top_feat_low[np.abs(top_feat_low[:, 0] - top_feat_low[:, 1]) >= 5.5]\n",
    "        new_top_feat_low = new_top_feat_low[np.flip(new_top_feat_low[:, 3].argsort())]\n",
    "        count = []\n",
    "        for z, j in itertools.product(new_top_feat[:top_number], new_top_feat_low[:top_number]):\n",
    "            if torch.allclose(torch.tensor([z[0] - j[0], z[1] - j[1]], dtype=torch.float),\n",
    "                              torch.tensor([0.0, 0.0], dtype=torch.float), atol=tolerance_float):\n",
    "                count.append(1)\n",
    "            if torch.allclose(torch.tensor([z[0] - j[1], z[1] - j[0]], dtype=torch.float),\n",
    "                              torch.tensor([0.0, 0.0], dtype=torch.float), atol=tolerance_float):\n",
    "                count.append(1)\n",
    "        num_array.append(len(count))\n",
    "        new_top_feat_lows.append(new_top_feat_low)\n",
    "        del top_feat_low, new_top_feat_low, count\n",
    "        gc.collect()\n",
    "    num_array.append(top_number)\n",
    "    new_top_feat_lows.append(new_top_feat)\n",
    "\n",
    "\n",
    "    print('num_array', num_array)\n",
    "    print('weights', weights)\n",
    "    \n",
    "    '''# Save weights\n",
    "    if os.path.exists(os.path.join(scan_dir, 'weights_da.npy')):\n",
    "        os.remove(os.path.join(scan_dir, 'weights_da.npy'))\n",
    "    np.save(os.path.join(scan_dir, 'weights_da.npy'), weights)'''\n",
    "\n",
    "    num_array = np.array(num_array)\n",
    "    speed_of_convergence = calculate_speed_of_convergence(np.array(num_array)) / top_number\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    return -speed_of_convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b7e0670",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_func9(scan_dir, weights, indexList, numScans='all'):\n",
    "    print(scan_dir)\n",
    "    scan1 = pC2DMS.Scan(scan_dir)\n",
    "    cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc(weights), numScans=numScans)\n",
    "    topfeat = cmap.sampleFeats(indexList)\n",
    "    tolerance = 0.8\n",
    "    tolerance_float = torch.tensor(tolerance, dtype=torch.float)\n",
    "    # Cut off diagonal with 5.5 Da\n",
    "    new_top_feat = []\n",
    "    for i in topfeat:\n",
    "        if abs(torch.tensor(i[0] - i[1])) >= 5.5:\n",
    "            new_top_feat.append(i)\n",
    "    new_top_feat = np.array(new_top_feat, dtype=object)\n",
    "    max_number = 1844.9996199999998\n",
    "    sequence = np.load(os.path.join(scan_dir, 'sequence.npy'), allow_pickle=True)\n",
    "    correlation = []\n",
    "    # compare the measured fragments pairs to theoretical values list\n",
    "    # if the difference is smaller than or equal to the tolerance, \n",
    "    # the array created will record this pair\n",
    "    for i in new_top_feat:\n",
    "        for j in sequence:\n",
    "            if abs(torch.tensor(i[0]-j[2])) <= tolerance and abs(torch.tensor(i[1]-j[1])) <= tolerance:\n",
    "                mass_deviation = np.sqrt((i[0]-j[2])**2+(i[1]-j[1])**2)\n",
    "                correlation.append([str(j[3]), i[0], i[1], str(j[0]), i[2], i[3], j[4], mass_deviation.round(3)])\n",
    "            elif abs(torch.tensor(i[0]-j[1])) <= tolerance and abs(torch.tensor(i[1]-j[2])) <= tolerance:\n",
    "                mass_deviation = np.sqrt((i[0]-j[1])**2+(i[1]-j[2])**2)\n",
    "                correlation.append([str(j[0]), i[0], i[1], str(j[3]), i[2], i[3], j[4], mass_deviation.round(3)])\n",
    "    correlation = np.array(correlation)\n",
    "    correlation[:,1:3]\n",
    "    correlation_list = correlation[:,1:3].astype(float).tolist()\n",
    "    # if there is no match with the theoretical values, will record the pair as unidentified fragments\n",
    "    unidentify = []\n",
    "    for item in new_top_feat.tolist():\n",
    "        if item[:2] not in correlation_list:\n",
    "            unidentify.append(['',item[0],item[1],'',item[2],item[3],'',''])\n",
    "    correlation_new = correlation.tolist()+unidentify\n",
    "    correlation_new = np.array(correlation_new)\n",
    "    # sort the fragments according to their normalised correlation scores\n",
    "    mapp = np.array([float(x) for x in correlation_new[:, 5]])\n",
    "    correlation_new = correlation_new[np.flip(mapp.argsort())]\n",
    "\n",
    "    df = pd.DataFrame(correlation_new, columns=['Interpretation A','m/z A', 'm/z B', 'Interpretation B',\n",
    "                     'CorrelationScore', 'NormalisedScore', 'Plausibility','MassDeviation'])\n",
    "    df.NormalisedScore = df.NormalisedScore.astype(float)\n",
    "    # sort the fragments according to normalisation scores and then to their interpretation plausibility and then to\n",
    "    # their mass deviation from theoretical values\n",
    "    df2 = df.sort_values(by=['NormalisedScore','Plausibility','MassDeviation'], ascending = [False, True, True])\n",
    "    correlation_new=df2.to_numpy()\n",
    "    # add index number to fragments\n",
    "    # different index numbers refer to different pairs\n",
    "    plus_index =[]\n",
    "    number=1\n",
    "    for row in nb.prange(len(correlation_new)):\n",
    "        if row < 1:\n",
    "            plus_index.append(np.insert(correlation_new[row],0,number))\n",
    "        elif row >= 1:\n",
    "            if correlation_new[row][1]==correlation_new[row-1][1] and correlation_new[row][2]==correlation_new[row-1][2]:\n",
    "                plus_index.append(np.insert(correlation_new[row],0,''))\n",
    "\n",
    "            else:\n",
    "                number = number+1\n",
    "                plus_index.append(np.insert(correlation_new[row],0,number))\n",
    "    df=pd.DataFrame(plus_index, columns=['Index', 'interpretation 1','m/z 1','m/z 2','interpretation 2','score',\n",
    "                                         'normalised score','Plausibility','MassDeviation'])\n",
    "    # drop repeated rows accoring to column m/z 1 & m/z (delete those ones with more than 1 interpretation)\n",
    "    df=df.drop_duplicates(subset=['m/z 1', 'm/z 2'], keep=\"first\").reset_index(drop=True)\n",
    "    df['m/z 1']=df['m/z 1'].astype(float)\n",
    "    df['m/z 2']=df['m/z 2'].astype(float)\n",
    "    unidentified_index = next((j+1 for j, row in df.iterrows() if row['interpretation 1'] == ''), None)\n",
    "    \n",
    "    top_number = (unidentified_index // 10) * 10\n",
    "\n",
    "    numscan_list = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "\n",
    "    converge_index = 10\n",
    "    for i in numscan_list[:-1]:\n",
    "        scan_low = pC2DMS.Scan(os.path.abspath(os.path.join(scan_dir, os.pardir)) + '/' + str(i) + ' scans/')\n",
    "        top_feat_low = pC2DMS.PCovMap(scan_low, scan_low.weightFunc(weights)).sampleFeats(indexList)\n",
    "        new_top_feat_low = top_feat_low[np.abs(top_feat_low[:, 0] - top_feat_low[:, 1]) >= 5.5]\n",
    "        new_top_feat_low = new_top_feat_low[np.flip(new_top_feat_low[:, 3].argsort())]\n",
    "        count = []\n",
    "        for z, j in itertools.product(new_top_feat[:top_number], new_top_feat_low[:top_number]):\n",
    "            if torch.allclose(torch.tensor([z[0] - j[0], z[1] - j[1]], dtype=torch.float),\n",
    "                              torch.tensor([0.0, 0.0], dtype=torch.float), atol=tolerance_float):\n",
    "                count.append(1)\n",
    "            if torch.allclose(torch.tensor([z[0] - j[1], z[1] - j[0]], dtype=torch.float),\n",
    "                              torch.tensor([0.0, 0.0], dtype=torch.float), atol=tolerance_float):\n",
    "                count.append(1) \n",
    "        if abs(torch.tensor(len(count) - top_number)) <= unidentified_index // 10:\n",
    "            converge_index = i // 1000\n",
    "\n",
    "    print('converge_index: ', converge_index)\n",
    "    print('weights: ', weights)\n",
    "    \n",
    "    '''# Save weights\n",
    "    if os.path.exists(os.path.join(scan_dir, 'weights.npy')):\n",
    "        os.remove(os.path.join(scan_dir, 'weights.npy'))\n",
    "    np.save(os.path.join(scan_dir, 'weights.npy'), weights)'''\n",
    "    \n",
    "    return converge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531de88b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0136a9d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T01:18:25.276492037Z",
     "start_time": "2023-06-28T01:18:25.218456954Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from typing import Tuple\n",
    "import pC2DMS\n",
    "\n",
    "def optimize_weights(dirname, filename, loss_function, first_time=False, last_time=False, opti_range=[23,100]):\n",
    "    numscan_list = [int(os.path.splitext(d.name)[0].split(' ')[0]) for d in os.scandir(os.path.join('./peptide output', dirname, filename)) if d.is_dir() and len(os.path.splitext(d.name)[0].split(' ')) == 2]\n",
    "    numscan_list.sort()\n",
    "    scan_number = numscan_list[-1]\n",
    "    scan_dir = os.path.join('./peptide output', dirname, filename, f'{numscan_list[-1]} scans')\n",
    "    weights=None\n",
    "    scan1 = pC2DMS.Scan(scan_dir)\n",
    "    if os.path.exists('weights_bayopt.npy'):\n",
    "        weights = np.load('weights_bayopt.npy')\n",
    "    if os.path.exists(os.path.join(scan_dir, 'top_indices_tic.npy')):\n",
    "        indexlist = np.load(os.path.join(scan_dir, 'top_indices_tic.npy'))\n",
    "    else:\n",
    "        cmap = pC2DMS.PCovMap(scan1, scan1.tic(), numScans=scan_number)\n",
    "        indexlist = cmap.topNfeats(3000)\n",
    "        topfeat = cmap.sampleFeatsIndex(indexlist)\n",
    "        topfeat_sorted = topfeat[np.flip(topfeat[:, 3].argsort())]\n",
    "        np.save(os.path.join(scan_dir, 'top_indices_tic.npy'), topfeat_sorted)\n",
    "    #     new_template = np.copy(cmap.array)\n",
    "    #     new_template.fill(0)\n",
    "        np.savetxt(os.path.join(scan_dir, 'top_indices_tic.csv'), topfeat_sorted, fmt = '%.2f', delimiter=',')\n",
    "    if first_time == True or last_time == True:\n",
    "        sub_start = timeit.default_timer()\n",
    "        '''logging.info(f\"Computing initial PCov for {filename}\")\n",
    "        sys.argv=[sys.argv[0], '--weight', weights, '--dirname', dirname, '--name', filename]\n",
    "        %run torch.ipynb'''\n",
    "        logging.info(f\"Computing initial loss for {filename}\")\n",
    "        sys.argv=[sys.argv[0], '--weight', weights, '--dirname', dirname, '--name', filename, \n",
    "                  '--parpath', os.path.join('./peptide output', dirname, filename), '--mode', 'w']\n",
    "        %run report.ipynb\n",
    "        loss = np.load(os.path.join('./peptide output', dirname, filename, f'{scan_number} scans/some_results.npy'), allow_pickle=True)[1:]\n",
    "        logging.info(f\"The running time of {filename}: {timeit.default_timer() - sub_start}\")\n",
    "        loss = loss[0] # * loss[1] * loss[2]\n",
    "        logging.info(f\"Weights: {weights}\")\n",
    "        logging.info(f\"Loss: {loss}\")\n",
    "\n",
    "    if not last_time:\n",
    "        logging.info(f\"Optimizing weights for {filename}\")\n",
    "        sub_start = timeit.default_timer()\n",
    "        # for epoch in np.arange(3):\n",
    "        if weights is None:\n",
    "            weights, loss = scan1.optimizeWeights(indexList=indexlist[opti_range[0]:opti_range[1]], loss_function=loss_function, scan_dir=scan_dir, weights=weights) \n",
    "        else:\n",
    "            weights, loss = scan1.optimizeWeights(indexList=indexlist[opti_range[0]:opti_range[1]], loss_function=loss_function, scan_dir=scan_dir, weights=weights) \n",
    "\n",
    "        logging.info(f\"Weights: {weights}\")\n",
    "        logging.info(f\"Loss: {loss}\")\n",
    "        '''if os.path.exists('weights_bayopt.npy'):\n",
    "            os.remove('weights_bayopt.npy')\n",
    "        np.save('weights_bayopt.npy', weights)'''\n",
    "        '''if len(weights) < scan1.scanList.shape[1] // (100 * (3-epoch)):\n",
    "                weights = np.repeat(weights, (3-epoch))'''\n",
    "        logging.info(f\"The running time of {filename}: {timeit.default_timer() - sub_start}\")\n",
    "    return weights, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "773290e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T01:18:25.276828817Z",
     "start_time": "2023-06-28T01:18:25.252079207Z"
    }
   },
   "outputs": [],
   "source": [
    "datafile_dict = {}\n",
    "\n",
    "for dirname in os.listdir('../IC/IC/raw data/'):\n",
    "    datafile_dict[dirname] = []\n",
    "    for filename in os.listdir('../IC/IC/raw data/' + str(dirname)):\n",
    "        datafile_dict[dirname].append(os.path.splitext(filename)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "411658aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T01:18:25.276981731Z",
     "start_time": "2023-06-28T01:18:25.262266349Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "weights, loss = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "036c2978",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T01:18:25.326972594Z",
     "start_time": "2023-06-28T01:18:25.271111223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['CID', '.ipynb_checkpoints', 'Test', 'HCD'])\n"
     ]
    }
   ],
   "source": [
    "print(datafile_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624441fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 10:18:19,301 - root - INFO - Optimizing weights for 20160428_2100_ME16_3+_CVscan_NCE35_Turbo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 280 to 294\n",
      "./peptide output/Test/20160428_2100_ME16_3+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "unidentified_index:  5\n",
      "weights:  [2. 0. 0. 0. 2. 0. 2. 0. 2. 2. 0. 2. 0. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 0. 0. 2. 0. 2. 2. 0. 2. 2. 0. 2. 0. 2. 2. 2.]\n",
      "|   iter    |  target   |    w_0    |    w_1    |   w_10    |   w_11    |   w_12    |   w_13    |   w_14    |   w_15    |   w_16    |   w_17    |   w_18    |   w_19    |    w_2    |   w_20    |   w_21    |   w_22    |   w_23    |   w_24    |   w_25    |   w_26    |   w_27    |   w_28    |   w_29    |    w_3    |   w_30    |   w_31    |   w_32    |   w_33    |   w_34    |   w_35    |   w_36    |   w_37    |   w_38    |    w_4    |    w_5    |    w_6    |    w_7    |    w_8    |    w_9    |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "./peptide output/Test/20160428_2100_ME16_3+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 10:40:40,405 - root - INFO - Weights: [2. 2. 0. 0. 2. 0. 0. 2. 2. 2. 0. 2. 0. 2. 0. 0. 2. 2. 0. 0. 2. 0. 0. 0.\n",
      " 2. 2. 2. 2. 0. 0. 2. 0. 0. 0. 2. 0. 2. 0. 2.]\n",
      "2023-08-16 10:40:40,407 - root - INFO - Loss: -4.0\n",
      "2023-08-16 10:40:40,407 - root - INFO - The running time of 20160428_2100_ME16_3+_CVscan_NCE35_Turbo: 1341.1053021512926\n",
      "2023-08-16 10:40:40,518 - root - INFO - Optimizing weights for 20160428_2222_ME16_2+_CVScan_NCE35_Turbo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unidentified_index:  4\n",
      "weights:  [2. 2. 0. 0. 0. 2. 2. 2. 2. 2. 0. 0. 0. 2. 0. 2. 2. 0. 2. 0. 2. 2. 0. 0.\n",
      " 2. 2. 0. 2. 2. 0. 2. 0. 0. 0. 0. 0. 2. 2. 0.]\n",
      "| \u001b[0m395      \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m |\n",
      "=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
      "./peptide output/Test/20160428_2222_ME16_2+_CVScan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "unidentified_index:  4\n",
      "weights:  [2. 0. 0. 0. 2. 0. 2. 0. 2. 2. 0. 2. 0. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 0. 0. 2. 0. 2. 2. 0. 2. 2. 0. 2. 0. 2. 2. 2.]\n",
      "|   iter    |  target   |    w_0    |    w_1    |   w_10    |   w_11    |   w_12    |   w_13    |   w_14    |   w_15    |   w_16    |   w_17    |   w_18    |   w_19    |    w_2    |   w_20    |   w_21    |   w_22    |   w_23    |   w_24    |   w_25    |   w_26    |   w_27    |   w_28    |   w_29    |    w_3    |   w_30    |   w_31    |   w_32    |   w_33    |   w_34    |   w_35    |   w_36    |   w_37    |   w_38    |    w_4    |    w_5    |    w_6    |    w_7    |    w_8    |    w_9    |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "./peptide output/Test/20160428_2100_ME16_3+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 11:00:42,149 - root - INFO - Weights: [2. 2. 0. 0. 2. 0. 0. 2. 2. 2. 0. 2. 0. 2. 0. 0. 2. 2. 0. 0. 2. 0. 0. 0.\n",
      " 2. 2. 2. 2. 0. 0. 2. 0. 0. 0. 2. 0. 2. 0. 2.]\n",
      "2023-08-16 11:00:42,150 - root - INFO - Loss: -4.0\n",
      "2023-08-16 11:00:42,151 - root - INFO - The running time of 20160428_2222_ME16_2+_CVScan_NCE35_Turbo: 1201.6313564833254\n",
      "2023-08-16 11:00:42,306 - root - INFO - Optimizing weights for 20160504_0930_ME4_3+_CVscan_NCE35_Turbo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unidentified_index:  4\n",
      "weights:  [2. 2. 2. 2. 0. 0. 2. 2. 2. 2. 0. 0. 0. 2. 2. 2. 2. 0. 2. 0. 2. 0. 0. 2.\n",
      " 2. 2. 2. 2. 2. 0. 0. 2. 0. 0. 0. 2. 2. 2. 0.]\n",
      "| \u001b[0m396      \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m |\n",
      "=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
      "./peptide output/Test/20160504_0930_ME4_3+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "unidentified_index:  3\n",
      "weights:  [2. 0. 0. 0. 2. 0. 2. 0. 2. 2. 0. 2. 0. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 0. 0. 2. 0. 2. 2. 0. 2. 2. 0. 2. 0. 2. 2. 2.]\n",
      "|   iter    |  target   |    w_0    |    w_1    |   w_10    |   w_11    |   w_12    |   w_13    |   w_14    |   w_15    |   w_16    |   w_17    |   w_18    |   w_19    |    w_2    |   w_20    |   w_21    |   w_22    |   w_23    |   w_24    |   w_25    |   w_26    |   w_27    |   w_28    |   w_29    |    w_3    |   w_30    |   w_31    |   w_32    |   w_33    |   w_34    |   w_35    |   w_36    |   w_37    |   w_38    |    w_4    |    w_5    |    w_6    |    w_7    |    w_8    |    w_9    |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "./peptide output/Test/20160428_2100_ME16_3+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 11:22:41,265 - root - INFO - Weights: [2. 2. 0. 0. 2. 0. 0. 2. 2. 2. 0. 2. 0. 2. 0. 0. 2. 2. 0. 0. 2. 0. 0. 0.\n",
      " 2. 2. 2. 2. 0. 0. 2. 0. 0. 0. 2. 0. 2. 0. 2.]\n",
      "2023-08-16 11:22:41,266 - root - INFO - Loss: -4.0\n",
      "2023-08-16 11:22:41,267 - root - INFO - The running time of 20160504_0930_ME4_3+_CVscan_NCE35_Turbo: 1318.9596207085997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unidentified_index:  4\n",
      "weights:  [0. 2. 0. 0. 0. 2. 0. 2. 2. 2. 0. 2. 0. 2. 0. 2. 0. 0. 0. 0. 2. 2. 0. 0.\n",
      " 0. 0. 0. 0. 0. 2. 0. 0. 0. 2. 0. 0. 2. 2. 2.]\n",
      "| \u001b[0m397      \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 11:22:41,510 - root - INFO - Optimizing weights for 20160511_2003_ME17_2+_CVscan_Turbo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./peptide output/Test/20160511_2003_ME17_2+_CVscan_Turbo/10000 scans\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "unidentified_index:  2\n",
      "weights:  [2. 0. 0. 0. 2. 0. 2. 0. 2. 2. 0. 2. 0. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 0. 0. 2. 0. 2. 2. 0. 2. 2. 0. 2. 0. 2. 2. 2.]\n",
      "|   iter    |  target   |    w_0    |    w_1    |   w_10    |   w_11    |   w_12    |   w_13    |   w_14    |   w_15    |   w_16    |   w_17    |   w_18    |   w_19    |    w_2    |   w_20    |   w_21    |   w_22    |   w_23    |   w_24    |   w_25    |   w_26    |   w_27    |   w_28    |   w_29    |    w_3    |   w_30    |   w_31    |   w_32    |   w_33    |   w_34    |   w_35    |   w_36    |   w_37    |   w_38    |    w_4    |    w_5    |    w_6    |    w_7    |    w_8    |    w_9    |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "./peptide output/Test/20160428_2100_ME16_3+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 11:41:47,514 - root - INFO - Weights: [2. 2. 0. 0. 2. 0. 0. 2. 2. 2. 0. 2. 0. 2. 0. 0. 2. 2. 0. 0. 2. 0. 0. 0.\n",
      " 2. 2. 2. 2. 0. 0. 2. 0. 0. 0. 2. 0. 2. 0. 2.]\n",
      "2023-08-16 11:41:47,515 - root - INFO - Loss: -4.0\n",
      "2023-08-16 11:41:47,516 - root - INFO - The running time of 20160511_2003_ME17_2+_CVscan_Turbo: 1146.0049841217697\n",
      "2023-08-16 11:41:47,627 - root - INFO - Optimizing weights for 20160602_1249_ME8_3+_CVscan_NCE35_Turbo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unidentified_index:  4\n",
      "weights:  [2. 2. 0. 0. 0. 2. 2. 2. 2. 2. 0. 0. 0. 2. 2. 2. 2. 2. 2. 0. 2. 2. 0. 2.\n",
      " 2. 2. 0. 2. 2. 0. 2. 2. 0. 0. 0. 2. 2. 2. 2.]\n",
      "| \u001b[0m398      \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
      "./peptide output/Test/20160602_1249_ME8_3+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "unidentified_index:  1\n",
      "weights:  [2. 0. 0. 0. 2. 0. 2. 0. 2. 2. 0. 2. 0. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 0. 0. 2. 0. 2. 2. 0. 2. 2. 0. 2. 0. 2. 2. 2.]\n",
      "|   iter    |  target   |    w_0    |    w_1    |   w_10    |   w_11    |   w_12    |   w_13    |   w_14    |   w_15    |   w_16    |   w_17    |   w_18    |   w_19    |    w_2    |   w_20    |   w_21    |   w_22    |   w_23    |   w_24    |   w_25    |   w_26    |   w_27    |   w_28    |   w_29    |    w_3    |   w_30    |   w_31    |   w_32    |   w_33    |   w_34    |   w_35    |   w_36    |   w_37    |   w_38    |    w_4    |    w_5    |    w_6    |    w_7    |    w_8    |    w_9    |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "./peptide output/Test/20160428_2100_ME16_3+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 13:04:47,918 - root - INFO - Weights: [2. 2. 0. 0. 2. 0. 0. 2. 2. 2. 0. 2. 0. 2. 0. 0. 2. 2. 0. 0. 2. 0. 0. 0.\n",
      " 2. 2. 2. 2. 0. 0. 2. 0. 0. 0. 2. 0. 2. 0. 2.]\n",
      "2023-08-16 13:04:47,919 - root - INFO - Loss: -4.0\n",
      "2023-08-16 13:04:47,920 - root - INFO - The running time of 20160622_1448_ME14_2+_1to2500_CVscan_NCE35: 1205.2230964265764\n",
      "2023-08-16 13:04:48,034 - root - INFO - Optimizing weights for 20160622_1514_ME14_3+_1to2500_CVscan_NCE35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unidentified_index:  4\n",
      "weights:  [0. 2. 2. 0. 0. 2. 0. 2. 2. 2. 0. 2. 0. 2. 2. 2. 2. 2. 0. 0. 2. 2. 0. 0.\n",
      " 0. 2. 2. 0. 0. 0. 2. 2. 0. 0. 0. 2. 2. 2. 0.]\n",
      "| \u001b[0m402      \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m |\n",
      "=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
      "./peptide output/Test/20160622_1514_ME14_3+_1to2500_CVscan_NCE35/10000 scans\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "unidentified_index:  6\n",
      "weights:  [2. 0. 0. 0. 2. 0. 2. 0. 2. 2. 0. 2. 0. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 0. 0. 2. 0. 2. 2. 0. 2. 2. 0. 2. 0. 2. 2. 2.]\n",
      "|   iter    |  target   |    w_0    |    w_1    |   w_10    |   w_11    |   w_12    |   w_13    |   w_14    |   w_15    |   w_16    |   w_17    |   w_18    |   w_19    |    w_2    |   w_20    |   w_21    |   w_22    |   w_23    |   w_24    |   w_25    |   w_26    |   w_27    |   w_28    |   w_29    |    w_3    |   w_30    |   w_31    |   w_32    |   w_33    |   w_34    |   w_35    |   w_36    |   w_37    |   w_38    |    w_4    |    w_5    |    w_6    |    w_7    |    w_8    |    w_9    |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "./peptide output/Test/20160428_2100_ME16_3+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 13:26:49,482 - root - INFO - Weights: [2. 2. 0. 0. 2. 0. 0. 2. 2. 2. 0. 2. 0. 2. 0. 0. 2. 2. 0. 0. 2. 0. 0. 0.\n",
      " 2. 2. 2. 2. 0. 0. 2. 0. 0. 0. 2. 0. 2. 0. 2.]\n",
      "2023-08-16 13:26:49,484 - root - INFO - Loss: -4.0\n",
      "2023-08-16 13:26:49,484 - root - INFO - The running time of 20160622_1514_ME14_3+_1to2500_CVscan_NCE35: 1321.4497544746846\n",
      "2023-08-16 13:26:49,571 - root - INFO - Optimizing weights for 20160708_1747_UN14_2+_0,01mM_CVscan_Turbo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unidentified_index:  4\n",
      "weights:  [0. 2. 2. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 2. 2. 2. 2. 0. 2. 0. 2. 2. 0. 2.\n",
      " 0. 2. 2. 2. 2. 2. 2. 2. 0. 2. 0. 0. 0. 0. 2.]\n",
      "| \u001b[0m403      \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
      "./peptide output/Test/20160708_1747_UN14_2+_0,01mM_CVscan_Turbo/10000 scans\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "unidentified_index:  1\n",
      "weights:  [2. 0. 0. 0. 2. 0. 2. 0. 2. 2. 0. 2. 0. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 0. 0. 2. 0. 2. 2. 0. 2. 2. 0. 2. 0. 2. 2. 2.]\n",
      "|   iter    |  target   |    w_0    |    w_1    |   w_10    |   w_11    |   w_12    |   w_13    |   w_14    |   w_15    |   w_16    |   w_17    |   w_18    |   w_19    |    w_2    |   w_20    |   w_21    |   w_22    |   w_23    |   w_24    |   w_25    |   w_26    |   w_27    |   w_28    |   w_29    |    w_3    |   w_30    |   w_31    |   w_32    |   w_33    |   w_34    |   w_35    |   w_36    |   w_37    |   w_38    |    w_4    |    w_5    |    w_6    |    w_7    |    w_8    |    w_9    |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/general/user/ww1922/home/anaconda3/envs/test1/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-12. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./peptide output/Test/20160428_2100_ME16_3+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 14:03:39,821 - root - INFO - Weights: [2. 2. 0. 0. 2. 0. 0. 2. 2. 2. 0. 2. 0. 2. 0. 0. 2. 2. 0. 0. 2. 0. 0. 0.\n",
      " 2. 2. 2. 2. 0. 0. 2. 0. 0. 0. 2. 0. 2. 0. 2.]\n",
      "2023-08-16 14:03:39,822 - root - INFO - Loss: -4.0\n",
      "2023-08-16 14:03:39,823 - root - INFO - The running time of PH8_2+_CVscan_NCE35_Turbo_20160505_1308: 1122.6507335416973\n",
      "2023-08-16 14:03:39,944 - root - INFO - Optimizing weights for 20160428_2100_ME16_3+_CVscan_NCE35_Turbo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unidentified_index:  4\n",
      "weights:  [2. 2. 2. 0. 2. 0. 2. 0. 2. 2. 0. 2. 0. 2. 2. 2. 0. 0. 0. 2. 2. 2. 0. 2.\n",
      " 2. 2. 2. 2. 2. 0. 2. 2. 0. 0. 0. 0. 0. 0. 0.]\n",
      "| \u001b[0m405      \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m |\n",
      "=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
      "epoch: 294 to 308\n",
      "./peptide output/Test/20160428_2100_ME16_3+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "unidentified_index:  5\n",
      "weights:  [2. 0. 0. 0. 2. 0. 2. 0. 2. 2. 0. 2. 0. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 0. 0. 2. 0. 2. 2. 0. 2. 2. 0. 2. 0. 2. 2. 2.]\n",
      "|   iter    |  target   |    w_0    |    w_1    |   w_10    |   w_11    |   w_12    |   w_13    |   w_14    |   w_15    |   w_16    |   w_17    |   w_18    |   w_19    |    w_2    |   w_20    |   w_21    |   w_22    |   w_23    |   w_24    |   w_25    |   w_26    |   w_27    |   w_28    |   w_29    |    w_3    |   w_30    |   w_31    |   w_32    |   w_33    |   w_34    |   w_35    |   w_36    |   w_37    |   w_38    |    w_4    |    w_5    |    w_6    |    w_7    |    w_8    |    w_9    |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "./peptide output/Test/20160428_2100_ME16_3+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 14:25:50,710 - root - INFO - Weights: [2. 2. 0. 0. 2. 0. 0. 2. 2. 2. 0. 2. 0. 2. 0. 0. 2. 2. 0. 0. 2. 0. 0. 0.\n",
      " 2. 2. 2. 2. 0. 0. 2. 0. 0. 0. 2. 0. 2. 0. 2.]\n",
      "2023-08-16 14:25:50,711 - root - INFO - Loss: -4.0\n",
      "2023-08-16 14:25:50,711 - root - INFO - The running time of 20160428_2100_ME16_3+_CVscan_NCE35_Turbo: 1330.7666233237833\n",
      "2023-08-16 14:25:50,823 - root - INFO - Optimizing weights for 20160428_2222_ME16_2+_CVScan_NCE35_Turbo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unidentified_index:  4\n",
      "weights:  [0. 2. 0. 0. 2. 0. 2. 0. 2. 2. 0. 2. 0. 2. 2. 2. 0. 0. 0. 0. 2. 2. 0. 0.\n",
      " 2. 2. 2. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 2.]\n",
      "| \u001b[0m406      \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
      "./peptide output/Test/20160428_2222_ME16_2+_CVScan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 300\n",
      "unidentified_index:  4\n",
      "weights:  [2. 0. 0. 0. 2. 0. 2. 0. 2. 2. 0. 2. 0. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 0. 0. 2. 0. 2. 2. 0. 2. 2. 0. 2. 0. 2. 2. 2.]\n",
      "|   iter    |  target   |    w_0    |    w_1    |   w_10    |   w_11    |   w_12    |   w_13    |   w_14    |   w_15    |   w_16    |   w_17    |   w_18    |   w_19    |    w_2    |   w_20    |   w_21    |   w_22    |   w_23    |   w_24    |   w_25    |   w_26    |   w_27    |   w_28    |   w_29    |    w_3    |   w_30    |   w_31    |   w_32    |   w_33    |   w_34    |   w_35    |   w_36    |   w_37    |   w_38    |    w_4    |    w_5    |    w_6    |    w_7    |    w_8    |    w_9    |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "./peptide output/Test/20160428_2100_ME16_3+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 14:45:58,695 - root - INFO - Weights: [2. 2. 0. 0. 2. 0. 0. 2. 2. 2. 0. 2. 0. 2. 0. 0. 2. 2. 0. 0. 2. 0. 0. 0.\n",
      " 2. 2. 2. 2. 0. 0. 2. 0. 0. 0. 2. 0. 2. 0. 2.]\n",
      "2023-08-16 14:45:58,696 - root - INFO - Loss: -4.0\n",
      "2023-08-16 14:45:58,696 - root - INFO - The running time of 20160428_2222_ME16_2+_CVScan_NCE35_Turbo: 1207.872574975714\n",
      "2023-08-16 14:45:58,851 - root - INFO - Optimizing weights for 20160504_0930_ME4_3+_CVscan_NCE35_Turbo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unidentified_index:  4\n",
      "weights:  [0. 2. 0. 0. 0. 2. 0. 0. 2. 2. 0. 2. 0. 2. 2. 0. 2. 2. 0. 2. 2. 0. 0. 2.\n",
      " 2. 2. 2. 2. 0. 2. 0. 2. 2. 2. 0. 2. 0. 2. 0.]\n",
      "| \u001b[0m407      \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m |\n",
      "=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
      "./peptide output/Test/20160504_0930_ME4_3+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "unidentified_index:  3\n",
      "weights:  [2. 0. 0. 0. 2. 0. 2. 0. 2. 2. 0. 2. 0. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 0. 0. 2. 0. 2. 2. 0. 2. 2. 0. 2. 0. 2. 2. 2.]\n",
      "|   iter    |  target   |    w_0    |    w_1    |   w_10    |   w_11    |   w_12    |   w_13    |   w_14    |   w_15    |   w_16    |   w_17    |   w_18    |   w_19    |    w_2    |   w_20    |   w_21    |   w_22    |   w_23    |   w_24    |   w_25    |   w_26    |   w_27    |   w_28    |   w_29    |    w_3    |   w_30    |   w_31    |   w_32    |   w_33    |   w_34    |   w_35    |   w_36    |   w_37    |   w_38    |    w_4    |    w_5    |    w_6    |    w_7    |    w_8    |    w_9    |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "./peptide output/Test/20160428_2100_ME16_3+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 15:08:24,774 - root - INFO - Weights: [2. 2. 0. 0. 2. 0. 0. 2. 2. 2. 0. 2. 0. 2. 0. 0. 2. 2. 0. 0. 2. 0. 0. 0.\n",
      " 2. 2. 2. 2. 0. 0. 2. 0. 0. 0. 2. 0. 2. 0. 2.]\n",
      "2023-08-16 15:08:24,775 - root - INFO - Loss: -4.0\n",
      "2023-08-16 15:08:24,775 - root - INFO - The running time of 20160504_0930_ME4_3+_CVscan_NCE35_Turbo: 1345.922848423943\n",
      "2023-08-16 15:08:24,867 - root - INFO - Optimizing weights for 20160511_2003_ME17_2+_CVscan_Turbo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unidentified_index:  4\n",
      "weights:  [0. 2. 0. 0. 0. 0. 0. 2. 2. 2. 0. 2. 0. 2. 2. 0. 0. 2. 2. 0. 2. 2. 0. 0.\n",
      " 0. 0. 0. 2. 2. 0. 0. 0. 0. 2. 0. 0. 2. 2. 0.]\n",
      "| \u001b[0m408      \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m |\n",
      "=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
      "./peptide output/Test/20160511_2003_ME17_2+_CVscan_Turbo/10000 scans\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "unidentified_index:  2\n",
      "weights:  [2. 0. 0. 0. 2. 0. 2. 0. 2. 2. 0. 2. 0. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 0. 0. 2. 0. 2. 2. 0. 2. 2. 0. 2. 0. 2. 2. 2.]\n",
      "|   iter    |  target   |    w_0    |    w_1    |   w_10    |   w_11    |   w_12    |   w_13    |   w_14    |   w_15    |   w_16    |   w_17    |   w_18    |   w_19    |    w_2    |   w_20    |   w_21    |   w_22    |   w_23    |   w_24    |   w_25    |   w_26    |   w_27    |   w_28    |   w_29    |    w_3    |   w_30    |   w_31    |   w_32    |   w_33    |   w_34    |   w_35    |   w_36    |   w_37    |   w_38    |    w_4    |    w_5    |    w_6    |    w_7    |    w_8    |    w_9    |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "./peptide output/Test/20160428_2100_ME16_3+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 15:27:55,600 - root - INFO - Weights: [2. 2. 0. 0. 2. 0. 0. 2. 2. 2. 0. 2. 0. 2. 0. 0. 2. 2. 0. 0. 2. 0. 0. 0.\n",
      " 2. 2. 2. 2. 0. 0. 2. 0. 0. 0. 2. 0. 2. 0. 2.]\n",
      "2023-08-16 15:27:55,601 - root - INFO - Loss: -4.0\n",
      "2023-08-16 15:27:55,601 - root - INFO - The running time of 20160511_2003_ME17_2+_CVscan_Turbo: 1170.7331245522946\n",
      "2023-08-16 15:27:55,714 - root - INFO - Optimizing weights for 20160602_1249_ME8_3+_CVscan_NCE35_Turbo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unidentified_index:  4\n",
      "weights:  [0. 2. 2. 2. 0. 2. 2. 2. 2. 2. 0. 2. 0. 2. 2. 2. 0. 0. 2. 0. 2. 2. 0. 0.\n",
      " 0. 0. 0. 2. 2. 0. 2. 0. 2. 2. 0. 0. 2. 2. 0.]\n",
      "| \u001b[0m409      \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m |\n",
      "=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
      "./peptide output/Test/20160602_1249_ME8_3+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "unidentified_index:  1\n",
      "weights:  [2. 0. 0. 0. 2. 0. 2. 0. 2. 2. 0. 2. 0. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 0. 0. 2. 0. 2. 2. 0. 2. 2. 0. 2. 0. 2. 2. 2.]\n",
      "|   iter    |  target   |    w_0    |    w_1    |   w_10    |   w_11    |   w_12    |   w_13    |   w_14    |   w_15    |   w_16    |   w_17    |   w_18    |   w_19    |    w_2    |   w_20    |   w_21    |   w_22    |   w_23    |   w_24    |   w_25    |   w_26    |   w_27    |   w_28    |   w_29    |    w_3    |   w_30    |   w_31    |   w_32    |   w_33    |   w_34    |   w_35    |   w_36    |   w_37    |   w_38    |    w_4    |    w_5    |    w_6    |    w_7    |    w_8    |    w_9    |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rds/general/user/ww1922/home/anaconda3/envs/test1/lib/python3.11/site-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified lower bound 1e-12. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./peptide output/Test/20160428_2100_ME16_3+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 15:50:37,132 - root - INFO - Weights: [2. 2. 0. 0. 2. 0. 0. 2. 2. 2. 0. 2. 0. 2. 0. 0. 2. 2. 0. 0. 2. 0. 0. 0.\n",
      " 2. 2. 2. 2. 0. 0. 2. 0. 0. 0. 2. 0. 2. 0. 2.]\n",
      "2023-08-16 15:50:37,133 - root - INFO - Loss: -4.0\n",
      "2023-08-16 15:50:37,134 - root - INFO - The running time of 20160602_1249_ME8_3+_CVscan_NCE35_Turbo: 1361.4190431889147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unidentified_index:  4\n",
      "weights:  [0. 2. 0. 2. 2. 0. 2. 0. 2. 2. 0. 2. 0. 2. 0. 0. 0. 2. 0. 0. 2. 0. 0. 2.\n",
      " 2. 2. 0. 2. 2. 0. 0. 2. 0. 2. 0. 2. 0. 0. 0.]\n",
      "| \u001b[0m410      \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m |\n",
      "=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 15:50:37,402 - root - INFO - Optimizing weights for 20160603_1005_ME9_2+_CVscan_NCE35_Turbo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./peptide output/Test/20160603_1005_ME9_2+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "unidentified_index:  1\n",
      "weights:  [2. 0. 0. 0. 2. 0. 2. 0. 2. 2. 0. 2. 0. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 0. 0. 2. 0. 2. 2. 0. 2. 2. 0. 2. 0. 2. 2. 2.]\n",
      "|   iter    |  target   |    w_0    |    w_1    |   w_10    |   w_11    |   w_12    |   w_13    |   w_14    |   w_15    |   w_16    |   w_17    |   w_18    |   w_19    |    w_2    |   w_20    |   w_21    |   w_22    |   w_23    |   w_24    |   w_25    |   w_26    |   w_27    |   w_28    |   w_29    |    w_3    |   w_30    |   w_31    |   w_32    |   w_33    |   w_34    |   w_35    |   w_36    |   w_37    |   w_38    |    w_4    |    w_5    |    w_6    |    w_7    |    w_8    |    w_9    |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "./peptide output/Test/20160428_2100_ME16_3+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 16:10:24,671 - root - INFO - Weights: [2. 2. 0. 0. 2. 0. 0. 2. 2. 2. 0. 2. 0. 2. 0. 0. 2. 2. 0. 0. 2. 0. 0. 0.\n",
      " 2. 2. 2. 2. 0. 0. 2. 0. 0. 0. 2. 0. 2. 0. 2.]\n",
      "2023-08-16 16:10:24,672 - root - INFO - Loss: -4.0\n",
      "2023-08-16 16:10:24,673 - root - INFO - The running time of 20160603_1005_ME9_2+_CVscan_NCE35_Turbo: 1187.2695826999843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unidentified_index:  4\n",
      "weights:  [0. 2. 2. 0. 0. 2. 0. 2. 2. 2. 0. 0. 0. 2. 2. 0. 0. 2. 0. 2. 2. 0. 2. 0.\n",
      " 0. 2. 2. 0. 2. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0.]\n",
      "| \u001b[0m411      \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m |\n",
      "=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 16:10:24,949 - root - INFO - Optimizing weights for 20160603_1040_ME9_3+_CVscan_NCE35_Turbo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./peptide output/Test/20160603_1040_ME9_3+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "unidentified_index:  4\n",
      "weights:  [2. 0. 0. 0. 2. 0. 2. 0. 2. 2. 0. 2. 0. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 0. 0. 2. 0. 2. 2. 0. 2. 2. 0. 2. 0. 2. 2. 2.]\n",
      "|   iter    |  target   |    w_0    |    w_1    |   w_10    |   w_11    |   w_12    |   w_13    |   w_14    |   w_15    |   w_16    |   w_17    |   w_18    |   w_19    |    w_2    |   w_20    |   w_21    |   w_22    |   w_23    |   w_24    |   w_25    |   w_26    |   w_27    |   w_28    |   w_29    |    w_3    |   w_30    |   w_31    |   w_32    |   w_33    |   w_34    |   w_35    |   w_36    |   w_37    |   w_38    |    w_4    |    w_5    |    w_6    |    w_7    |    w_8    |    w_9    |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "./peptide output/Test/20160428_2100_ME16_3+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 16:30:57,934 - root - INFO - Weights: [2. 2. 0. 0. 2. 0. 0. 2. 2. 2. 0. 2. 0. 2. 0. 0. 2. 2. 0. 0. 2. 0. 0. 0.\n",
      " 2. 2. 2. 2. 0. 0. 2. 0. 0. 0. 2. 0. 2. 0. 2.]\n",
      "2023-08-16 16:30:57,935 - root - INFO - Loss: -4.0\n",
      "2023-08-16 16:30:57,936 - root - INFO - The running time of 20160603_1040_ME9_3+_CVscan_NCE35_Turbo: 1232.9856652207673\n",
      "2023-08-16 16:30:58,050 - root - INFO - Optimizing weights for 20160622_1448_ME14_2+_1to2500_CVscan_NCE35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unidentified_index:  4\n",
      "weights:  [0. 2. 2. 2. 0. 0. 2. 2. 2. 2. 0. 2. 0. 2. 0. 0. 0. 0. 0. 0. 2. 2. 0. 0.\n",
      " 2. 2. 0. 2. 0. 2. 2. 0. 2. 0. 2. 2. 2. 2. 0.]\n",
      "| \u001b[0m412      \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m |\n",
      "=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
      "./peptide output/Test/20160622_1448_ME14_2+_1to2500_CVscan_NCE35/10000 scans\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "unidentified_index:  1\n",
      "weights:  [2. 0. 0. 0. 2. 0. 2. 0. 2. 2. 0. 2. 0. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 0. 0. 2. 0. 2. 2. 0. 2. 2. 0. 2. 0. 2. 2. 2.]\n",
      "|   iter    |  target   |    w_0    |    w_1    |   w_10    |   w_11    |   w_12    |   w_13    |   w_14    |   w_15    |   w_16    |   w_17    |   w_18    |   w_19    |    w_2    |   w_20    |   w_21    |   w_22    |   w_23    |   w_24    |   w_25    |   w_26    |   w_27    |   w_28    |   w_29    |    w_3    |   w_30    |   w_31    |   w_32    |   w_33    |   w_34    |   w_35    |   w_36    |   w_37    |   w_38    |    w_4    |    w_5    |    w_6    |    w_7    |    w_8    |    w_9    |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "./peptide output/Test/20160428_2100_ME16_3+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 16:50:41,498 - root - INFO - Weights: [2. 2. 0. 0. 2. 0. 0. 2. 2. 2. 0. 2. 0. 2. 0. 0. 2. 2. 0. 0. 2. 0. 0. 0.\n",
      " 2. 2. 2. 2. 0. 0. 2. 0. 0. 0. 2. 0. 2. 0. 2.]\n",
      "2023-08-16 16:50:41,499 - root - INFO - Loss: -4.0\n",
      "2023-08-16 16:50:41,499 - root - INFO - The running time of 20160622_1448_ME14_2+_1to2500_CVscan_NCE35: 1183.4486548956484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unidentified_index:  4\n",
      "weights:  [2. 2. 2. 0. 0. 0. 0. 2. 2. 2. 0. 0. 0. 2. 2. 0. 2. 0. 2. 2. 2. 0. 0. 2.\n",
      " 2. 2. 2. 2. 2. 0. 0. 0. 0. 2. 0. 2. 2. 0. 0.]\n",
      "| \u001b[0m413      \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m |\n",
      "=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 16:50:41,614 - root - INFO - Optimizing weights for 20160622_1514_ME14_3+_1to2500_CVscan_NCE35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./peptide output/Test/20160622_1514_ME14_3+_1to2500_CVscan_NCE35/10000 scans\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "unidentified_index:  6\n",
      "weights:  [2. 0. 0. 0. 2. 0. 2. 0. 2. 2. 0. 2. 0. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 0. 0. 2. 0. 2. 2. 0. 2. 2. 0. 2. 0. 2. 2. 2.]\n",
      "|   iter    |  target   |    w_0    |    w_1    |   w_10    |   w_11    |   w_12    |   w_13    |   w_14    |   w_15    |   w_16    |   w_17    |   w_18    |   w_19    |    w_2    |   w_20    |   w_21    |   w_22    |   w_23    |   w_24    |   w_25    |   w_26    |   w_27    |   w_28    |   w_29    |    w_3    |   w_30    |   w_31    |   w_32    |   w_33    |   w_34    |   w_35    |   w_36    |   w_37    |   w_38    |    w_4    |    w_5    |    w_6    |    w_7    |    w_8    |    w_9    |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "./peptide output/Test/20160428_2100_ME16_3+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 17:12:54,947 - root - INFO - Weights: [2. 2. 0. 0. 2. 0. 0. 2. 2. 2. 0. 2. 0. 2. 0. 0. 2. 2. 0. 0. 2. 0. 0. 0.\n",
      " 2. 2. 2. 2. 0. 0. 2. 0. 0. 0. 2. 0. 2. 0. 2.]\n",
      "2023-08-16 17:12:54,948 - root - INFO - Loss: -4.0\n",
      "2023-08-16 17:12:54,949 - root - INFO - The running time of 20160622_1514_ME14_3+_1to2500_CVscan_NCE35: 1333.3337427657098\n",
      "2023-08-16 17:12:55,036 - root - INFO - Optimizing weights for 20160708_1747_UN14_2+_0,01mM_CVscan_Turbo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unidentified_index:  4\n",
      "weights:  [2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 0. 2. 0. 2. 0. 2. 0. 2. 0. 0. 2. 2. 2. 2.\n",
      " 2. 2. 0. 2. 0. 2. 0. 0. 2. 0. 0. 2. 0. 0. 2.]\n",
      "| \u001b[0m414      \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m |\n",
      "=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
      "./peptide output/Test/20160708_1747_UN14_2+_0,01mM_CVscan_Turbo/10000 scans\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "unidentified_index:  1\n",
      "weights:  [2. 0. 0. 0. 2. 0. 2. 0. 2. 2. 0. 2. 0. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 0. 0. 2. 0. 2. 2. 0. 2. 2. 0. 2. 0. 2. 2. 2.]\n",
      "|   iter    |  target   |    w_0    |    w_1    |   w_10    |   w_11    |   w_12    |   w_13    |   w_14    |   w_15    |   w_16    |   w_17    |   w_18    |   w_19    |    w_2    |   w_20    |   w_21    |   w_22    |   w_23    |   w_24    |   w_25    |   w_26    |   w_27    |   w_28    |   w_29    |    w_3    |   w_30    |   w_31    |   w_32    |   w_33    |   w_34    |   w_35    |   w_36    |   w_37    |   w_38    |    w_4    |    w_5    |    w_6    |    w_7    |    w_8    |    w_9    |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "./peptide output/Test/20160428_2100_ME16_3+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 17:32:35,714 - root - INFO - Weights: [2. 2. 0. 0. 2. 0. 0. 2. 2. 2. 0. 2. 0. 2. 0. 0. 2. 2. 0. 0. 2. 0. 0. 0.\n",
      " 2. 2. 2. 2. 0. 0. 2. 0. 0. 0. 2. 0. 2. 0. 2.]\n",
      "2023-08-16 17:32:35,716 - root - INFO - Loss: -4.0\n",
      "2023-08-16 17:32:35,716 - root - INFO - The running time of 20160708_1747_UN14_2+_0,01mM_CVscan_Turbo: 1180.67930621095\n",
      "2023-08-16 17:32:35,842 - root - INFO - Optimizing weights for PH8_2+_CVscan_NCE35_Turbo_20160505_1308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unidentified_index:  4\n",
      "weights:  [0. 2. 2. 2. 0. 0. 0. 0. 2. 2. 0. 2. 0. 2. 2. 2. 0. 0. 0. 0. 2. 2. 0. 2.\n",
      " 0. 2. 2. 0. 2. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0.]\n",
      "| \u001b[0m415      \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m |\n",
      "=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
      "./peptide output/Test/PH8_2+_CVscan_NCE35_Turbo_20160505_1308/10000 scans\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "unidentified_index:  1\n",
      "weights:  [2. 0. 0. 0. 2. 0. 2. 0. 2. 2. 0. 2. 0. 0. 2. 2. 2. 2. 0. 2. 2. 2. 2. 2.\n",
      " 0. 0. 2. 0. 2. 2. 0. 2. 2. 0. 2. 0. 2. 2. 2.]\n",
      "|   iter    |  target   |    w_0    |    w_1    |   w_10    |   w_11    |   w_12    |   w_13    |   w_14    |   w_15    |   w_16    |   w_17    |   w_18    |   w_19    |    w_2    |   w_20    |   w_21    |   w_22    |   w_23    |   w_24    |   w_25    |   w_26    |   w_27    |   w_28    |   w_29    |    w_3    |   w_30    |   w_31    |   w_32    |   w_33    |   w_34    |   w_35    |   w_36    |   w_37    |   w_38    |    w_4    |    w_5    |    w_6    |    w_7    |    w_8    |    w_9    |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "./peptide output/Test/20160428_2100_ME16_3+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-16 17:51:30,306 - root - INFO - Weights: [2. 2. 0. 0. 2. 0. 0. 2. 2. 2. 0. 2. 0. 2. 0. 0. 2. 2. 0. 0. 2. 0. 0. 0.\n",
      " 2. 2. 2. 2. 0. 0. 2. 0. 0. 0. 2. 0. 2. 0. 2.]\n",
      "2023-08-16 17:51:30,307 - root - INFO - Loss: -4.0\n",
      "2023-08-16 17:51:30,308 - root - INFO - The running time of PH8_2+_CVscan_NCE35_Turbo_20160505_1308: 1134.464910723269\n",
      "2023-08-16 17:51:30,428 - root - INFO - Optimizing weights for 20160428_2100_ME16_3+_CVscan_NCE35_Turbo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unidentified_index:  4\n",
      "weights:  [0. 2. 2. 0. 0. 2. 0. 0. 2. 2. 0. 0. 0. 2. 2. 2. 2. 2. 0. 0. 2. 2. 2. 2.\n",
      " 2. 2. 0. 2. 2. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0.]\n",
      "| \u001b[0m416      \u001b[0m | \u001b[0m4.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m2.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m0.0      \u001b[0m |\n",
      "=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================\n",
      "epoch: 308 to 322\n",
      "./peptide output/Test/20160428_2100_ME16_3+_CVscan_NCE35_Turbo/10000 scans\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 1000):\n",
    "    print(\"epoch: \" + str(int(i * 14)) + \" to \" + str(int(i * 12 + 12)))\n",
    "    weights['20160428_2100_ME16_3+_CVscan_NCE35_Turbo'], loss['20160428_2100_ME16_3+_CVscan_NCE35_Turbo'] = optimize_weights(list(datafile_dict)[2], '20160428_2100_ME16_3+_CVscan_NCE35_Turbo', loss_function=loss_func2,  opti_range=[66,366])\n",
    "    # weights['7255_2d-PC-MS_10pmol-ul_1AGC_0-7quadiso_CID'], loss['7255_2d-PC-MS_10pmol-ul_1AGC_0-7quadiso_CID'] = optimize_weights(list(datafile_dict)[2], '7255_2d-PC-MS_10pmol-ul_1AGC_0-7quadiso_CID', loss_function=loss_func2,  opti_range=[23,323])\n",
    "    # weights['6727_2d-PC-MS_1pmol-ul_1AGC_0-7quadiso_HCD'], loss['6727_2d-PC-MS_1pmol-ul_1AGC_0-7quadiso_HCD'] = optimize_weights(list(datafile_dict)[2], '6727_2d-PC-MS_1pmol-ul_1AGC_0-7quadiso_HCD', loss_function=loss_func2,  opti_range=[8,308])\n",
    "    weights['20160428_2222_ME16_2+_CVScan_NCE35_Turbo'], loss['20160428_2222_ME16_2+_CVScan_NCE35_Turbo'] = optimize_weights(list(datafile_dict)[2], '20160428_2222_ME16_2+_CVScan_NCE35_Turbo', loss_function=loss_func2,  opti_range=[28, 328])\n",
    "    weights['20160504_0930_ME4_3+_CVscan_NCE35_Turbo'], loss['20160504_0930_ME4_3+_CVscan_NCE35_Turbo'] = optimize_weights(list(datafile_dict)[2], '20160504_0930_ME4_3+_CVscan_NCE35_Turbo', loss_function=loss_func2,  opti_range=[56, 356])\n",
    "    weights['20160511_2003_ME17_2+_CVscan_Turbo'], loss['20160511_2003_ME17_2+_CVscan_Turbo'] = optimize_weights(list(datafile_dict)[2], '20160511_2003_ME17_2+_CVscan_Turbo', loss_function=loss_func2,  opti_range=[40, 340])\n",
    "    weights['20160602_1249_ME8_3+_CVscan_NCE35_Turbo'], loss['20160602_1249_ME8_3+_CVscan_NCE35_Turbo'] = optimize_weights(list(datafile_dict)[2], '20160602_1249_ME8_3+_CVscan_NCE35_Turbo', loss_function=loss_func2,  opti_range=[48, 348])\n",
    "    weights['20160603_1005_ME9_2+_CVscan_NCE35_Turbo'], loss['20160603_1005_ME9_2+_CVscan_NCE35_Turbo'] = optimize_weights(list(datafile_dict)[2], '20160603_1005_ME9_2+_CVscan_NCE35_Turbo', loss_function=loss_func2,  opti_range=[23, 323])\n",
    "    weights['20160603_1040_ME9_3+_CVscan_NCE35_Turbo'], loss['20160603_1040_ME9_3+_CVscan_NCE35_Turbo'] = optimize_weights(list(datafile_dict)[2], '20160603_1040_ME9_3+_CVscan_NCE35_Turbo', loss_function=loss_func2,  opti_range=[34, 334])\n",
    "    weights['20160622_1448_ME14_2+_1to2500_CVscan_NCE35'], loss['20160622_1448_ME14_2+_1to2500_CVscan_NCE35'] = optimize_weights(list(datafile_dict)[2], '20160622_1448_ME14_2+_1to2500_CVscan_NCE35', loss_function=loss_func2, opti_range=[33, 333])\n",
    "    weights['20160622_1514_ME14_3+_1to2500_CVscan_NCE35'], loss['20160622_1514_ME14_3+_1to2500_CVscan_NCE35'] = optimize_weights(list(datafile_dict)[2], '20160622_1514_ME14_3+_1to2500_CVscan_NCE35', loss_function=loss_func2, opti_range=[55, 355])\n",
    "    # weights['20160629_1738_ME15_3+_CVscan_NCE35'], loss['20160629_1738_ME15_3+_CVscan_NCE35'] = optimize_weights(list(datafile_dict)[2], '20160629_1738_ME15_3+_CVscan_NCE35', loss_function=loss_func2, opti_range=[27, 327])\n",
    "    weights['20160708_1747_UN14_2+_0,01mM_CVscan_Turbo'], loss['20160708_1747_UN14_2+_0,01mM_CVscan_Turbo'] = optimize_weights(list(datafile_dict)[2], '20160708_1747_UN14_2+_0,01mM_CVscan_Turbo', loss_function=loss_func2, opti_range=[27, 327])\n",
    "    weights['PH8_2+_CVscan_NCE35_Turbo_20160505_1308'], loss['PH8_2+_CVscan_NCE35_Turbo_20160505_1308'] = optimize_weights(list(datafile_dict)[2], 'PH8_2+_CVscan_NCE35_Turbo_20160505_1308', loss_function=loss_func2, opti_range=[20, 320])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c74c5633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def convert_to_1d_list(lst_2d):\n",
    "    return [item for sublist in lst_2d for item in sublist]\n",
    "\n",
    "# Read the data from the file\n",
    "with open('weights.txt', 'r') as file:\n",
    "    data = file.readlines()\n",
    "\n",
    "unidentified_peaks = []\n",
    "weights = []\n",
    "\n",
    "# Extract the 'Number of unidentified peaks' and 'Weights' data\n",
    "for line in data:\n",
    "    if line.startswith('Number of unidentified peaks:'):\n",
    "        unidentified_peaks.append([int(x) for x in line.split('[')[1].split(']')[0].strip().split(',')])\n",
    "\n",
    "firstline = data[0]\n",
    "weight = []\n",
    "for line in data:\n",
    "    if line.startswith('Weights:'):\n",
    "        weight.append([float(x) for x in line.split('[')[1].strip().split()])\n",
    "        firstline = line\n",
    "    elif line.startswith('|'):\n",
    "        firstline = line\n",
    "        weight = convert_to_1d_list(weight)\n",
    "        weights.append(weight)\n",
    "        weight = []\n",
    "    else:\n",
    "        if firstline.startswith('Weights:'):\n",
    "            if ']' in line:\n",
    "                line = line.split(']')[0]  # Remove the closing square bracket if present\n",
    "            weight.append([float(x) for x in line.strip().split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "edd31e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.5075366232265835\n",
      "Mean Absolute Error (MAE): 0.5716404360082807\n",
      "R-squared (R2): -1.314628293764832\n",
      "Inferred weights: [[0.78136551 0.55991325 0.38953968 1.17144206 1.7280392  1.41160309\n",
      "  0.7367098  0.50568374 1.2368116  0.50839384 0.08375496 1.09107162\n",
      "  0.81373969 0.94177423 1.14768979 1.2541851  1.49812077 1.09032953\n",
      "  0.93572582 0.95428169 0.77798594 0.62427191 0.05172322 1.00278769\n",
      "  1.15315906 0.06058991 0.56622675 0.70896753 0.32560625 1.20539019\n",
      "  0.55439145 0.95871153 0.86254659 0.59045698 0.53019357 0.56329984\n",
      "  0.97629813 1.2658647  0.89319855 1.27864025 0.3694841  2.05249244\n",
      "  0.55211351 1.30889555 1.26396847 0.78141277 1.3252244  1.61221797\n",
      "  0.63785475 0.74358429 0.88887083 1.70667033 1.69263916 0.70931013\n",
      "  1.00956138 0.980615   0.73625802 1.05577139 1.3413735  0.71028473\n",
      "  0.93015966 1.18033695 0.48946416 0.75422201 0.69985193 0.52413704\n",
      "  0.05455846 0.87979785 1.09858879 0.53437486 1.46856541 1.61117002\n",
      "  1.03077104 1.33974688 0.74895944 0.55188431 1.37146052 1.47781093\n",
      "  1.74333906 1.01086118 0.56322457 1.11701844 1.10513431 1.45797611\n",
      "  0.92896571 0.41569472 1.40264455 0.68579877 1.01298011 0.99592447\n",
      "  1.11912809 0.93236538 0.73879908 0.91641327 1.01413258 1.05110397\n",
      "  0.68193595 0.78488048 0.9891729  1.35024866 0.2804118  0.98596387\n",
      "  1.49462814 0.91902787 0.0444935  0.94563258 1.63422014 1.31482516\n",
      "  0.09422707 0.98635211 1.03578708 1.4355857  0.97274469 0.87204414\n",
      "  0.48734955 1.59075048 1.3718336  1.04451106 1.03211334 0.94293745\n",
      "  2.06154334 0.51306971 0.48121321 1.35452272 1.71700351 1.59147706\n",
      "  0.89903245 0.86096743 1.29359562 0.86461101 1.13616527 1.36733177\n",
      "  0.61661238 1.0413519  1.33808162]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Convert data to numpy arrays\n",
    "unidentified_peaks = np.array(unidentified_peaks)\n",
    "weights = np.array(weights)\n",
    "\n",
    "# Create polynomial features\n",
    "poly_features = PolynomialFeatures(degree=4)\n",
    "X_poly = poly_features.fit_transform(unidentified_peaks)\n",
    "\n",
    "# Train the model using cross-validation\n",
    "model = ElasticNet(alpha=5, l1_ratio=0.9, max_iter=10000)  # Set appropriate alpha and l1_ratio values, increase max_iter\n",
    "kf = KFold(n_splits=10, random_state=100, shuffle=True)  # Using 10-fold cross-validation\n",
    "mse_scores = -cross_val_score(model, X_poly, weights, cv=kf, scoring='neg_mean_squared_error')\n",
    "mae_scores = -cross_val_score(model, X_poly, weights, cv=kf, scoring='neg_mean_absolute_error')\n",
    "r2_scores = cross_val_score(model, X_poly, weights, cv=kf, scoring='r2')\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Mean Squared Error (MSE):\", mse_scores.mean())\n",
    "print(\"Mean Absolute Error (MAE):\", mae_scores.mean())\n",
    "print(\"R-squared (R2):\", r2_scores.mean())\n",
    "\n",
    "# Train the model on the full dataset\n",
    "model.fit(X_poly, weights)\n",
    "\n",
    "# Infer new weights to achieve [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] unidentified peaks\n",
    "new_unidentified_peaks = poly_features.transform(np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n",
    "new_weights = model.predict(new_unidentified_peaks)\n",
    "\n",
    "print(\"Inferred weights:\", new_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fbc85e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-10T03:22:07.756833Z",
     "start_time": "2023-04-10T03:22:07.599362Z"
    }
   },
   "outputs": [],
   "source": [
    "def pcov_map(array1, array2, pCov_params, norm_factor):\n",
    "    array1[1:] *= norm_factor\n",
    "    array2[1:] *= norm_factor\n",
    "\n",
    "    num_scans = pCov_params.shape[0]\n",
    "\n",
    "    av_yx = array2[1:].T @ array1[1:] / (num_scans - 1)\n",
    "\n",
    "    sx_vec = array1[1:].sum(axis=0)\n",
    "    sy_vec = array2[1:].sum(axis=0)\n",
    "    av_y_av_x = np.outer(sy_vec, sx_vec) / (num_scans * (num_scans - 1))\n",
    "\n",
    "    si2 = (pCov_params ** 2).sum(axis=0)\n",
    "    s2i = pCov_params.sum(axis=0) ** 2\n",
    "    var = (si2 - s2i / num_scans) / (num_scans - 1)\n",
    "\n",
    "    si_sx = pCov_params.sum(axis=0) * array1[1:].sum(axis=0)\n",
    "    cov_xi_vec = (pCov_params.T @ array1[1:]) / (num_scans - 1) - si_sx / (num_scans * (num_scans - 1))\n",
    "\n",
    "    si_sy = pCov_params.sum(axis=0) * array2[1:].sum(axis=0)\n",
    "    cov_yi_vec = (pCov_params.T @ array2[1:]) / (num_scans - 1) - si_sy / (num_scans * (num_scans - 1))\n",
    "\n",
    "    pcov_matrix = av_yx - av_y_av_x - cov_yi_vec.reshape(-1, 1) @ cov_xi_vec.reshape(1, -1) / var\n",
    "    pcov_matrix *= (num_scans - 1) / (num_scans - 2)\n",
    "\n",
    "    sx = array1[1:][:num_scans].sum(axis=0)\n",
    "    sy = array2[1:][:num_scans].sum(axis=0)\n",
    "    sysx = np.outer(sy, sx)\n",
    "    cov_matrix = array2[1:].T @ array1[1:] - sysx / num_scans\n",
    "    cov_matrix /= num_scans - 1\n",
    "\n",
    "    return pcov_matrix.T, cov_matrix.T\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLBD-MRes",
   "language": "python",
   "name": "mlbd-mres"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
