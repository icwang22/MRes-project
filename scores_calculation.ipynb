{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T23:27:20.427924Z",
     "start_time": "2023-04-06T23:27:20.426654Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename='torch_time.log', filemode='w', format='%(asctime)s - %(message)s', level=logging.INFO)\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6727_2d-PC-MS_1pmol-ul_1AGC_0-7quadiso_HCD\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--weights', nargs='?', type=np.array, default=None, help='weights for weighted covariance map (default: None)')\n",
    "parser.add_argument('--epoch', nargs='?', type=int, default=None, help='epoch number (default: None)')\n",
    "parser.add_argument('--dirname', nargs='?', type=str, default='Test', help='directory name')\n",
    "parser.add_argument('--name', nargs='?', type=str, default='6727_2d-PC-MS_1pmol-ul_1AGC_0-7quadiso_HCD', help='file name')\n",
    "parser.add_argument('--run_all', nargs='?', type=bool, default='False', help='whether to run all files in the directory or not (default: False)')\n",
    "if sys.argv[1] == '-f':\n",
    "    sys.argv=[sys.argv[0:-2]]\n",
    "args = parser.parse_args()\n",
    "print(args.name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pC2DMSUtils import maxIndices#, varII, covXI, cutAC, scaleToPower\n",
    "import pC2DMS\n",
    "import pC2DMSUtils\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "def txt2cache(textFile, path, numscanInterval=None):\n",
    "    saveName=path+'/save'\n",
    "    numScans='all'\n",
    "\n",
    "    print('path:', path)\n",
    "    print(numScans, 'scans')\n",
    "    print('save in', saveName)\n",
    "\n",
    "    start = time.time()\n",
    "    pC2DMSUtils.readTextFile(textFile, path, numscanInterval=numscanInterval)\n",
    "    readTime = time.time()\n",
    "\n",
    "\n",
    "def preprocess(prefix, data_path='../IC/raw data/Test/', cache_path='./peptide output/Test/', numscanInterval=None):\n",
    "    data_filename = data_path + prefix + '.txt'\n",
    "    cache_dir = cache_path + prefix + '/10000 scans'\n",
    "    if not os.path.exists(cache_path + prefix):\n",
    "        os.makedirs(cache_path + prefix)\n",
    "    #     if Path(cache_dir).is_dir():\n",
    "    #         print('Cache directory exists. Preprocessing skipped.')\n",
    "    #     else:\n",
    "    txt2cache(data_filename, cache_dir, numscanInterval=numscanInterval)\n",
    "    return cache_dir\n",
    "    \n",
    "    \n",
    "def pcovmap(path, mode, weights=None):\n",
    "    scan1=pC2DMS.Scan(path)\n",
    "    scanTime = time.time()\n",
    "    if mode == 'w':\n",
    "        map1=pC2DMS.PCovMap(scan1, scan1.weightFunc(weights), numScans='all')\n",
    "    elif mode == 'tic':\n",
    "        map1=pC2DMS.PCovMap(scan1, scan1.tic(), numScans='all')\n",
    "    else:\n",
    "        map1=pC2DMS.CovMap(scan1, numScans='all')\n",
    "    mapTime = time.time()\n",
    "    print('map time', mapTime-scanTime)\n",
    "    del scan1, mapTime, scanTime\n",
    "    return map1\n",
    "\n",
    "\n",
    "def run_covmap(prefix, mode = 'tic', data_path='../IC/raw data/Test/', cache_path='./peptide output/Test/',\n",
    "               numscanInterval=None, weights=None, epoch=None):\n",
    "    cache_dir = cache_path + prefix + '/10000 scans'\n",
    "    if numscanInterval is not None:\n",
    "        for numscan in np.arange(numscanInterval, 10000, numscanInterval):\n",
    "            if not os.path.exists(cache_path + prefix + '/' + str(numscan) + ' scans/' + 'array.npy'):\n",
    "                cache_dir = preprocess(prefix, data_path=data_path, cache_path=cache_path, numscanInterval=numscanInterval)\n",
    "                break\n",
    "    else:\n",
    "        if not os.path.exists(cache_path + prefix + '/10000 scans'):\n",
    "            cache_dir = preprocess(prefix, data_path=data_path, cache_path=cache_path)\n",
    "    for dirname in os.listdir(os.path.abspath(os.path.join(cache_dir, os.pardir))):\n",
    "        if numscanInterval is not None:\n",
    "            cache_dir = os.path.join(os.path.abspath(os.path.join(cache_dir, os.pardir)), dirname)\n",
    "        if not os.path.exists(cache_dir + '/array.npy'):\n",
    "            continue\n",
    "        featfile = cache_dir+'/'+mode+'_topfeat.npy'\n",
    "        reportfile = cache_dir+'/'+mode+'_topfeat.csv'\n",
    "        if epoch is not None:\n",
    "            featfile = cache_dir+'/'+mode+'_'+epoch+'_topfeat.npy'\n",
    "            reportfile = cache_dir+'/'+mode+'_'+epoch+ '_topfeat.csv'\n",
    "        print(cache_dir)\n",
    "        cmap = pcovmap(cache_dir, mode, weights=weights)\n",
    "        mapfile = cache_dir+'/'+mode+'_map.npy'\n",
    "        np.save(mapfile, cmap.array)\n",
    "        if mode == 'w':\n",
    "            if Path(featfile).is_file():\n",
    "                os.remove(featfile)\n",
    "            topfeat = cmap.analyse(3000)\n",
    "            np.save(featfile, topfeat)\n",
    "        elif Path(featfile).is_file():\n",
    "            print('Feature file exists. Features loaded. Analysis skipped.')\n",
    "            topfeat = np.load(featfile)\n",
    "        else:\n",
    "            if mode == 'tic':\n",
    "                topfeat = cmap.analyse(3000)\n",
    "            else:\n",
    "                topfeat = cmap.analyse(1000)\n",
    "            np.save(featfile, topfeat)\n",
    "\n",
    "        topfeat_sorted = topfeat[np.flip(topfeat[:, 3].argsort())]\n",
    "    #     new_template = np.copy(cmap.array)\n",
    "    #     new_template.fill(0)\n",
    "        np.savetxt(reportfile, topfeat_sorted, fmt = '%.2f', delimiter=',')\n",
    "        if numscanInterval is None:\n",
    "            break\n",
    "    return cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if args.run_all:\n",
    "    # # # datafile_list = ['20200206_1455,20_ME16_3+_NCE15_AGC100_cvScan2000_turbo_1,7V_2uL',\n",
    "    # # #                  '20160503_1127_PH4_3+_CVscan_NCE35_Turbo',\n",
    "    # # #                  '20160603_1040_ME9_3+_CVscan_NCE35_Turbo',\n",
    "    # # #                  '20160622_1514_ME14_3+_1to2500_CVscan_NCE35'\n",
    "    ####                    '7255_2D-PC-MS_0-5pmol-ul_0-1AGC_0-7quadiso',\n",
    "    # # #                 ]\n",
    "\n",
    "    # datafile_list = ['7302_2D-PC-MS_2pmol-ul_1AGC_0-7quadiso',\n",
    "    #                  '7302_2D-PC-MS_1pmol-ul_1AGC_0-7quadiso',\n",
    "    #                  '7302_2D-PC-MS_0-5pmol-ul_0-1AGC_0-7quadiso',\n",
    "    #                  '7302_2D-PC-MS_0-1pmol-ul_0-1AGC_0-7quadiso',\n",
    "    #                  '7255_2D-PC-MS_5pmol-ul_1AGC_0-7quadiso',\n",
    "    #                  '7255_2D-PC-MS_1pmol-ul_0-1AGC_0-7quadiso']\n",
    "\n",
    "    datafile_dict = {}\n",
    "\n",
    "    for dirname in os.listdir('../IC/IC/raw data/'):\n",
    "        datafile_dict[dirname] = []\n",
    "        for filename in os.listdir('../IC/IC/raw data/' + str(dirname)):\n",
    "            datafile_dict[dirname].append(os.path.splitext(filename)[0])\n",
    "        # 6727_2d-PC-MS_1pmol-ul_1AGC_0-7quadiso_HCD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run(dirname, datafile_list, numscanInterval=None, mode=['tic'], weights=None, epoch=None):\n",
    "    try:\n",
    "        for prefix in datafile_list:\n",
    "            sub_start = datetime.now()\n",
    "            for mode in mode:\n",
    "                cmap = run_covmap(prefix, mode, data_path=os.path.join('../IC/IC/raw data', str(dirname), ''),\n",
    "                                  cache_path=os.path.join('./peptide output', str(dirname)+'/'), numscanInterval=numscanInterval,\n",
    "                                  weights=weights, epoch=epoch)\n",
    "            sub_stop = datetime.now()\n",
    "            logging.info(f\"The running time of {prefix}: {sub_stop - sub_start}\")\n",
    "            del prefix, sub_start, sub_stop, mode, cmap, dirname, datafile_list, numscanInterval, weights, epoch\n",
    "    except FileExistsError:\n",
    "        logging.info('File exists. Running skipped.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['CID', '.ipynb_checkpoints', 'Test', 'HCD'])\n"
     ]
    }
   ],
   "source": [
    "if args.run_all:\n",
    "    print(datafile_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-24 01:46:21,070 - root - INFO - Saving files different from 1000 scans to maximum scans\n"
     ]
    }
   ],
   "source": [
    "\n",
    "numscanInterval = 1000\n",
    "logging.info(f\"Saving files different from {numscanInterval} scans to maximum scans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = list(datafile_dict.values())[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rds/general/user/ww1922/home/src/peptide output/Test/6727_2d-PC-MS_1pmol-ul_1AGC_0-7quadiso_HCD/3000 scans\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run(args.dirname, [args.name], numscanInterval=numscanInterval, weights=args.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.run_all:\n",
    "    run(list(datafile_dict)[0], [list(datafile_dict.values())[0][0]], numscanInterval=numscanInterval, weights=args.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.run_all:\n",
    "    run(list(datafile_dict)[0], [list(datafile_dict.values())[0][1]], numscanInterval=numscanInterval, weights=args.weights, epoch=args.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.run_all:\n",
    "    run(list(datafile_dict)[0], [list(datafile_dict.values())[0][2]], numscanInterval=numscanInterval, weights=args.weights, epoch=args.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.run_all:\n",
    "    run(list(datafile_dict)[1], [list(datafile_dict.values())[1][0]], numscanInterval=numscanInterval, weights=args.weights, epoch=args.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.run_all:\n",
    "    run(list(datafile_dict)[1], [list(datafile_dict.values())[1][1]], numscanInterval=numscanInterval, weights=args.weights, epoch=args.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.run_all:\n",
    "    run(list(datafile_dict)[1], [list(datafile_dict.values())[1][2]], numscanInterval=numscanInterval, weights=args.weights, epoch=args.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.run_all:\n",
    "    run(list(datafile_dict)[1], [list(datafile_dict.values())[1][3]], numscanInterval=numscanInterval, weights=args.weights, epoch=args.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.run_all:\n",
    "    run(list(datafile_dict)[1], [list(datafile_dict.values())[1][4]], numscanInterval=numscanInterval, weights=args.weights, epoch=args.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mrun_all:\n\u001b[0;32m----> 2\u001b[0m     run(\u001b[38;5;28mlist\u001b[39m(datafile_dict)[\u001b[38;5;241m1\u001b[39m], [\u001b[38;5;28mlist\u001b[39m(datafile_dict\u001b[38;5;241m.\u001b[39mvalues())[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m5\u001b[39m]], numscanInterval\u001b[38;5;241m=\u001b[39mnumscanInterval, weights\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mweights, epoch\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mepoch)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "if args.run_all:\n",
    "    run(list(datafile_dict)[1], [list(datafile_dict.values())[1][5]], numscanInterval=numscanInterval, weights=args.weights, epoch=args.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: ./peptide output/Test/7255_2d-PC-MS_5pmol-ul_Energystepping_1AGC_0-7ltqiso_35CID/10000 scans\n",
      "all scans\n",
      "save in ./peptide output/Test/7255_2d-PC-MS_5pmol-ul_Energystepping_1AGC_0-7ltqiso_35CID/10000 scans/save\n",
      "Reading file ../IC/IC/raw data/Test/7255_2d-PC-MS_5pmol-ul_Energystepping_1AGC_0-7ltqiso_35CID.txt to ./peptide output/Test/7255_2d-PC-MS_5pmol-ul_Energystepping_1AGC_0-7ltqiso_35CID/10000 scans\n",
      "Reading scan number 100\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/rds/general/user/ww1922/home/anaconda3/envs/test1/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/tmp/pbs.7977053.pbs/ipykernel_1810035/2107556360.py\", line 2, in <module>\n",
      "    run(list(datafile_dict)[2], [list(datafile_dict.values())[2][0]], numscanInterval=1000, weights=args.weights, epoch=args.epoch)\n",
      "  File \"/var/tmp/pbs.7977053.pbs/ipykernel_1810035/1911021906.py\", line 6, in run\n",
      "    cmap = run_covmap(prefix, mode, data_path=os.path.join('../IC/IC/raw data', str(dirname), ''),\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/tmp/pbs.7977053.pbs/ipykernel_1810035/3292070307.py\", line 55, in run_covmap\n",
      "    cache_dir = preprocess(prefix, data_path=data_path, cache_path=cache_path, numscanInterval=numscanInterval)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/tmp/pbs.7977053.pbs/ipykernel_1810035/3292070307.py\", line 30, in preprocess\n",
      "    txt2cache(data_filename, cache_dir, numscanInterval=numscanInterval)\n",
      "  File \"/var/tmp/pbs.7977053.pbs/ipykernel_1810035/3292070307.py\", line 18, in txt2cache\n",
      "    pC2DMSUtils.readTextFile(textFile, path, numscanInterval=numscanInterval)\n",
      "  File \"/rds/general/user/ww1922/home/src/pC2DMSUtils.py\", line -1, in readTextFile\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/rds/general/user/ww1922/home/anaconda3/envs/test1/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/ww1922/home/anaconda3/envs/test1/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/ww1922/home/anaconda3/envs/test1/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/ww1922/home/anaconda3/envs/test1/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/ww1922/home/anaconda3/envs/test1/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 978, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/ww1922/home/anaconda3/envs/test1/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 878, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/ww1922/home/anaconda3/envs/test1/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 712, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/ww1922/home/anaconda3/envs/test1/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/ww1922/home/anaconda3/envs/test1/lib/python3.11/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/ww1922/home/anaconda3/envs/test1/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/ww1922/home/anaconda3/envs/test1/lib/python3.11/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/ww1922/home/anaconda3/envs/test1/lib/python3.11/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/rds/general/user/ww1922/home/anaconda3/envs/test1/lib/python3.11/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/rds/general/user/ww1922/home/anaconda3/envs/test1/lib/python3.11/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "if args.run_all:\n",
    "    run(list(datafile_dict)[2], [list(datafile_dict.values())[2][0]], numscanInterval=1000, weights=args.weights, epoch=args.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rds/general/user/ww1922/home/src/peptide output/Test/20160603_1005_ME9_2+_CVscan_NCE35_Turbo/3000 scans\n",
      "map time 0.4995143413543701\n",
      "Feature file exists. Features loaded. Analysis skipped.\n",
      "/rds/general/user/ww1922/home/src/peptide output/Test/20160603_1005_ME9_2+_CVscan_NCE35_Turbo/6000 scans\n",
      "map time 0.56827712059021\n",
      "Feature file exists. Features loaded. Analysis skipped.\n",
      "/rds/general/user/ww1922/home/src/peptide output/Test/20160603_1005_ME9_2+_CVscan_NCE35_Turbo/9000 scans\n",
      "map time 0.804645299911499\n",
      "Feature file exists. Features loaded. Analysis skipped.\n",
      "/rds/general/user/ww1922/home/src/peptide output/Test/20160603_1005_ME9_2+_CVscan_NCE35_Turbo/2000 scans\n",
      "map time 0.5336406230926514\n",
      "Feature file exists. Features loaded. Analysis skipped.\n",
      "/rds/general/user/ww1922/home/src/peptide output/Test/20160603_1005_ME9_2+_CVscan_NCE35_Turbo/10000 scans\n",
      "map time 0.7258303165435791\n",
      "Feature file exists. Features loaded. Analysis skipped.\n",
      "/rds/general/user/ww1922/home/src/peptide output/Test/20160603_1005_ME9_2+_CVscan_NCE35_Turbo/4000 scans\n",
      "map time 0.4519541263580322\n",
      "Feature file exists. Features loaded. Analysis skipped.\n",
      "/rds/general/user/ww1922/home/src/peptide output/Test/20160603_1005_ME9_2+_CVscan_NCE35_Turbo/7000 scans\n",
      "map time 0.6098482608795166\n",
      "Feature file exists. Features loaded. Analysis skipped.\n",
      "/rds/general/user/ww1922/home/src/peptide output/Test/20160603_1005_ME9_2+_CVscan_NCE35_Turbo/1000 scans\n",
      "map time 0.5306713581085205\n",
      "Feature file exists. Features loaded. Analysis skipped.\n",
      "/rds/general/user/ww1922/home/src/peptide output/Test/20160603_1005_ME9_2+_CVscan_NCE35_Turbo/5000 scans\n",
      "map time 0.6278455257415771\n",
      "Feature file exists. Features loaded. Analysis skipped.\n",
      "/rds/general/user/ww1922/home/src/peptide output/Test/20160603_1005_ME9_2+_CVscan_NCE35_Turbo/8000 scans\n",
      "map time 0.5617713928222656\n",
      "Feature file exists. Features loaded. Analysis skipped.\n"
     ]
    }
   ],
   "source": [
    "if args.run_all:\n",
    "    run(list(datafile_dict)[2], [list(datafile_dict.values())[2][1]], numscanInterval=1000, weights=args.weights, epoch=args.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import pC2DMS\n",
    "weights = np.array()\n",
    "with open('optimizer_tpe_loss_func3.dill', \"rb\") as file:\n",
    "    optimizer = dill.load(file)\n",
    "scan1 = pC2DMS.Scan(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + list(datafile_dict.values())[2][1] + '/') + str(10000)+ ' scans/')\n",
    "cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc([optimizer.best_params[f'w_{i}'] for i in range(len(weights))]), numScans='all')\n",
    "top_feat = cmap.analyse(3000)\n",
    "np.save(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + list(datafile_dict.values())[2][1] + '/') + str(10000)+ ' scans/w_top_feat_tpe(loss_func3,bins:100).npy', top_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 100 good features\n",
      "found 200 good features\n",
      "found 300 good features\n",
      "found 400 good features\n",
      "found 500 good features\n",
      "found 600 good features\n",
      "found 700 good features\n",
      "found 800 good features\n",
      "found 900 good features\n",
      "found 1000 good features\n",
      "found 1100 good features\n",
      "found 1200 good features\n",
      "found 1300 good features\n",
      "found 1400 good features\n",
      "found 1500 good features\n",
      "found 1600 good features\n",
      "found 1700 good features\n",
      "found 1800 good features\n",
      "found 1900 good features\n",
      "found 2000 good features\n",
      "found 2100 good features\n",
      "found 2200 good features\n",
      "found 2300 good features\n",
      "found 2400 good features\n",
      "found 2500 good features\n",
      "found 2600 good features\n",
      "found 2700 good features\n",
      "found 2800 good features\n",
      "found 2900 good features\n",
      "found 3000 good features\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "sig calculated for feature 350\n",
      "sig calculated for feature 400\n",
      "sig calculated for feature 450\n",
      "sig calculated for feature 500\n",
      "sig calculated for feature 550\n",
      "sig calculated for feature 600\n",
      "sig calculated for feature 650\n",
      "sig calculated for feature 700\n",
      "sig calculated for feature 750\n",
      "sig calculated for feature 800\n",
      "sig calculated for feature 850\n",
      "sig calculated for feature 900\n",
      "sig calculated for feature 950\n",
      "sig calculated for feature 1000\n",
      "sig calculated for feature 1050\n",
      "sig calculated for feature 1100\n",
      "sig calculated for feature 1150\n",
      "sig calculated for feature 1200\n",
      "sig calculated for feature 1250\n",
      "sig calculated for feature 1300\n",
      "sig calculated for feature 1350\n",
      "sig calculated for feature 1400\n",
      "sig calculated for feature 1450\n",
      "sig calculated for feature 1500\n",
      "sig calculated for feature 1550\n",
      "sig calculated for feature 1600\n",
      "sig calculated for feature 1650\n",
      "sig calculated for feature 1700\n",
      "sig calculated for feature 1750\n",
      "sig calculated for feature 1800\n",
      "sig calculated for feature 1850\n",
      "sig calculated for feature 1900\n",
      "sig calculated for feature 1950\n",
      "sig calculated for feature 2000\n",
      "sig calculated for feature 2050\n",
      "sig calculated for feature 2100\n",
      "sig calculated for feature 2150\n",
      "sig calculated for feature 2200\n",
      "sig calculated for feature 2250\n",
      "sig calculated for feature 2300\n",
      "sig calculated for feature 2350\n",
      "sig calculated for feature 2400\n",
      "sig calculated for feature 2450\n",
      "sig calculated for feature 2500\n",
      "sig calculated for feature 2550\n",
      "sig calculated for feature 2600\n",
      "sig calculated for feature 2650\n",
      "sig calculated for feature 2700\n",
      "sig calculated for feature 2750\n",
      "sig calculated for feature 2800\n",
      "sig calculated for feature 2850\n",
      "sig calculated for feature 2900\n",
      "sig calculated for feature 2950\n",
      "sig calculated for feature 3000\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "import pC2DMS\n",
    "weights = np.array([1.10342122, 1.74157288, 0.95289493, 0.8134206, 0.64139062, 0.56111331\n",
    ", 0.86290635, 0.32493003, 0.17430608, 0.2823324, 0.14877877, 1.03187679\n",
    ", 1.30729305, 1.52126059, 1.19581792, 1.36287387, 1.11043203, 1.2143669\n",
    ", 1.45443463, 0.69125176, 0.82259259, 1.68136914, 1.17540044, 1.35825569\n",
    ", 1.03686637, 1.31171829, 1.92378558, 1.31088039, 1.40793891, 0.11603733\n",
    ", 1.85584202, 1.40675701, 1.32256018, 0.66425642, 1.09700642, 1.47240462\n",
    ", 1.37059904, 0.35769377, 1.2734134])\n",
    "scan1 = pC2DMS.Scan(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + list(datafile_dict.values())[2][1] + '/') + str(10000)+ ' scans/')\n",
    "cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc(weights), numScans='all')\n",
    "top_feat = cmap.analyse(3000)\n",
    "np.save(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + list(datafile_dict.values())[2][1] + '/') + str(10000)+ ' scans/w_top_feat_tpe(loss_func2,bins:100)_new.npy', top_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20160603_1005_ME9_2+_CVscan_NCE35_Turbo\n"
     ]
    }
   ],
   "source": [
    "print(list(datafile_dict.values())[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 100 good features\n",
      "found 200 good features\n",
      "found 300 good features\n",
      "found 400 good features\n",
      "found 500 good features\n",
      "found 600 good features\n",
      "found 700 good features\n",
      "found 800 good features\n",
      "found 900 good features\n",
      "found 1000 good features\n",
      "found 1100 good features\n",
      "found 1200 good features\n",
      "found 1300 good features\n",
      "found 1400 good features\n",
      "found 1500 good features\n",
      "found 1600 good features\n",
      "found 1700 good features\n",
      "found 1800 good features\n",
      "found 1900 good features\n",
      "found 2000 good features\n",
      "found 2100 good features\n",
      "found 2200 good features\n",
      "found 2300 good features\n",
      "found 2400 good features\n",
      "found 2500 good features\n",
      "found 2600 good features\n",
      "found 2700 good features\n",
      "found 2800 good features\n",
      "found 2900 good features\n",
      "found 3000 good features\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "sig calculated for feature 350\n",
      "sig calculated for feature 400\n",
      "sig calculated for feature 450\n",
      "sig calculated for feature 500\n",
      "sig calculated for feature 550\n",
      "sig calculated for feature 600\n",
      "sig calculated for feature 650\n",
      "sig calculated for feature 700\n",
      "sig calculated for feature 750\n",
      "sig calculated for feature 800\n",
      "sig calculated for feature 850\n",
      "sig calculated for feature 900\n",
      "sig calculated for feature 950\n",
      "sig calculated for feature 1000\n",
      "sig calculated for feature 1050\n",
      "sig calculated for feature 1100\n",
      "sig calculated for feature 1150\n",
      "sig calculated for feature 1200\n",
      "sig calculated for feature 1250\n",
      "sig calculated for feature 1300\n",
      "sig calculated for feature 1350\n",
      "sig calculated for feature 1400\n",
      "sig calculated for feature 1450\n",
      "sig calculated for feature 1500\n",
      "sig calculated for feature 1550\n",
      "sig calculated for feature 1600\n",
      "sig calculated for feature 1650\n",
      "sig calculated for feature 1700\n",
      "sig calculated for feature 1750\n",
      "sig calculated for feature 1800\n",
      "sig calculated for feature 1850\n",
      "sig calculated for feature 1900\n",
      "sig calculated for feature 1950\n",
      "sig calculated for feature 2000\n",
      "sig calculated for feature 2050\n",
      "sig calculated for feature 2100\n",
      "sig calculated for feature 2150\n",
      "sig calculated for feature 2200\n",
      "sig calculated for feature 2250\n",
      "sig calculated for feature 2300\n",
      "sig calculated for feature 2350\n",
      "sig calculated for feature 2400\n",
      "sig calculated for feature 2450\n",
      "sig calculated for feature 2500\n",
      "sig calculated for feature 2550\n",
      "sig calculated for feature 2600\n",
      "sig calculated for feature 2650\n",
      "sig calculated for feature 2700\n",
      "sig calculated for feature 2750\n",
      "sig calculated for feature 2800\n",
      "sig calculated for feature 2850\n",
      "sig calculated for feature 2900\n",
      "sig calculated for feature 2950\n",
      "sig calculated for feature 3000\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "import pC2DMS\n",
    "weights = np.array([1.77742493, 0.26736808, 1.79356456, 0.60505574, 0.68142098, 0.55045802\n",
    ", 0.41505678, 0.23192339, 0.74895327, 1.05238373, 0.46418789, 0.92438629\n",
    ", 1.76361323, 1.11775136, 1.21711266, 1.95209787, 0.04707956, 0.78448469\n",
    ", 0.17315706, 1.0403938, 0.32855712, 1.61574099, 0.19461299, 0.5621586\n",
    ", 0.5571355, 1.09908919, 0.70036215, 1.75407656, 1.01922498, 0.46050934\n",
    ", 0.25243517, 0.33699795, 0.48094845, 1.39329184, 0.57847116, 1.91095005\n",
    ", 1.09944832, 0.97740241, 1.67971844])\n",
    "scan1 = pC2DMS.Scan(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name) + '/') + str(10000)+ ' scans/')\n",
    "cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc(weights), numScans='all')\n",
    "top_feat = cmap.analyse(3000)\n",
    "np.save(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name) + '/') + str(10000)+ ' scans/w_top_feat_tpe(loss_func1,bins:100)_new.npy', top_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 100 good features\n",
      "found 200 good features\n",
      "found 300 good features\n",
      "found 400 good features\n",
      "found 500 good features\n",
      "found 600 good features\n",
      "found 700 good features\n",
      "found 800 good features\n",
      "found 900 good features\n",
      "found 1000 good features\n",
      "found 1100 good features\n",
      "found 1200 good features\n",
      "found 1300 good features\n",
      "found 1400 good features\n",
      "found 1500 good features\n",
      "found 1600 good features\n",
      "found 1700 good features\n",
      "found 1800 good features\n",
      "found 1900 good features\n",
      "found 2000 good features\n",
      "found 2100 good features\n",
      "found 2200 good features\n",
      "found 2300 good features\n",
      "found 2400 good features\n",
      "found 2500 good features\n",
      "found 2600 good features\n",
      "found 2700 good features\n",
      "found 2800 good features\n",
      "found 2900 good features\n",
      "found 3000 good features\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "sig calculated for feature 350\n",
      "sig calculated for feature 400\n",
      "sig calculated for feature 450\n",
      "sig calculated for feature 500\n",
      "sig calculated for feature 550\n",
      "sig calculated for feature 600\n",
      "sig calculated for feature 650\n",
      "sig calculated for feature 700\n",
      "sig calculated for feature 750\n",
      "sig calculated for feature 800\n",
      "sig calculated for feature 850\n",
      "sig calculated for feature 900\n",
      "sig calculated for feature 950\n",
      "sig calculated for feature 1000\n",
      "sig calculated for feature 1050\n",
      "sig calculated for feature 1100\n",
      "sig calculated for feature 1150\n",
      "sig calculated for feature 1200\n",
      "sig calculated for feature 1250\n",
      "sig calculated for feature 1300\n",
      "sig calculated for feature 1350\n",
      "sig calculated for feature 1400\n",
      "sig calculated for feature 1450\n",
      "sig calculated for feature 1500\n",
      "sig calculated for feature 1550\n",
      "sig calculated for feature 1600\n",
      "sig calculated for feature 1650\n",
      "sig calculated for feature 1700\n",
      "sig calculated for feature 1750\n",
      "sig calculated for feature 1800\n",
      "sig calculated for feature 1850\n",
      "sig calculated for feature 1900\n",
      "sig calculated for feature 1950\n",
      "sig calculated for feature 2000\n",
      "sig calculated for feature 2050\n",
      "sig calculated for feature 2100\n",
      "sig calculated for feature 2150\n",
      "sig calculated for feature 2200\n",
      "sig calculated for feature 2250\n",
      "sig calculated for feature 2300\n",
      "sig calculated for feature 2350\n",
      "sig calculated for feature 2400\n",
      "sig calculated for feature 2450\n",
      "sig calculated for feature 2500\n",
      "sig calculated for feature 2550\n",
      "sig calculated for feature 2600\n",
      "sig calculated for feature 2650\n",
      "sig calculated for feature 2700\n",
      "sig calculated for feature 2750\n",
      "sig calculated for feature 2800\n",
      "sig calculated for feature 2850\n",
      "sig calculated for feature 2900\n",
      "sig calculated for feature 2950\n",
      "sig calculated for feature 3000\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "import pC2DMS\n",
    "weights = np.load('weights_tpe.npy')\n",
    "with open('optimizer_tpe_loss_func3.dill', \"rb\") as file:\n",
    "    optimizer = dill.load(file)\n",
    "scan1 = pC2DMS.Scan(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name) + '/') + str(10000)+ ' scans/')\n",
    "cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc([optimizer.best_params[f'w_{i}'] for i in range(len(weights))]), numScans='all')\n",
    "top_feat = cmap.analyse(3000)\n",
    "np.save(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name) + '/') + str(10000)+ ' scans/w_top_feat_tpe(loss_func3,bins:100).npy', top_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 100 good features\n",
      "found 200 good features\n",
      "found 300 good features\n",
      "found 400 good features\n",
      "found 500 good features\n",
      "found 600 good features\n",
      "found 700 good features\n",
      "found 800 good features\n",
      "found 900 good features\n",
      "found 1000 good features\n",
      "found 1100 good features\n",
      "found 1200 good features\n",
      "found 1300 good features\n",
      "found 1400 good features\n",
      "found 1500 good features\n",
      "found 1600 good features\n",
      "found 1700 good features\n",
      "found 1800 good features\n",
      "found 1900 good features\n",
      "found 2000 good features\n",
      "found 2100 good features\n",
      "found 2200 good features\n",
      "found 2300 good features\n",
      "found 2400 good features\n",
      "found 2500 good features\n",
      "found 2600 good features\n",
      "found 2700 good features\n",
      "found 2800 good features\n",
      "found 2900 good features\n",
      "found 3000 good features\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "sig calculated for feature 350\n",
      "sig calculated for feature 400\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "import pC2DMS\n",
    "weights = np.array([1.10852360e+00, 1.03970814e+00, 4.36300691e-01, 1.31365090e+00\n",
    ", 4.55692794e-01, 8.46024175e-01, 1.11253384e+00, 1.80067251e+00\n",
    ", 1.21228033e+00, 2.34350823e-01, 1.57904318e-03, 7.55099172e-01\n",
    ", 3.18901463e-01, 9.36451730e-01, 1.34660578e+00, 1.13076963e+00\n",
    ", 1.12982963e+00, 1.45391947e+00, 1.00338455e+00, 1.86823253e+00\n",
    ", 1.17855160e+00, 1.57184266e+00, 1.30403196e+00, 9.19918662e-01\n",
    ", 2.79588481e-01, 2.75350232e-01, 1.10435645e+00, 1.10348111e+00\n",
    ", 1.12390881e+00, 7.22830155e-01, 1.36734980e+00, 1.91305993e+00\n",
    ", 1.95821784e+00, 4.40292674e-01, 1.01821398e+00, 2.66211971e-01\n",
    ", 1.69587907e+00, 1.02986897e+00, 1.45997074e+00])\n",
    "scan1 = pC2DMS.Scan(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name) + '/') + str(10000)+ ' scans/')\n",
    "cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc(weights), numScans='all')\n",
    "top_feat = cmap.analyse(3000)\n",
    "np.save(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name) + '/') + str(10000)+ ' scans/w_top_feat_tpe(loss_func4,bins:100)_new.npy', top_feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 100 good features\n",
      "found 200 good features\n",
      "found 300 good features\n",
      "found 400 good features\n",
      "found 500 good features\n",
      "found 600 good features\n",
      "found 700 good features\n",
      "found 800 good features\n",
      "found 900 good features\n",
      "found 1000 good features\n",
      "found 1100 good features\n",
      "found 1200 good features\n",
      "found 1300 good features\n",
      "found 1400 good features\n",
      "found 1500 good features\n",
      "found 1600 good features\n",
      "found 1700 good features\n",
      "found 1800 good features\n",
      "found 1900 good features\n",
      "found 2000 good features\n",
      "found 2100 good features\n",
      "found 2200 good features\n",
      "found 2300 good features\n",
      "found 2400 good features\n",
      "found 2500 good features\n",
      "found 2600 good features\n",
      "found 2700 good features\n",
      "found 2800 good features\n",
      "found 2900 good features\n",
      "found 3000 good features\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "sig calculated for feature 350\n",
      "sig calculated for feature 400\n",
      "sig calculated for feature 450\n",
      "sig calculated for feature 500\n",
      "sig calculated for feature 550\n",
      "sig calculated for feature 600\n",
      "sig calculated for feature 650\n",
      "sig calculated for feature 700\n",
      "sig calculated for feature 750\n",
      "sig calculated for feature 800\n",
      "sig calculated for feature 850\n",
      "sig calculated for feature 900\n",
      "sig calculated for feature 950\n",
      "sig calculated for feature 1000\n",
      "sig calculated for feature 1050\n",
      "sig calculated for feature 1100\n",
      "sig calculated for feature 1150\n",
      "sig calculated for feature 1200\n",
      "sig calculated for feature 1250\n",
      "sig calculated for feature 1300\n",
      "sig calculated for feature 1350\n",
      "sig calculated for feature 1400\n",
      "sig calculated for feature 1450\n",
      "sig calculated for feature 1500\n",
      "sig calculated for feature 1550\n",
      "sig calculated for feature 1600\n",
      "sig calculated for feature 1650\n",
      "sig calculated for feature 1700\n",
      "sig calculated for feature 1750\n",
      "sig calculated for feature 1800\n",
      "sig calculated for feature 1850\n",
      "sig calculated for feature 1900\n",
      "sig calculated for feature 1950\n",
      "sig calculated for feature 2000\n",
      "sig calculated for feature 2050\n",
      "sig calculated for feature 2100\n",
      "sig calculated for feature 2150\n",
      "sig calculated for feature 2200\n",
      "sig calculated for feature 2250\n",
      "sig calculated for feature 2300\n",
      "sig calculated for feature 2350\n",
      "sig calculated for feature 2400\n",
      "sig calculated for feature 2450\n",
      "sig calculated for feature 2500\n",
      "sig calculated for feature 2550\n",
      "sig calculated for feature 2600\n",
      "sig calculated for feature 2650\n",
      "sig calculated for feature 2700\n",
      "sig calculated for feature 2750\n",
      "sig calculated for feature 2800\n",
      "sig calculated for feature 2850\n",
      "sig calculated for feature 2900\n",
      "sig calculated for feature 2950\n",
      "sig calculated for feature 3000\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "import pC2DMS\n",
    "weights = [2, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 2, 0, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2,\n",
    " 0, 2, 2, 0, 0, 2, 0, 2, 2, 0, 2, 2, 0, 2, 2]\n",
    "weights = np.array(weights)\n",
    "weights[weights==0] = 0.00001\n",
    "scan1 = pC2DMS.Scan(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name) + '/') + str(10000)+ ' scans/')\n",
    "cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc(weights), numScans='all')\n",
    "top_feat = cmap.analyse(3000)\n",
    "np.save(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name) + '/') + str(10000)+ ' scans/w_top_feat_bayopt(loss_func_2,bins:100)_new.npy', top_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import pC2DMS\n",
    "weights = [0, 0, 0, 2, 2, 0, 2, 2, 2, 0, 0, 2, 2, 0, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2,\n",
    " 0, 2, 2, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 0, 2]\n",
    "weights = np.array(weights)\n",
    "weights[weights==0] = 0.00001\n",
    "scan1 = pC2DMS.Scan(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name) + '/') + str(10000)+ ' scans/')\n",
    "cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc(weights), numScans='all')\n",
    "top_feat = cmap.analyse(3000)\n",
    "np.save(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name) + '/') + str(10000)+ ' scans/w_top_feat_bayopt(loss_func1,bins:100)_new.npy', top_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 100 good features\n",
      "found 200 good features\n",
      "found 300 good features\n",
      "found 400 good features\n",
      "found 500 good features\n",
      "found 600 good features\n",
      "found 700 good features\n",
      "found 800 good features\n",
      "found 900 good features\n",
      "found 1000 good features\n",
      "found 1100 good features\n",
      "found 1200 good features\n",
      "found 1300 good features\n",
      "found 1400 good features\n",
      "found 1500 good features\n",
      "found 1600 good features\n",
      "found 1700 good features\n",
      "found 1800 good features\n",
      "found 1900 good features\n",
      "found 2000 good features\n",
      "found 2100 good features\n",
      "found 2200 good features\n",
      "found 2300 good features\n",
      "found 2400 good features\n",
      "found 2500 good features\n",
      "found 2600 good features\n",
      "found 2700 good features\n",
      "found 2800 good features\n",
      "found 2900 good features\n",
      "found 3000 good features\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "sig calculated for feature 350\n",
      "sig calculated for feature 400\n",
      "sig calculated for feature 450\n",
      "sig calculated for feature 500\n",
      "sig calculated for feature 550\n",
      "sig calculated for feature 600\n",
      "sig calculated for feature 650\n",
      "sig calculated for feature 700\n",
      "sig calculated for feature 750\n",
      "sig calculated for feature 800\n",
      "sig calculated for feature 850\n",
      "sig calculated for feature 900\n",
      "sig calculated for feature 950\n",
      "sig calculated for feature 1000\n",
      "sig calculated for feature 1050\n",
      "sig calculated for feature 1100\n",
      "sig calculated for feature 1150\n",
      "sig calculated for feature 1200\n",
      "sig calculated for feature 1250\n",
      "sig calculated for feature 1300\n",
      "sig calculated for feature 1350\n",
      "sig calculated for feature 1400\n",
      "sig calculated for feature 1450\n",
      "sig calculated for feature 1500\n",
      "sig calculated for feature 1550\n",
      "sig calculated for feature 1600\n",
      "sig calculated for feature 1650\n",
      "sig calculated for feature 1700\n",
      "sig calculated for feature 1750\n",
      "sig calculated for feature 1800\n",
      "sig calculated for feature 1850\n",
      "sig calculated for feature 1900\n",
      "sig calculated for feature 1950\n",
      "sig calculated for feature 2000\n",
      "sig calculated for feature 2050\n",
      "sig calculated for feature 2100\n",
      "sig calculated for feature 2150\n",
      "sig calculated for feature 2200\n",
      "sig calculated for feature 2250\n",
      "sig calculated for feature 2300\n",
      "sig calculated for feature 2350\n",
      "sig calculated for feature 2400\n",
      "sig calculated for feature 2450\n",
      "sig calculated for feature 2500\n",
      "sig calculated for feature 2550\n",
      "sig calculated for feature 2600\n",
      "sig calculated for feature 2650\n",
      "sig calculated for feature 2700\n",
      "sig calculated for feature 2750\n",
      "sig calculated for feature 2800\n",
      "sig calculated for feature 2850\n",
      "sig calculated for feature 2900\n",
      "sig calculated for feature 2950\n",
      "sig calculated for feature 3000\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "import pC2DMS\n",
    "weights = [0.74908024, 1.90142861, 1.66488528, 0.73272369, 1.89777107, 1.93126407\n",
    ", 1.6167947, 0.60922754, 0.19534423, 1.36846605, 1.46398788, 1.19731697\n",
    ", 0.31203728, 0.31198904, 0.11616722, 1.73235229, 1.20223002, 1.41614516\n",
    ", 0.04116899, 1.9398197, 0.42467822, 0.36364993, 0.36680902, 0.60848449\n",
    ", 1.04951286, 0.86389004, 0.58245828, 1.22370579, 0.27898772, 0.5842893\n",
    ", 0.91213997, 1.57035192, 0.39934756, 1.02846888, 1.18482914, 0.09290083\n",
    ", 1.2150897, 0.34104825, 0.13010319]\n",
    "weights = np.array(weights)\n",
    "weights[weights==0] = 0.00001\n",
    "scan1 = pC2DMS.Scan(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name) + '/') + str(10000)+ ' scans/')\n",
    "cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc(weights), numScans='all')\n",
    "top_feat = cmap.analyse(3000)\n",
    "np.save(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name) + '/') + str(10000)+ ' scans/w_top_feat_bayopt(loss_func3,bins:100)_new.npy', top_feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 100 good features\n",
      "found 200 good features\n",
      "found 300 good features\n",
      "found 400 good features\n",
      "found 500 good features\n",
      "found 600 good features\n",
      "found 700 good features\n",
      "found 800 good features\n",
      "found 900 good features\n",
      "found 1000 good features\n",
      "found 1100 good features\n",
      "found 1200 good features\n",
      "found 1300 good features\n",
      "found 1400 good features\n",
      "found 1500 good features\n",
      "found 1600 good features\n",
      "found 1700 good features\n",
      "found 1800 good features\n",
      "found 1900 good features\n",
      "found 2000 good features\n",
      "found 2100 good features\n",
      "found 2200 good features\n",
      "found 2300 good features\n",
      "found 2400 good features\n",
      "found 2500 good features\n",
      "found 2600 good features\n",
      "found 2700 good features\n",
      "found 2800 good features\n",
      "found 2900 good features\n",
      "found 3000 good features\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "sig calculated for feature 350\n",
      "sig calculated for feature 400\n",
      "sig calculated for feature 450\n",
      "sig calculated for feature 500\n",
      "sig calculated for feature 550\n",
      "sig calculated for feature 600\n",
      "sig calculated for feature 650\n",
      "sig calculated for feature 700\n",
      "sig calculated for feature 750\n",
      "sig calculated for feature 800\n",
      "sig calculated for feature 850\n",
      "sig calculated for feature 900\n",
      "sig calculated for feature 950\n",
      "sig calculated for feature 1000\n",
      "sig calculated for feature 1050\n",
      "sig calculated for feature 1100\n",
      "sig calculated for feature 1150\n",
      "sig calculated for feature 1200\n",
      "sig calculated for feature 1250\n",
      "sig calculated for feature 1300\n",
      "sig calculated for feature 1350\n",
      "sig calculated for feature 1400\n",
      "sig calculated for feature 1450\n",
      "sig calculated for feature 1500\n",
      "sig calculated for feature 1550\n",
      "sig calculated for feature 1600\n",
      "sig calculated for feature 1650\n",
      "sig calculated for feature 1700\n",
      "sig calculated for feature 1750\n",
      "sig calculated for feature 1800\n",
      "sig calculated for feature 1850\n",
      "sig calculated for feature 1900\n",
      "sig calculated for feature 1950\n",
      "sig calculated for feature 2000\n",
      "sig calculated for feature 2050\n",
      "sig calculated for feature 2100\n",
      "sig calculated for feature 2150\n",
      "sig calculated for feature 2200\n",
      "sig calculated for feature 2250\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "import pC2DMS\n",
    "weights = [0, 0, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 2, 2, 2, 2, 2, 0, 2,\n",
    " 0, 2, 2, 0, 2, 2, 0, 0, 0, 0, 0, 2, 0, 0, 2]\n",
    "weights = np.array(weights)\n",
    "weights[weights==0] = 0.00001\n",
    "scan1 = pC2DMS.Scan(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name) + '/') + str(10000)+ ' scans/')\n",
    "cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc(weights), numScans='all')\n",
    "top_feat = cmap.analyse(3000)\n",
    "np.save(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name) + '/') + str(10000)+ ' scans/w_top_feat_bayopt(loss_func4,bins:100)_new.npy', top_feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import pC2DMS\n",
    "weights = [ 0.54078058, 1.91235936, 1.62655215, 0.98051032, 0.0998208, 0.0011511416\n",
    ", 0.002332195,  1.86147134, 1.38810316, 1.7995949, 0.0019062048, 1.65904835\n",
    ", 1.44386554, 0.56240998, 0.73713614, 0.01487633, 0.53509304, 0.92510706\n",
    ", 1.03444032, 0.49783055, 1.63770276, 0.17656574, 0.66594761, 0.43713566\n",
    ", 1.11780525, 1.38201146, 0.51417041, 1.11179031, 0.94547257, 0.16143622\n",
    ", 0.90912566, 0.48962911, 0.02293372, 2.16782376, 1.7012007,  1.5526123\n",
    ", 0.52629643, 0.0004528244, 1.09591412]\n",
    "weights = np.array(weights)\n",
    "weights[weights==0] = 0.00001\n",
    "scan1 = pC2DMS.Scan(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name) + '/') + str(10000)+ ' scans/')\n",
    "cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc(weights), numScans='all')\n",
    "top_feat = cmap.analyse(3000)\n",
    "np.save(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name) + '/') + str(10000)+ ' scans/w_top_feat_cma(loss_func2,bins:100)_new.npy', top_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import pC2DMS\n",
    "weights = [0.87198428, 0.68777021, 1.01795217, 0.7716168, 1.0648289, 0.86397994\n",
    ", 0.75136499, 1.11068005, 1.26627141, 1.66521011, 0.99750603, 0.89477338\n",
    ", 1.22469546, 0.71488525, 1.44985682, 1.12219983, 1.22642495, 0.86819807\n",
    ", 0.95986557, 1.23142302, 0.81214284, 0.67746308, 1.00690961, 0.95587001\n",
    ", 1.4200546, 0.65757342, 0.78818154, 0.89232315, 0.81914443, 0.76466677\n",
    ", 0.7983191, 0.82793738, 0.85807479, 0.8224566, 0.78738265, 1.47025078\n",
    ", 1.19951112, 1.2040894, 0.94259879, 0.80715694, 1.11184515, 0.63483471\n",
    ", 1.13313472, 1.05243056, 0.87514524, 1.67005402, 0.862337, 0.67911068\n",
    ", 1.05266955, 0.7748282, 1.25342226, 1.26187574, 0.90096771, 0.88453938\n",
    ", 1.11914874, 1.08272728, 1.0683505, 0.83556591, 0.76477427, 0.91128541\n",
    ", 1.26674233, 0.92334216, 0.78757248, 1.19857263, 0.84077996, 0.86083711\n",
    ", 1.03330723, 0.98194791, 1.21986751, 1.63670788, 1.02694419, 0.70286656\n",
    ", 0.90663071, 1.33366441, 1.11390551, 0.86586875, 1.58646817, 1.04841778\n",
    ", 1.12230492, 0.79567624, 1.18097354, 1.07825851, 1.12660943, 1.04212653\n",
    ", 0.91695336, 1.25433897, 0.94266378, 1.11215407, 0.72935826, 0.76088341\n",
    ", 0.931085, 1.05899334, 1.8259861, 0.91874732, 0.61429458, 1.03770328\n",
    ", 0.82013083, 1.09337602, 1.01910799, 0.6806872, 1.18878701, 0.86793008\n",
    ", 1.18087911, 0.87001041, 0.73416922, 0.8967718, 0.94117019, 1.14080505\n",
    ", 0.78194158, 0.63973257, 1.27371626, 1.04440407, 1.00156664, 0.84381549\n",
    ", 0.71709606, 0.99613254, 1.24006755, 0.73606457, 1.33987664, 0.99302416\n",
    ", 0.96541988, 1.28297668, 0.75528914, 1.26241501, 0.89946114, 1.06764348\n",
    ", 0.643176, 0.58360713, 1.02761103, 1.07593344, 1.12800901, 1.04378883]\n",
    "weights = np.array(weights)\n",
    "weights[weights==0] = 0.00001\n",
    "scan1 = pC2DMS.Scan(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name) + '/') + str(10000)+ ' scans/')\n",
    "cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc(weights), numScans='all')\n",
    "top_feat = cmap.analyse(3000)\n",
    "np.save(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name) + '/') + str(10000)+ ' scans/w_top_feat_cma(loss_func1,bins:100)_new.npy', top_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import pC2DMS\n",
    "weights = [0.004339374, 0.50798032, 1.64351229, 0.67775987, 0.01657082, 0.75269187\n",
    ", 1.18337271, 0.93812863, 0.48505375, 0.21487606, 1.09743312, 0.71238726\n",
    ", 1.55784343, 1.51592355, 0.35794554, 0.12552721, 0.85348355, 0.83238549\n",
    ", 1.2352135, 1.30421881, 1.18605693, 0.40048151, 1.35825518, 0.31463801\n",
    ", 1.06113056, 1.40472375, 0.53190937, 1.1505039, 1.05797006, 0.85244303\n",
    ", 1.00309384, 0.51703579, 0.46421245, 0.90394702, 0.92111887, 1.03359514\n",
    ", 1.24296482, 0.89522082, 1.1051857]\n",
    "\n",
    "weights = np.array(weights)\n",
    "weights[weights==0] = 0.00001\n",
    "scan1 = pC2DMS.Scan(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name) + '/') + str(10000)+ ' scans/')\n",
    "cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc(weights), numScans='all')\n",
    "top_feat = cmap.analyse(3000)\n",
    "np.save(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name) + '/') + str(10000)+ ' scans/w_top_feat_cma(loss_func3,bins:100)_new.npy', top_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import pC2DMS\n",
    "weights = [0.74160025, 1.17554347, 1.22119417, 1.36680097, 1.08999095, 1.22091697\n",
    ", 0.99335227, 0.97095117, 0.95386964, 0.76903355, 0.82569467, 0.89859331\n",
    ", 1.1068456, 0.74264423, 0.79489182, 0.7577499, 0.97953646, 1.08452884\n",
    ", 0.9237643, 0.91474233, 1.03872657, 1.01189228, 1.12106478, 0.66953374\n",
    ", 1.36448624, 1.35191527, 0.95438715, 1.05684252, 0.90583876, 0.84336418\n",
    ", 1.19558659, 1.18986215, 1.16811959, 0.85614931, 0.79294955, 1.28305282\n",
    ", 0.9318661, 0.89405129, 0.80366216, 1.65484042, 0.87095745, 0.95280496\n",
    ", 0.84949559, 0.95754092, 0.84557664, 0.98703843, 0.7958573, 1.13044703\n",
    ", 2.17460658, 1.0500949, 0.71339967, 0.82321625, 0.9881275, 0.88146648\n",
    ", 1.27887592, 0.78665736, 0.84669591, 1.18625178, 1.07888715, 0.69218346\n",
    ", 0.80818622, 0.83819566, 0.73283246, 1.01583718, 1.50590281, 0.81426785\n",
    ", 1.06637176, 0.86762882, 1.11352546, 0.95965498, 0.6892157, 0.79029643\n",
    ", 1.03726876, 1.0607466, 0.90106773, 0.86688621, 0.90564701, 0.57473586\n",
    ", 0.89276744, 0.8087, 0.58280645, 1.142229, 0.76628774, 0.88648582\n",
    ", 0.99050494, 1.07598116, 0.96590563, 0.88465524, 1.00834114, 0.965941\n",
    ", 0.6895089, 0.65006118, 1.02729331, 1.10461439, 0.55728501, 1.43483551\n",
    ", 0.96925734, 0.9050727, 1.96655056, 0.94289558, 1.74285389, 0.68880273\n",
    ", 0.90954084, 1.0566941, 1.11382684, 0.95825931, 1.12884838, 1.04043493\n",
    ", 0.89725065, 1.00061915, 0.97365071, 1.22249004, 0.95852889, 0.89027841\n",
    ", 1.07254553, 0.79384854, 1.10607781, 0.82076046, 0.83609501, 1.08345376\n",
    ", 0.99252562, 1.03735257, 0.95148837, 0.88197611, 1.22194952, 1.1413378\n",
    ", 0.8436952, 0.78352289, 0.97301558, 1.14048605, 1.09396806, 1.23491957]\n",
    "\n",
    "weights = np.array(weights)\n",
    "weights[weights==0] = 0.00001\n",
    "scan1 = pC2DMS.Scan(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name) + '/') + str(10000)+ ' scans/')\n",
    "cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc(weights), numScans='all')\n",
    "top_feat = cmap.analyse(3000)\n",
    "np.save(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name) + '/') + str(10000)+ ' scans/w_top_feat_cma(loss_func4,bins:100)_new.npy', top_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import pC2DMS\n",
    "weights = np.load('weights_cma.npy')\n",
    "with open('optimizer_cma_loss_func3.dill', \"rb\") as file:\n",
    "    optimizer = dill.load(file)\n",
    "scan1 = pC2DMS.Scan(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + list(datafile_dict.values())[2][13] + '/') + str(10000)+ ' scans/')\n",
    "cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc(optimizer.result.xbest), numScans='all')\n",
    "top_feat = cmap.analyse(3000)\n",
    "np.save(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + list(datafile_dict.values())[2][13] + '/') + str(10000)+ ' scans/w_top_feat_cma(loss_func3,bins:100).npy', top_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7255_2d-PC-MS_5pmol-ul_Energystepping_1AGC_0-7ltqiso_35CID', '20160603_1005_ME9_2+_CVscan_NCE35_Turbo', '20160629_1557_ME15_2+_CVscan_NCE25', '20160603_1040_ME9_3+_CVscan_NCE35_Turbo', '6727_2d-PC-MS_1pmol-ul_1AGC_0-7quadiso_HCD', '20160318_1809_PH4_2+_CVScan_Turbo', '20160708_1939_UN15_2+_0,01mM_CVscan_Turbo', '20160622_1514_ME14_3+_1to2500_CVscan_NCE35', '20160318_1424_SU5_2+_CVScan_Turbo', '.ipynb_checkpoints', '20160602_1249_ME8_3+_CVscan_NCE35_Turbo', 'SU4_2+_TurboCVscan_NCE20_20160224_1604', '20160629_1625_ME15_2+_CVscan_NCE35', '20160511_2003_ME17_2+_CVscan_Turbo', '20160708_1747_UN14_2+_0,01mM_CVscan_Turbo', '20160629_1738_ME15_3+_CVscan_NCE35', '20160622_1448_ME14_2+_1to2500_CVscan_NCE35', '20160428_2100_ME16_3+_CVscan_NCE35_Turbo', '7255_2d-PC-MS_5pmol-ul_AGCstepping_0.8AGC_0-7ltqiso_CID', '20160503_1649_ME4_2+_CVscan_NCE35_Turbo', '7255_2d-PC-MS_5pmol-ul_Energystepping_1AGC_0-7ltqiso_25HCD', '7255_2d-PC-MS_10pmol-ul_1AGC_0-7quadiso_CID', 'PH8_2+_CVscan_NCE35_Turbo_20160505_1308', '7255_2d-PC-MS_5pmol-ul_AGCstepping_0.5AGC_0-7ltqiso_HCD', '20160503_1127_PH4_3+_CVscan_NCE35_Turbo', '20160602_1158_ME8_2+_CVscan_NCE35_Turbo_isoWidth2', '20160428_2222_ME16_2+_CVScan_NCE35_Turbo', '20160504_0930_ME4_3+_CVscan_NCE35_Turbo', '20150626_1623_PH15_2+_CID_NCE_10_CVscan_scanTime60mins', '20160511_2120_ME17_3+_CVscan_Turbo']\n"
     ]
    }
   ],
   "source": [
    "print(list(datafile_dict.values())[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./peptide output/Test/20160603_1005_ME9_2+_CVscan_NCE35_Turbo/10000 scans/\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pC2DMS\n",
    "import itertools\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "numscan_list = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "\n",
    "for i in numscan_list[::-1]:\n",
    "    scan_low = pC2DMS.Scan(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + list(datafile_dict.values())[2][1] + '/') + str(i) + ' scans/')\n",
    "    if os.path.exists(os.path.join(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + list(datafile_dict.values())[2][1] + '/') + str(i) + ' scans/', 'top_indices_tic.npy')):\n",
    "        indexlist = np.load(os.path.join(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + list(datafile_dict.values())[2][1] + '/') + str(i) + ' scans/', 'top_indices_tic.npy'))\n",
    "    else:\n",
    "        cmap = pC2DMS.PCovMap(scan_low, scan_low.tic(), numScans=i)\n",
    "        indexlist = cmap.topNfeats(3000)\n",
    "        topfeat = cmap.sampleFeatsIndex(indexlist)\n",
    "        topfeat_sorted = topfeat[np.flip(topfeat[:, 3].argsort())]\n",
    "        np.save(os.path.join(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + list(datafile_dict.values())[2][1] + '/') + str(i) + ' scans/', 'top_indices_tic.npy'), topfeat_sorted)\n",
    "        np.savetxt(os.path.join(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + list(datafile_dict.values())[2][1] + '/') + str(i) + ' scans/', 'top_indices_tic.csv'), topfeat_sorted, fmt='%.2f', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rds/general/user/ww1922/home/src/peptide output/Test/6727_2d-PC-MS_1pmol-ul_1AGC_0-7quadiso_HCD/3000 scans\n",
      "map time 2.557283401489258\n",
      "Feature file exists. Features loaded. Analysis skipped.\n",
      "/rds/general/user/ww1922/home/src/peptide output/Test/6727_2d-PC-MS_1pmol-ul_1AGC_0-7quadiso_HCD/6000 scans\n",
      "map time 2.779189109802246\n",
      "Feature file exists. Features loaded. Analysis skipped.\n",
      "/rds/general/user/ww1922/home/src/peptide output/Test/6727_2d-PC-MS_1pmol-ul_1AGC_0-7quadiso_HCD/9000 scans\n",
      "map time 2.657087564468384\n",
      "Feature file exists. Features loaded. Analysis skipped.\n",
      "/rds/general/user/ww1922/home/src/peptide output/Test/6727_2d-PC-MS_1pmol-ul_1AGC_0-7quadiso_HCD/2000 scans\n",
      "map time 2.1841752529144287\n",
      "Feature file exists. Features loaded. Analysis skipped.\n",
      "/rds/general/user/ww1922/home/src/peptide output/Test/6727_2d-PC-MS_1pmol-ul_1AGC_0-7quadiso_HCD/10000 scans\n",
      "map time 2.546226978302002\n",
      "Feature file exists. Features loaded. Analysis skipped.\n",
      "/rds/general/user/ww1922/home/src/peptide output/Test/6727_2d-PC-MS_1pmol-ul_1AGC_0-7quadiso_HCD/4000 scans\n",
      "map time 2.726982593536377\n",
      "Feature file exists. Features loaded. Analysis skipped.\n",
      "/rds/general/user/ww1922/home/src/peptide output/Test/6727_2d-PC-MS_1pmol-ul_1AGC_0-7quadiso_HCD/7000 scans\n",
      "map time 2.918510913848877\n",
      "Feature file exists. Features loaded. Analysis skipped.\n",
      "/rds/general/user/ww1922/home/src/peptide output/Test/6727_2d-PC-MS_1pmol-ul_1AGC_0-7quadiso_HCD/1000 scans\n",
      "map time 2.6106486320495605\n",
      "Feature file exists. Features loaded. Analysis skipped.\n",
      "/rds/general/user/ww1922/home/src/peptide output/Test/6727_2d-PC-MS_1pmol-ul_1AGC_0-7quadiso_HCD/5000 scans\n",
      "map time 2.1955084800720215\n",
      "Feature file exists. Features loaded. Analysis skipped.\n",
      "/rds/general/user/ww1922/home/src/peptide output/Test/6727_2d-PC-MS_1pmol-ul_1AGC_0-7quadiso_HCD/8000 scans\n",
      "map time 2.374530076980591\n",
      "found 100 good features\n",
      "found 200 good features\n",
      "found 300 good features\n",
      "found 400 good features\n",
      "found 500 good features\n",
      "found 600 good features\n",
      "found 700 good features\n",
      "found 800 good features\n",
      "found 900 good features\n",
      "found 1000 good features\n",
      "found 1100 good features\n",
      "found 1200 good features\n",
      "found 1300 good features\n",
      "found 1400 good features\n",
      "found 1500 good features\n",
      "found 1600 good features\n",
      "found 1700 good features\n",
      "found 1800 good features\n",
      "found 1900 good features\n",
      "found 2000 good features\n",
      "found 2100 good features\n",
      "found 2200 good features\n",
      "found 2300 good features\n",
      "found 2400 good features\n",
      "found 2500 good features\n",
      "found 2600 good features\n",
      "found 2700 good features\n",
      "found 2800 good features\n",
      "found 2900 good features\n",
      "found 3000 good features\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "sig calculated for feature 350\n",
      "sig calculated for feature 400\n",
      "sig calculated for feature 450\n",
      "sig calculated for feature 500\n",
      "sig calculated for feature 550\n",
      "sig calculated for feature 600\n",
      "sig calculated for feature 650\n",
      "sig calculated for feature 700\n",
      "sig calculated for feature 750\n",
      "sig calculated for feature 800\n",
      "sig calculated for feature 850\n",
      "sig calculated for feature 900\n",
      "sig calculated for feature 950\n",
      "sig calculated for feature 1000\n",
      "sig calculated for feature 1050\n",
      "sig calculated for feature 1100\n",
      "sig calculated for feature 1150\n",
      "sig calculated for feature 1200\n",
      "sig calculated for feature 1250\n",
      "sig calculated for feature 1300\n",
      "sig calculated for feature 1350\n",
      "sig calculated for feature 1400\n",
      "sig calculated for feature 1450\n",
      "sig calculated for feature 1500\n",
      "sig calculated for feature 1550\n",
      "sig calculated for feature 1600\n",
      "sig calculated for feature 1650\n",
      "sig calculated for feature 1700\n",
      "sig calculated for feature 1750\n",
      "sig calculated for feature 1800\n",
      "sig calculated for feature 1850\n",
      "sig calculated for feature 1900\n",
      "sig calculated for feature 1950\n",
      "sig calculated for feature 2000\n",
      "sig calculated for feature 2050\n",
      "sig calculated for feature 2100\n",
      "sig calculated for feature 2150\n",
      "sig calculated for feature 2200\n",
      "sig calculated for feature 2250\n",
      "sig calculated for feature 2300\n",
      "sig calculated for feature 2350\n",
      "sig calculated for feature 2400\n",
      "sig calculated for feature 2450\n",
      "sig calculated for feature 2500\n",
      "sig calculated for feature 2550\n",
      "sig calculated for feature 2600\n",
      "sig calculated for feature 2650\n",
      "sig calculated for feature 2700\n",
      "sig calculated for feature 2750\n",
      "sig calculated for feature 2800\n",
      "sig calculated for feature 2850\n",
      "sig calculated for feature 2900\n",
      "sig calculated for feature 2950\n",
      "sig calculated for feature 3000\n"
     ]
    }
   ],
   "source": [
    "if args.run_all:\n",
    "    run(list(datafile_dict)[2], [list(datafile_dict.values())[2][2]], numscanInterval=1000, weights=args.weights, epoch=args.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rds/general/user/ww1922/home/src/peptide output/Test/20160603_1040_ME9_3+_CVscan_NCE35_Turbo/3000 scans\n",
      "map time 0.5093557834625244\n",
      "found 100 good features\n",
      "found 200 good features\n",
      "found 300 good features\n",
      "found 400 good features\n",
      "found 500 good features\n",
      "found 600 good features\n",
      "found 700 good features\n",
      "found 800 good features\n",
      "found 900 good features\n",
      "found 1000 good features\n",
      "found 1100 good features\n",
      "found 1200 good features\n",
      "found 1300 good features\n",
      "found 1400 good features\n",
      "found 1500 good features\n",
      "found 1600 good features\n",
      "found 1700 good features\n",
      "found 1800 good features\n",
      "found 1900 good features\n",
      "found 2000 good features\n",
      "found 2100 good features\n",
      "found 2200 good features\n",
      "found 2300 good features\n",
      "found 2400 good features\n",
      "found 2500 good features\n",
      "found 2600 good features\n",
      "found 2700 good features\n",
      "found 2800 good features\n",
      "found 2900 good features\n",
      "found 3000 good features\n"
     ]
    }
   ],
   "source": [
    "if args.run_all:\n",
    "    run(list(datafile_dict)[2], [list(datafile_dict.values())[2][3]], numscanInterval=1000, weights=args.weights, epoch=args.epoch)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: ./peptide output/CID/6727_2D-MS-PC_turbo/10000 scans\n",
      "all scans\n",
      "save in ./peptide output/CID/6727_2D-MS-PC_turbo/10000 scans/save\n",
      "Reading file ../IC/IC/raw data/CID/6727_2D-MS-PC_turbo.txt to ./peptide output/CID/6727_2D-MS-PC_turbo/10000 scans\n",
      "Reading scan number 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To prove the code is forward compatible\n",
    "\n",
    "datafile_list=['6727_2D-MS-PC_turbo']\n",
    "\n",
    "'''if Path('./peptide output/CID/' + str(datafile_list[0])).is_dir():\n",
    "    shutil.rmtree('./peptide output/CID/' + str(datafile_list[0]))\n",
    "'''\n",
    "for prefix in datafile_list:\n",
    "    for mode in ['tic']:\n",
    "        cmap = run_covmap(prefix, mode, data_path=os.path.join('../IC/IC/raw data', 'CID', ''), cache_path=os.path.join('./peptide output', 'CID'+'/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '7302_2D-PC-MS_1pmol-ul_1AGC_0-7quadiso'\n",
    "name1 = '6727_2d-PC-MS_1pmol-ul_1AGC_0-7quadiso_HCD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: ./peptide output/HCD/7302_2D-PC-MS_1pmol-ul_1AGC_0-7quadiso/10000 scans\n",
      "all scans\n",
      "save in ./peptide output/HCD/7302_2D-PC-MS_1pmol-ul_1AGC_0-7quadiso/10000 scans/save\n",
      "Reading file ../IC/IC/raw data/HCD/7302_2D-PC-MS_1pmol-ul_1AGC_0-7quadiso.txt to ./peptide output/HCD/7302_2D-PC-MS_1pmol-ul_1AGC_0-7quadiso/10000 scans\n",
      "Reading scan number 100\n",
      "Reading scan number 200\n",
      "Reading scan number 300\n",
      "Reading scan number 400\n",
      "Reading scan number 500\n",
      "Reading scan number 600\n",
      "Reading scan number 700\n",
      "Reading scan number 800\n",
      "Reading scan number 900\n",
      "Reading scan number 1000\n",
      "Reading scan number 1100\n",
      "Reading scan number 1200\n",
      "Reading scan number 1300\n",
      "Reading scan number 1400\n",
      "Reading scan number 1500\n",
      "Reading scan number 1600\n",
      "Reading scan number 1700\n",
      "Reading scan number 1800\n",
      "Reading scan number 1900\n",
      "Reading scan number 2000\n",
      "Reading scan number 2100\n",
      "Reading scan number 2200\n",
      "Reading scan number 2300\n",
      "Reading scan number 2400\n",
      "Reading scan number 2500\n",
      "Reading scan number 2600\n",
      "Reading scan number 2700\n",
      "Reading scan number 2800\n",
      "Reading scan number 2900\n",
      "Reading scan number 3000\n",
      "Reading scan number 3100\n",
      "Reading scan number 3200\n",
      "Reading scan number 3300\n",
      "Reading scan number 3400\n",
      "Reading scan number 3500\n",
      "Reading scan number 3600\n",
      "Reading scan number 3700\n",
      "Reading scan number 3800\n",
      "Reading scan number 3900\n",
      "Reading scan number 4000\n",
      "Reading scan number 4100\n",
      "Reading scan number 4200\n",
      "Reading scan number 4300\n",
      "Reading scan number 4400\n",
      "Reading scan number 4500\n",
      "Reading scan number 4600\n",
      "Reading scan number 4700\n",
      "Reading scan number 4800\n",
      "Reading scan number 4900\n",
      "Reading scan number 5000\n",
      "Reading scan number 5100\n",
      "Reading scan number 5200\n",
      "Reading scan number 5300\n",
      "Reading scan number 5400\n",
      "Reading scan number 5500\n",
      "Reading scan number 5600\n",
      "Reading scan number 5700\n",
      "Reading scan number 5800\n",
      "Reading scan number 5900\n",
      "Reading scan number 6000\n",
      "Reading scan number 6100\n",
      "Reading scan number 6200\n",
      "Reading scan number 6300\n",
      "Reading scan number 6400\n",
      "Reading scan number 6500\n",
      "Reading scan number 6600\n",
      "Reading scan number 6700\n",
      "Reading scan number 6800\n",
      "Reading scan number 6900\n",
      "Reading scan number 7000\n",
      "Reading scan number 7100\n",
      "Reading scan number 7200\n",
      "Reading scan number 7300\n",
      "Reading scan number 7400\n",
      "Reading scan number 7500\n",
      "Reading scan number 7600\n",
      "Reading scan number 7700\n",
      "Reading scan number 7800\n",
      "Reading scan number 7900\n",
      "Reading scan number 8000\n",
      "Reading scan number 8100\n",
      "Reading scan number 8200\n",
      "Reading scan number 8300\n",
      "Reading scan number 8400\n",
      "Reading scan number 8500\n",
      "Reading scan number 8600\n",
      "Reading scan number 8700\n",
      "Reading scan number 8800\n",
      "Reading scan number 8900\n",
      "Reading scan number 9000\n",
      "Reading scan number 9100\n",
      "Reading scan number 9200\n",
      "Reading scan number 9300\n",
      "Reading scan number 9400\n",
      "Reading scan number 9500\n",
      "Reading scan number 9600\n",
      "Reading scan number 9700\n",
      "Reading scan number 9800\n",
      "Reading scan number 9900\n",
      "Reading scan number 10000\n",
      "readTextFile complete for ./peptide output/HCD/7302_2D-PC-MS_1pmol-ul_1AGC_0-7quadiso/10000 scans\n",
      "readTextFile complete for /rds/general/user/ww1922/home/src/peptide output/HCD/7302_2D-PC-MS_1pmol-ul_1AGC_0-7quadiso/5000 scans\n",
      "/rds/general/user/ww1922/home/src/peptide output/HCD/7302_2D-PC-MS_1pmol-ul_1AGC_0-7quadiso/10000 scans\n",
      "Syx not saved for this map, beginning calculation with saveSyxEinSum...\n",
      "Performing saveSyxEinSum for /rds/general/user/ww1922/home/src/peptide output/HCD/7302_2D-PC-MS_1pmol-ul_1AGC_0-7quadiso/10000 scans for 10000 scans\n",
      "Completed saveSyxEinSum for /rds/general/user/ww1922/home/src/peptide output/HCD/7302_2D-PC-MS_1pmol-ul_1AGC_0-7quadiso/10000 scans for 10000 scans\n",
      "map time 408.9733202457428\n",
      "found 100 good features\n",
      "found 200 good features\n",
      "found 300 good features\n",
      "found 400 good features\n",
      "found 500 good features\n",
      "found 600 good features\n",
      "found 700 good features\n",
      "found 800 good features\n",
      "found 900 good features\n",
      "found 1000 good features\n",
      "found 1100 good features\n",
      "found 1200 good features\n",
      "found 1300 good features\n",
      "found 1400 good features\n",
      "found 1500 good features\n",
      "found 1600 good features\n",
      "found 1700 good features\n",
      "found 1800 good features\n",
      "found 1900 good features\n",
      "found 2000 good features\n",
      "found 2100 good features\n",
      "found 2200 good features\n",
      "found 2300 good features\n",
      "found 2400 good features\n",
      "found 2500 good features\n",
      "found 2600 good features\n",
      "found 2700 good features\n",
      "found 2800 good features\n",
      "found 2900 good features\n",
      "found 3000 good features\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "sig calculated for feature 350\n",
      "sig calculated for feature 400\n",
      "sig calculated for feature 450\n",
      "sig calculated for feature 500\n",
      "sig calculated for feature 550\n",
      "sig calculated for feature 600\n",
      "sig calculated for feature 650\n",
      "sig calculated for feature 700\n",
      "sig calculated for feature 750\n",
      "sig calculated for feature 800\n",
      "sig calculated for feature 850\n",
      "sig calculated for feature 900\n",
      "sig calculated for feature 950\n",
      "sig calculated for feature 1000\n",
      "sig calculated for feature 1050\n",
      "sig calculated for feature 1100\n",
      "sig calculated for feature 1150\n",
      "sig calculated for feature 1200\n",
      "sig calculated for feature 1250\n",
      "sig calculated for feature 1300\n",
      "sig calculated for feature 1350\n",
      "sig calculated for feature 1400\n",
      "sig calculated for feature 1450\n",
      "sig calculated for feature 1500\n",
      "sig calculated for feature 1550\n",
      "sig calculated for feature 1600\n",
      "sig calculated for feature 1650\n",
      "sig calculated for feature 1700\n",
      "sig calculated for feature 1750\n",
      "sig calculated for feature 1800\n",
      "sig calculated for feature 1850\n",
      "sig calculated for feature 1900\n",
      "sig calculated for feature 1950\n",
      "sig calculated for feature 2000\n",
      "sig calculated for feature 2050\n",
      "sig calculated for feature 2100\n",
      "sig calculated for feature 2150\n",
      "sig calculated for feature 2200\n",
      "sig calculated for feature 2250\n",
      "sig calculated for feature 2300\n",
      "sig calculated for feature 2350\n",
      "sig calculated for feature 2400\n",
      "sig calculated for feature 2450\n",
      "sig calculated for feature 2500\n",
      "sig calculated for feature 2550\n",
      "sig calculated for feature 2600\n",
      "sig calculated for feature 2650\n",
      "sig calculated for feature 2700\n",
      "sig calculated for feature 2750\n",
      "sig calculated for feature 2800\n",
      "sig calculated for feature 2850\n",
      "sig calculated for feature 2900\n",
      "sig calculated for feature 2950\n",
      "sig calculated for feature 3000\n",
      "/rds/general/user/ww1922/home/src/peptide output/HCD/7302_2D-PC-MS_1pmol-ul_1AGC_0-7quadiso/5000 scans\n",
      "Syx not saved for this map, beginning calculation with saveSyxEinSum...\n",
      "Performing saveSyxEinSum for /rds/general/user/ww1922/home/src/peptide output/HCD/7302_2D-PC-MS_1pmol-ul_1AGC_0-7quadiso/5000 scans for 5000 scans\n",
      "Completed saveSyxEinSum for /rds/general/user/ww1922/home/src/peptide output/HCD/7302_2D-PC-MS_1pmol-ul_1AGC_0-7quadiso/5000 scans for 5000 scans\n",
      "map time 206.67497158050537\n",
      "found 100 good features\n",
      "found 200 good features\n",
      "found 300 good features\n",
      "found 400 good features\n",
      "found 500 good features\n",
      "found 600 good features\n",
      "found 700 good features\n",
      "found 800 good features\n",
      "found 900 good features\n",
      "found 1000 good features\n",
      "found 1100 good features\n",
      "found 1200 good features\n",
      "found 1300 good features\n",
      "found 1400 good features\n",
      "found 1500 good features\n",
      "found 1600 good features\n",
      "found 1700 good features\n",
      "found 1800 good features\n",
      "found 1900 good features\n",
      "found 2000 good features\n",
      "found 2100 good features\n",
      "found 2200 good features\n",
      "found 2300 good features\n",
      "found 2400 good features\n",
      "found 2500 good features\n",
      "found 2600 good features\n",
      "found 2700 good features\n",
      "found 2800 good features\n",
      "found 2900 good features\n",
      "found 3000 good features\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "sig calculated for feature 350\n",
      "sig calculated for feature 400\n",
      "sig calculated for feature 450\n",
      "sig calculated for feature 500\n",
      "sig calculated for feature 550\n",
      "sig calculated for feature 600\n",
      "sig calculated for feature 650\n",
      "sig calculated for feature 700\n",
      "sig calculated for feature 750\n",
      "sig calculated for feature 800\n",
      "sig calculated for feature 850\n",
      "sig calculated for feature 900\n",
      "sig calculated for feature 950\n",
      "sig calculated for feature 1000\n",
      "sig calculated for feature 1050\n",
      "sig calculated for feature 1100\n",
      "sig calculated for feature 1150\n",
      "sig calculated for feature 1200\n",
      "sig calculated for feature 1250\n",
      "sig calculated for feature 1300\n",
      "sig calculated for feature 1350\n",
      "sig calculated for feature 1400\n",
      "sig calculated for feature 1450\n",
      "sig calculated for feature 1500\n",
      "sig calculated for feature 1550\n",
      "sig calculated for feature 1600\n",
      "sig calculated for feature 1650\n",
      "sig calculated for feature 1700\n",
      "sig calculated for feature 1750\n",
      "sig calculated for feature 1800\n",
      "sig calculated for feature 1850\n",
      "sig calculated for feature 1900\n",
      "sig calculated for feature 1950\n",
      "sig calculated for feature 2000\n",
      "sig calculated for feature 2050\n",
      "sig calculated for feature 2100\n",
      "sig calculated for feature 2150\n",
      "sig calculated for feature 2200\n",
      "sig calculated for feature 2250\n",
      "sig calculated for feature 2300\n",
      "sig calculated for feature 2350\n",
      "sig calculated for feature 2400\n",
      "sig calculated for feature 2450\n",
      "sig calculated for feature 2500\n",
      "sig calculated for feature 2550\n",
      "sig calculated for feature 2600\n",
      "sig calculated for feature 2650\n",
      "sig calculated for feature 2700\n",
      "sig calculated for feature 2750\n",
      "sig calculated for feature 2800\n",
      "sig calculated for feature 2850\n",
      "sig calculated for feature 2900\n",
      "sig calculated for feature 2950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-22 04:45:02,373 - root - INFO - The running time of 7302_2D-PC-MS_1pmol-ul_1AGC_0-7quadiso: 3:36:21.177922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sig calculated for feature 3000\n"
     ]
    }
   ],
   "source": [
    "if args.run_all:\n",
    "    run(list(datafile_dict)[3], [str(name)], numscanInterval=5000, weights=args.weights, epoch=args.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pC2DMS\n",
    "import itertools\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "numscan_list = [10000]\n",
    "\n",
    "for i in numscan_list[::-1]:\n",
    "    scan_low = pC2DMS.Scan(os.path.join('./peptide output', list(datafile_dict.keys())[3] + '/' + str(name) + '/') + str(i) + ' scans/')\n",
    "    if os.path.exists(os.path.join(os.path.join('./peptide output', list(datafile_dict.keys())[3] + '/' + str(name) + '/') + str(i) + ' scans/', 'top_indices_tic.npy')):\n",
    "        indexlist = np.load(os.path.join(os.path.join('./peptide output', list(datafile_dict.keys())[3] + '/' + str(name) + '/') + str(i) + ' scans/', 'top_indices_tic.npy'))\n",
    "    else:\n",
    "        cmap = pC2DMS.PCovMap(scan_low, scan_low.tic(), numScans=i)\n",
    "        indexlist = cmap.topNfeats(3000)\n",
    "        topfeat = cmap.sampleFeatsIndex(indexlist)\n",
    "        topfeat_sorted = topfeat[np.flip(topfeat[:, 3].argsort())]\n",
    "        np.save(os.path.join(os.path.join('./peptide output', list(datafile_dict.keys())[3] + '/' + str(name) + '/') + str(i) + ' scans/', 'top_indices_tic.npy'), topfeat_sorted)\n",
    "        np.savetxt(os.path.join(os.path.join('./peptide output', list(datafile_dict.keys())[3] + '/' + str(name) + '/') + str(i) + ' scans/', 'top_indices_tic.csv'), topfeat_sorted, fmt='%.2f', delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 100 good features\n",
      "found 200 good features\n",
      "found 300 good features\n",
      "found 400 good features\n",
      "found 500 good features\n",
      "found 600 good features\n",
      "found 700 good features\n",
      "found 800 good features\n",
      "found 900 good features\n",
      "found 1000 good features\n",
      "found 1100 good features\n",
      "found 1200 good features\n",
      "found 1300 good features\n",
      "found 1400 good features\n",
      "found 1500 good features\n",
      "found 1600 good features\n",
      "found 1700 good features\n",
      "found 1800 good features\n",
      "found 1900 good features\n",
      "found 2000 good features\n",
      "found 2100 good features\n",
      "found 2200 good features\n",
      "found 2300 good features\n",
      "found 2400 good features\n",
      "found 2500 good features\n",
      "found 2600 good features\n",
      "found 2700 good features\n",
      "found 2800 good features\n",
      "found 2900 good features\n",
      "found 3000 good features\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "sig calculated for feature 350\n",
      "sig calculated for feature 400\n",
      "sig calculated for feature 450\n",
      "sig calculated for feature 500\n",
      "sig calculated for feature 550\n",
      "sig calculated for feature 600\n",
      "sig calculated for feature 650\n",
      "sig calculated for feature 700\n",
      "sig calculated for feature 750\n",
      "sig calculated for feature 800\n",
      "sig calculated for feature 850\n",
      "sig calculated for feature 900\n",
      "sig calculated for feature 950\n",
      "sig calculated for feature 1000\n",
      "sig calculated for feature 1050\n",
      "sig calculated for feature 1100\n",
      "sig calculated for feature 1150\n",
      "sig calculated for feature 1200\n",
      "sig calculated for feature 1250\n",
      "sig calculated for feature 1300\n",
      "sig calculated for feature 1350\n",
      "sig calculated for feature 1400\n",
      "sig calculated for feature 1450\n",
      "sig calculated for feature 1500\n",
      "sig calculated for feature 1550\n",
      "sig calculated for feature 1600\n",
      "sig calculated for feature 1650\n",
      "sig calculated for feature 1700\n",
      "sig calculated for feature 1750\n",
      "sig calculated for feature 1800\n",
      "sig calculated for feature 1850\n",
      "sig calculated for feature 1900\n",
      "sig calculated for feature 1950\n",
      "sig calculated for feature 2000\n",
      "sig calculated for feature 2050\n",
      "sig calculated for feature 2100\n",
      "sig calculated for feature 2150\n",
      "sig calculated for feature 2200\n",
      "sig calculated for feature 2250\n",
      "sig calculated for feature 2300\n",
      "sig calculated for feature 2350\n",
      "sig calculated for feature 2400\n",
      "sig calculated for feature 2450\n",
      "sig calculated for feature 2500\n",
      "sig calculated for feature 2550\n",
      "sig calculated for feature 2600\n",
      "sig calculated for feature 2650\n",
      "sig calculated for feature 2700\n",
      "sig calculated for feature 2750\n",
      "sig calculated for feature 2800\n",
      "sig calculated for feature 2850\n",
      "sig calculated for feature 2900\n",
      "sig calculated for feature 2950\n",
      "sig calculated for feature 3000\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "import pC2DMS\n",
    "weights = np.load(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name1) + '/') + str(10000)+ ' scans/' + 'weights_tpe.npy')\n",
    "with open(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name1) + '/') + str(10000)+ ' scans/' + 'optimizer_tpe.dill', \"rb\") as file:\n",
    "    optimizer = dill.load(file)\n",
    "scan1 = pC2DMS.Scan(os.path.join('./peptide output', list(datafile_dict.keys())[3] + '/' + str(name) + '/') + str(10000)+ ' scans/')\n",
    "cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc([optimizer.best_params[f'w_{i}'] for i in range(len(weights))]), numScans=10000)\n",
    "top_feat = cmap.analyse(3000)\n",
    "np.save(os.path.join('./peptide output', list(datafile_dict.keys())[3] + '/' + str(name) + '/') + str(10000)+ ' scans/w_top_feat_tpe(loss_func2,bins:100).npy', top_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 100 good features\n",
      "found 200 good features\n",
      "found 300 good features\n",
      "found 400 good features\n",
      "found 500 good features\n",
      "found 600 good features\n",
      "found 700 good features\n",
      "found 800 good features\n",
      "found 900 good features\n",
      "found 1000 good features\n",
      "found 1100 good features\n",
      "found 1200 good features\n",
      "found 1300 good features\n",
      "found 1400 good features\n",
      "found 1500 good features\n",
      "found 1600 good features\n",
      "found 1700 good features\n",
      "found 1800 good features\n",
      "found 1900 good features\n",
      "found 2000 good features\n",
      "found 2100 good features\n",
      "found 2200 good features\n",
      "found 2300 good features\n",
      "found 2400 good features\n",
      "found 2500 good features\n",
      "found 2600 good features\n",
      "found 2700 good features\n",
      "found 2800 good features\n",
      "found 2900 good features\n",
      "found 3000 good features\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "sig calculated for feature 350\n",
      "sig calculated for feature 400\n",
      "sig calculated for feature 450\n",
      "sig calculated for feature 500\n",
      "sig calculated for feature 550\n",
      "sig calculated for feature 600\n",
      "sig calculated for feature 650\n",
      "sig calculated for feature 700\n",
      "sig calculated for feature 750\n",
      "sig calculated for feature 800\n",
      "sig calculated for feature 850\n",
      "sig calculated for feature 900\n",
      "sig calculated for feature 950\n",
      "sig calculated for feature 1000\n",
      "sig calculated for feature 1050\n",
      "sig calculated for feature 1100\n",
      "sig calculated for feature 1150\n",
      "sig calculated for feature 1200\n",
      "sig calculated for feature 1250\n",
      "sig calculated for feature 1300\n",
      "sig calculated for feature 1350\n",
      "sig calculated for feature 1400\n",
      "sig calculated for feature 1450\n",
      "sig calculated for feature 1500\n",
      "sig calculated for feature 1550\n",
      "sig calculated for feature 1600\n",
      "sig calculated for feature 1650\n",
      "sig calculated for feature 1700\n",
      "sig calculated for feature 1750\n",
      "sig calculated for feature 1800\n",
      "sig calculated for feature 1850\n",
      "sig calculated for feature 1900\n",
      "sig calculated for feature 1950\n",
      "sig calculated for feature 2000\n",
      "sig calculated for feature 2050\n",
      "sig calculated for feature 2100\n",
      "sig calculated for feature 2150\n",
      "sig calculated for feature 2200\n",
      "sig calculated for feature 2250\n",
      "sig calculated for feature 2300\n",
      "sig calculated for feature 2350\n",
      "sig calculated for feature 2400\n",
      "sig calculated for feature 2450\n",
      "sig calculated for feature 2500\n",
      "sig calculated for feature 2550\n",
      "sig calculated for feature 2600\n",
      "sig calculated for feature 2650\n",
      "sig calculated for feature 2700\n",
      "sig calculated for feature 2750\n",
      "sig calculated for feature 2800\n",
      "sig calculated for feature 2850\n",
      "sig calculated for feature 2900\n",
      "sig calculated for feature 2950\n",
      "sig calculated for feature 3000\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "import pC2DMS\n",
    "weights = np.load(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name1) + '/') + str(10000)+ ' scans/' + 'weights_tpe.npy')\n",
    "with open(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name1) + '/') + str(10000)+ ' scans/' + 'optimizer_tpe_loss_func_1.dill', \"rb\") as file:\n",
    "    optimizer = dill.load(file)\n",
    "scan1 = pC2DMS.Scan(os.path.join('./peptide output', list(datafile_dict.keys())[3] + '/' + str(name) + '/') + str(10000)+ ' scans/')\n",
    "cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc([optimizer.best_params[f'w_{i}'] for i in range(len(weights))]), numScans=10000)\n",
    "top_feat = cmap.analyse(3000)\n",
    "np.save(os.path.join('./peptide output', list(datafile_dict.keys())[3] + '/' + str(name) + '/') + str(10000)+ ' scans/w_top_feat_tpe(loss_func1,bins:100).npy', top_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 100 good features\n",
      "found 200 good features\n",
      "found 300 good features\n",
      "found 400 good features\n",
      "found 500 good features\n",
      "found 600 good features\n",
      "found 700 good features\n",
      "found 800 good features\n",
      "found 900 good features\n",
      "found 1000 good features\n",
      "found 1100 good features\n",
      "found 1200 good features\n",
      "found 1300 good features\n",
      "found 1400 good features\n",
      "found 1500 good features\n",
      "found 1600 good features\n",
      "found 1700 good features\n",
      "found 1800 good features\n",
      "found 1900 good features\n",
      "found 2000 good features\n",
      "found 2100 good features\n",
      "found 2200 good features\n",
      "found 2300 good features\n",
      "found 2400 good features\n",
      "found 2500 good features\n",
      "found 2600 good features\n",
      "found 2700 good features\n",
      "found 2800 good features\n",
      "found 2900 good features\n",
      "found 3000 good features\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "sig calculated for feature 350\n",
      "sig calculated for feature 400\n",
      "sig calculated for feature 450\n",
      "sig calculated for feature 500\n",
      "sig calculated for feature 550\n",
      "sig calculated for feature 600\n",
      "sig calculated for feature 650\n",
      "sig calculated for feature 700\n",
      "sig calculated for feature 750\n",
      "sig calculated for feature 800\n",
      "sig calculated for feature 850\n",
      "sig calculated for feature 900\n",
      "sig calculated for feature 950\n",
      "sig calculated for feature 1000\n",
      "sig calculated for feature 1050\n",
      "sig calculated for feature 1100\n",
      "sig calculated for feature 1150\n",
      "sig calculated for feature 1200\n",
      "sig calculated for feature 1250\n",
      "sig calculated for feature 1300\n",
      "sig calculated for feature 1350\n",
      "sig calculated for feature 1400\n",
      "sig calculated for feature 1450\n",
      "sig calculated for feature 1500\n",
      "sig calculated for feature 1550\n",
      "sig calculated for feature 1600\n",
      "sig calculated for feature 1650\n",
      "sig calculated for feature 1700\n",
      "sig calculated for feature 1750\n",
      "sig calculated for feature 1800\n",
      "sig calculated for feature 1850\n",
      "sig calculated for feature 1900\n",
      "sig calculated for feature 1950\n",
      "sig calculated for feature 2000\n",
      "sig calculated for feature 2050\n",
      "sig calculated for feature 2100\n",
      "sig calculated for feature 2150\n",
      "sig calculated for feature 2200\n",
      "sig calculated for feature 2250\n",
      "sig calculated for feature 2300\n",
      "sig calculated for feature 2350\n",
      "sig calculated for feature 2400\n",
      "sig calculated for feature 2450\n",
      "sig calculated for feature 2500\n",
      "sig calculated for feature 2550\n",
      "sig calculated for feature 2600\n",
      "sig calculated for feature 2650\n",
      "sig calculated for feature 2700\n",
      "sig calculated for feature 2750\n",
      "sig calculated for feature 2800\n",
      "sig calculated for feature 2850\n",
      "sig calculated for feature 2900\n",
      "sig calculated for feature 2950\n",
      "sig calculated for feature 3000\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "import pC2DMS\n",
    "weights = np.load(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name1) + '/') + str(10000)+ ' scans/' + 'weights_tpe.npy')\n",
    "with open(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name1) + '/') + str(10000)+ ' scans/' + 'optimizer_tpe_loss_func3.dill', \"rb\") as file:\n",
    "    optimizer = dill.load(file)\n",
    "scan1 = pC2DMS.Scan(os.path.join('./peptide output', list(datafile_dict.keys())[3] + '/' + str(name) + '/') + str(10000)+ ' scans/')\n",
    "cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc([optimizer.best_params[f'w_{i}'] for i in range(len(weights))]), numScans=10000)\n",
    "top_feat = cmap.analyse(3000)\n",
    "np.save(os.path.join('./peptide output', list(datafile_dict.keys())[3] + '/' + str(name) + '/') + str(10000)+ ' scans/w_top_feat_tpe(loss_func3,bins:100).npy', top_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 100 good features\n",
      "found 200 good features\n",
      "found 300 good features\n",
      "found 400 good features\n",
      "found 500 good features\n",
      "found 600 good features\n",
      "found 700 good features\n",
      "found 800 good features\n",
      "found 900 good features\n",
      "found 1000 good features\n",
      "found 1100 good features\n",
      "found 1200 good features\n",
      "found 1300 good features\n",
      "found 1400 good features\n",
      "found 1500 good features\n",
      "found 1600 good features\n",
      "found 1700 good features\n",
      "found 1800 good features\n",
      "found 1900 good features\n",
      "found 2000 good features\n",
      "found 2100 good features\n",
      "found 2200 good features\n",
      "found 2300 good features\n",
      "found 2400 good features\n",
      "found 2500 good features\n",
      "found 2600 good features\n",
      "found 2700 good features\n",
      "found 2800 good features\n",
      "found 2900 good features\n",
      "found 3000 good features\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "sig calculated for feature 350\n",
      "sig calculated for feature 400\n",
      "sig calculated for feature 450\n",
      "sig calculated for feature 500\n",
      "sig calculated for feature 550\n",
      "sig calculated for feature 600\n",
      "sig calculated for feature 650\n",
      "sig calculated for feature 700\n",
      "sig calculated for feature 750\n",
      "sig calculated for feature 800\n",
      "sig calculated for feature 850\n",
      "sig calculated for feature 900\n",
      "sig calculated for feature 950\n",
      "sig calculated for feature 1000\n",
      "sig calculated for feature 1050\n",
      "sig calculated for feature 1100\n",
      "sig calculated for feature 1150\n",
      "sig calculated for feature 1200\n",
      "sig calculated for feature 1250\n",
      "sig calculated for feature 1300\n",
      "sig calculated for feature 1350\n",
      "sig calculated for feature 1400\n",
      "sig calculated for feature 1450\n",
      "sig calculated for feature 1500\n",
      "sig calculated for feature 1550\n",
      "sig calculated for feature 1600\n",
      "sig calculated for feature 1650\n",
      "sig calculated for feature 1700\n",
      "sig calculated for feature 1750\n",
      "sig calculated for feature 1800\n",
      "sig calculated for feature 1850\n",
      "sig calculated for feature 1900\n",
      "sig calculated for feature 1950\n",
      "sig calculated for feature 2000\n",
      "sig calculated for feature 2050\n",
      "sig calculated for feature 2100\n",
      "sig calculated for feature 2150\n",
      "sig calculated for feature 2200\n",
      "sig calculated for feature 2250\n",
      "sig calculated for feature 2300\n",
      "sig calculated for feature 2350\n",
      "sig calculated for feature 2400\n",
      "sig calculated for feature 2450\n",
      "sig calculated for feature 2500\n",
      "sig calculated for feature 2550\n",
      "sig calculated for feature 2600\n",
      "sig calculated for feature 2650\n",
      "sig calculated for feature 2700\n",
      "sig calculated for feature 2750\n",
      "sig calculated for feature 2800\n",
      "sig calculated for feature 2850\n",
      "sig calculated for feature 2900\n",
      "sig calculated for feature 2950\n",
      "sig calculated for feature 3000\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "import pC2DMS\n",
    "weights = np.load(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name1) + '/') + str(10000)+ ' scans/' + 'weights_tpe.npy')\n",
    "with open(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name1) + '/') + str(10000)+ ' scans/' + 'optimizer_tpe_loss_func_4.dill', \"rb\") as file:\n",
    "    optimizer = dill.load(file)\n",
    "scan1 = pC2DMS.Scan(os.path.join('./peptide output', list(datafile_dict.keys())[3] + '/' + str(name) + '/') + str(10000)+ ' scans/')\n",
    "cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc([optimizer.best_params[f'w_{i}'] for i in range(len(weights))]), numScans=10000)\n",
    "top_feat = cmap.analyse(3000)\n",
    "np.save(os.path.join('./peptide output', list(datafile_dict.keys())[3] + '/' + str(name) + '/') + str(10000)+ ' scans/w_top_feat_tpe(loss_func4,bins:100).npy', top_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 100 good features\n",
      "found 200 good features\n",
      "found 300 good features\n",
      "found 400 good features\n",
      "found 500 good features\n",
      "found 600 good features\n",
      "found 700 good features\n",
      "found 800 good features\n",
      "found 900 good features\n",
      "found 1000 good features\n",
      "found 1100 good features\n",
      "found 1200 good features\n",
      "found 1300 good features\n",
      "found 1400 good features\n",
      "found 1500 good features\n",
      "found 1600 good features\n",
      "found 1700 good features\n",
      "found 1800 good features\n",
      "found 1900 good features\n",
      "found 2000 good features\n",
      "found 2100 good features\n",
      "found 2200 good features\n",
      "found 2300 good features\n",
      "found 2400 good features\n",
      "found 2500 good features\n",
      "found 2600 good features\n",
      "found 2700 good features\n",
      "found 2800 good features\n",
      "found 2900 good features\n",
      "found 3000 good features\n",
      "sig calculated for feature 50\n",
      "sig calculated for feature 100\n",
      "sig calculated for feature 150\n",
      "sig calculated for feature 200\n",
      "sig calculated for feature 250\n",
      "sig calculated for feature 300\n",
      "sig calculated for feature 350\n",
      "sig calculated for feature 400\n",
      "sig calculated for feature 450\n",
      "sig calculated for feature 500\n",
      "sig calculated for feature 550\n",
      "sig calculated for feature 600\n",
      "sig calculated for feature 650\n",
      "sig calculated for feature 700\n",
      "sig calculated for feature 750\n",
      "sig calculated for feature 800\n",
      "sig calculated for feature 850\n",
      "sig calculated for feature 900\n",
      "sig calculated for feature 950\n",
      "sig calculated for feature 1000\n",
      "sig calculated for feature 1050\n",
      "sig calculated for feature 1100\n",
      "sig calculated for feature 1150\n",
      "sig calculated for feature 1200\n",
      "sig calculated for feature 1250\n",
      "sig calculated for feature 1300\n",
      "sig calculated for feature 1350\n",
      "sig calculated for feature 1400\n",
      "sig calculated for feature 1450\n",
      "sig calculated for feature 1500\n",
      "sig calculated for feature 1550\n",
      "sig calculated for feature 1600\n",
      "sig calculated for feature 1650\n",
      "sig calculated for feature 1700\n",
      "sig calculated for feature 1750\n",
      "sig calculated for feature 1800\n",
      "sig calculated for feature 1850\n",
      "sig calculated for feature 1900\n",
      "sig calculated for feature 1950\n",
      "sig calculated for feature 2000\n",
      "sig calculated for feature 2050\n",
      "sig calculated for feature 2100\n",
      "sig calculated for feature 2150\n",
      "sig calculated for feature 2200\n",
      "sig calculated for feature 2250\n",
      "sig calculated for feature 2300\n",
      "sig calculated for feature 2350\n",
      "sig calculated for feature 2400\n",
      "sig calculated for feature 2450\n",
      "sig calculated for feature 2500\n",
      "sig calculated for feature 2550\n",
      "sig calculated for feature 2600\n",
      "sig calculated for feature 2650\n",
      "sig calculated for feature 2700\n",
      "sig calculated for feature 2750\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "import pC2DMS\n",
    "weights = np.load(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name1) + '/') + str(10000)+ ' scans/' + 'weights_tpe.npy')\n",
    "with open(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name1) + '/') + str(10000)+ ' scans/' + 'optimizer_cma.dill', \"rb\") as file:\n",
    "    optimizer = dill.load(file)\n",
    "scan1 = pC2DMS.Scan(os.path.join('./peptide output', list(datafile_dict.keys())[3] + '/' + str(name) + '/') + str(10000)+ ' scans/')\n",
    "cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc(optimizer.result.xbest), numScans=10000)\n",
    "top_feat = cmap.analyse(3000)\n",
    "np.save(os.path.join('./peptide output', list(datafile_dict.keys())[3] + '/' + str(name) + '/') + str(10000)+ ' scans/w_top_feat_cma(loss_func2,bins:100).npy', top_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import pC2DMS\n",
    "weights = np.load(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name1) + '/') + str(10000)+ ' scans/' + 'weights_tpe.npy')\n",
    "with open(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name1) + '/') + str(10000)+ ' scans/' + 'optimizer_cma_loss_func1.dill', \"rb\") as file:\n",
    "    optimizer = dill.load(file)\n",
    "scan1 = pC2DMS.Scan(os.path.join('./peptide output', list(datafile_dict.keys())[3] + '/' + str(name) + '/') + str(10000)+ ' scans/')\n",
    "cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc(optimizer.result.xbest), numScans=10000)\n",
    "top_feat = cmap.analyse(3000)\n",
    "np.save(os.path.join('./peptide output', list(datafile_dict.keys())[3] + '/' + str(name) + '/') + str(10000)+ ' scans/w_top_feat_cma(loss_func1,bins:100).npy', top_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import pC2DMS\n",
    "weights = np.load(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name1) + '/') + str(10000)+ ' scans/' + 'weights_tpe.npy')\n",
    "with open(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name1) + '/') + str(10000)+ ' scans/' + 'optimizer_cma_loss_func3.dill', \"rb\") as file:\n",
    "    optimizer = dill.load(file)\n",
    "scan1 = pC2DMS.Scan(os.path.join('./peptide output', list(datafile_dict.keys())[3] + '/' + str(name) + '/') + str(10000)+ ' scans/')\n",
    "cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc(optimizer.result.xbest), numScans=10000)\n",
    "top_feat = cmap.analyse(3000)\n",
    "np.save(os.path.join('./peptide output', list(datafile_dict.keys())[3] + '/' + str(name) + '/') + str(10000)+ ' scans/w_top_feat_cma(loss_func3,bins:100).npy', top_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import pC2DMS\n",
    "weights = np.load(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name1) + '/') + str(10000)+ ' scans/' + 'weights_tpe.npy')\n",
    "with open(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name1) + '/') + str(10000)+ ' scans/' + 'optimizer_cma_loss_func4.dill', \"rb\") as file:\n",
    "    optimizer = dill.load(file)\n",
    "scan1 = pC2DMS.Scan(os.path.join('./peptide output', list(datafile_dict.keys())[3] + '/' + str(name) + '/') + str(10000)+ ' scans/')\n",
    "cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc(optimizer.result.xbest), numScans=10000)\n",
    "top_feat = cmap.analyse(3000)\n",
    "np.save(os.path.join('./peptide output', list(datafile_dict.keys())[3] + '/' + str(name) + '/') + str(10000)+ ' scans/w_top_feat_cma(loss_func4,bins:100).npy', top_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import pC2DMS\n",
    "weights = np.load(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name1) + '/') + str(10000)+ ' scans/' + 'weights_tpe.npy')\n",
    "with open(os.path.join('./peptide output', list(datafile_dict.keys())[2] + '/' + str(name1) + '/') + str(10000)+ ' scans/' + 'optimizer_bayopt_loss_func3.dill', \"rb\") as file:\n",
    "    optimizer = dill.load(file)\n",
    "scan1 = pC2DMS.Scan(os.path.join('./peptide output', list(datafile_dict.keys())[3] + '/' + str(name) + '/') + str(10000)+ ' scans/')\n",
    "cmap = pC2DMS.PCovMap(scan1, scan1.weightFunc(np.array([optimizer.max['params'][f'w_{i}'] for i in range(len(weights))])), numScans=10000)\n",
    "top_feat = cmap.analyse(3000)\n",
    "np.save(os.path.join('./peptide output', list(datafile_dict.keys())[3] + '/' + str(name) + '/') + str(10000)+ ' scans/w_top_feat_bayopt(loss_func3,bins:100).npy', top_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = datetime.now()\n",
    "logging.info(f\"The running time of the whole file: {stop - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test1]",
   "language": "python",
   "name": "conda-env-test1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
